<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Reading | Abhinau Kumar</title>
    <link>https://abhinaukumar.github.io/category/reading/</link>
      <atom:link href="https://abhinaukumar.github.io/category/reading/index.xml" rel="self" type="application/rss+xml" />
    <description>Reading</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sat, 08 Aug 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://abhinaukumar.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Reading</title>
      <link>https://abhinaukumar.github.io/category/reading/</link>
    </image>
    
    <item>
      <title>Texture Features</title>
      <link>https://abhinaukumar.github.io/post/texture-feature/</link>
      <pubDate>Sat, 08 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://abhinaukumar.github.io/post/texture-feature/</guid>
      <description>&lt;p&gt;This article is a review of existing texture characterization, identification and segmentation methods. These methods typically involve statistical methods, i.e. inferring from histograms or co-occurence matrices, and/or signal processing methods, either simple filtering and energy based methods, or as a method of pre-processing before applying statistical analysis.&lt;/p&gt;
&lt;p&gt;But, what is texture? Simply, &amp;lsquo;&amp;lsquo;texture&amp;rsquo;&amp;rsquo; describes the local arrangement of pixels/intensity values. For a classic example, consider two $8\times 8$ squares, one painted half white and half black, while the other has a checkerboard pattern. While they have the same mean luminance (brightness), they vary in the arrangement in the actual pixel (intensity) values. That is, they vary in &amp;lsquo;&amp;lsquo;texture&amp;rsquo;&amp;rsquo;.&lt;/p&gt;
&lt;p&gt;Texture can also be thought of as local &amp;lsquo;&amp;lsquo;complexity&amp;rsquo;&amp;rsquo;. The first square in our example had two plain regions, so it was &amp;lsquo;&amp;lsquo;simple&amp;rsquo;&amp;rsquo;, while the checkerboard pattern is more &amp;lsquo;&amp;lsquo;complex&amp;rsquo;&amp;rsquo;. Natural examples of texture include grass, fabric patterns, ripples, falling confetti, etc. Note that we have not rigorously defined the term yet. That is because texture is not a precisely defined concept, merely a notion. Even so, describing local arrangements can be very useful. Texture analysis has found great use in medical image processing, document processing and remote sensing. An example closer to my work would be that &amp;lsquo;&amp;lsquo;simple&amp;rsquo;&amp;rsquo; or &amp;lsquo;&amp;lsquo;plain&amp;rsquo;&amp;rsquo; regions can be compressed easily, while complex regions may demand higher bandwidth.&lt;/p&gt;
&lt;p&gt;What type of questions can we ask about texture? The simplest question is to identify it. Given a set of texture classes, can we identify a given texture as being one of these? This is called texture classification, and typically involves statistical methods. Another task is texture segmentation, where we wish to segment (split) an image into regions having different textures from each other. Think of the Windows XP wallpaper, but without clouds and less tidy grass. Such a method would split the image into the sky and the grass, because they have different textures.&lt;/p&gt;
&lt;p&gt;We will now look at features that we can use to describe textures. Currently, I&amp;rsquo;m not interested in specific algorithms. My goal is to find ways to &amp;lsquo;&amp;lsquo;describe&amp;rsquo;&amp;rsquo; texture. This will also not be an exhaustive review of all texture features. After all, texture is not the focus of my work. Learning this is just the means to an end, so I will only go so far as I need to.&lt;/p&gt;
&lt;p&gt;Most of my reading has used this presentation [1] and this review article [2] as jumping points to other sources wherever necessary.&lt;/p&gt;
&lt;h1 id=&#34;first-order-statistical-features&#34;&gt;First-Order Statistical Features&lt;/h1&gt;
&lt;p&gt;Statistics that only depend on individual pixel values are called first-order statistics. The local range and variance are simple first-order statistical features to describe textures. Plain regions have a smaller range (max - min) while textured regions have larger ranges because of the greater diversity in intensity values. A similar argument can be made about local variances because of which we expect textured regions to have higher variance than plain regions.&lt;/p&gt;
&lt;h1 id=&#34;gray-level-co-occurence-matrix-glcm&#34;&gt;Gray Level Co-occurence Matrix (GLCM)&lt;/h1&gt;
&lt;p&gt;The Gray Level Co-occurence Matric (GLCM) is arguably the most common method of describing textures. The GLCM records second-order statistics, because it depends on pairs of pixels, storing information about the relative positions of pixels having similar intensity values. Given an offset  $\delta = (\delta x, \delta y)$ , the GLCM is a $256 \times 256$ matrix counting the co-occurence of gray levels at an offset  $ \delta $ . That is, we construct a matrix whose entries are
$$ G_{\delta}[i,j] = \sum_x \sum_y \mathbb{1}(G[x,y] = i) \mathbb{1}(G[x+\delta y,y+\delta y] = j) $$&lt;/p&gt;
&lt;p&gt;The entries of this matrix are then normalized by the sum of all entries, giving us a normalized GLCM, say  $ P_\delta $ , which is a valid probability mass function. While the GLCM itself is not used to, say, compare textures, we derive numerical features from these which are used to describe texture. Some examples of such features are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Maximum&lt;/strong&gt; :  $ \max P_\delta[i,j] $ , i.e., the most likely pair of intensities.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Order $ k $ difference moment&lt;/strong&gt; : $ E[(i-j)^k)] $ , or its inverse  $ E[1/(i-j)^k] $ . A special case of this is the contrast, which is the 2nd difference moment, i.e.,  $ E[(i-j)^2] $ .&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Homogeneity&lt;/strong&gt;: $ E\left[\frac{1}{(1 + \lvert i-j\rvert)}\right] $ . A homogeneous image will have non-zero entries close to the principal diagonal, i.e  $ i \approx j $ , while a heterogeneous image will have a more even spread since many pairs occur.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Entropy&lt;/strong&gt;: $ E[-\log P_\delta[i,j]] $ , which is a measure of the &amp;lsquo;&amp;lsquo;spread&amp;rsquo;&amp;rsquo; or the amount of information in the distribution.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Correlation&lt;/strong&gt;: $ \frac{E[ij] - \mu_i\mu_j}{\sigma_i \sigma_j} $  which is high when pixels have a linear dependence.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Haralick [3] defined 14 texture features based on the GLCN. In a similar vein, the Gray Level Difference statistics (GLDS) are derived from a vector of 256 values, which count the number of times each difference  $ \lvert i-j\rvert $  occurs between pairs of intensity values separated by a distance  $ \delta $ .&lt;/p&gt;
&lt;p&gt;A main drawback of the GLCM and the GLDS is finding a good choice of  $ \delta $ . In the current deep learning/gradient-based optimization era, I would add that the non-differentiability of the counting process is an added drawback.&lt;/p&gt;
&lt;h1 id=&#34;autocorrelation-function&#34;&gt;Autocorrelation Function&lt;/h1&gt;
&lt;p&gt;The autocorrelation function (ACF) is a powerful signal processing method to extract repeating patterns. Given an image  $ I(x,y) $  the ACF is defined as&lt;/p&gt;
&lt;p&gt;$$ \rho(u,v) = \frac{\sum_x \sum_y I(x,y) I(x+u, y+v)}{\sum_x\sum_y I^2(x,y)} $$&lt;/p&gt;
&lt;p&gt;The auto-correlation function is a function of the &amp;lsquo;&amp;lsquo;offset&amp;rsquo;&amp;rsquo; between pairs of pixels. Given an offset, we multiply corresponding intensity values and consider the normalized sum. Why is this relevant? When the offset is close to the &amp;lsquo;&amp;lsquo;true&amp;rsquo;&amp;rsquo; offset between similar texture elements, the value of the ACF is close to 1, which is its highest value.&lt;/p&gt;
&lt;p&gt;Why is this the case? The short technical answer is &amp;lsquo;&amp;lsquo;Cauchy-Schwarz Inequality&amp;rsquo;&amp;rsquo;. More simply, we know that textures involve patterns which repeat at some intervals (although not exactly). So, it makes sense that we would like to &amp;lsquo;&amp;lsquo;test&amp;rsquo;&amp;rsquo; various offsets. When we choose the correct offset, the repeating intensities &amp;lsquo;&amp;lsquo;line up&amp;rsquo;&amp;rsquo;, so they get squared in the summation, leading to an ACF value close to 1. When we choose an &amp;lsquo;&amp;lsquo;incorrect&amp;rsquo;&amp;rsquo; offset, the repeating values do not overlap, leading to lower values. The figure below shows a visual argument for why &amp;lsquo;&amp;lsquo;lining up&amp;rsquo;&amp;rsquo; is a good thing.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;cs_ineq.png&#34; alt=&#34;Visualizing Cauchy-Scwarz Inequality&#34;&gt;&lt;/p&gt;
&lt;p&gt;At this point, much like Dumbledore to Harry on the Astronomy tower, I must ask for your trust in believing that squaring values when lining up is the &amp;lsquo;&amp;lsquo;best&amp;rsquo;&amp;rsquo; you can do (that is, lining up maximizes the ACF). Thankfully, you don&amp;rsquo;t need to wait for the death of a morally grey character to find out why this is actually the case. There are elegant proofs of the Cauchy Schwarz inequality that I would encourage you to find online. Several proofs have been reviewed in [4].&lt;/p&gt;
&lt;p&gt;Moving on, the ACF falls off slowly when the texture is coarse, because it takes a large shifts to fall out of, or out of phase with, the texture. On the other hand, fine textures cause sharp drop in the ACF.&lt;/p&gt;
&lt;h1 id=&#34;signal-processing-methods&#34;&gt;Signal Processing Methods&lt;/h1&gt;
&lt;p&gt;We begin with the simple observation that the coarseness of texture in a region is related to the density of edges in that region. Fine textures have higher edge density compared to coarse textures. To extract these features we can use edge operators like the Laplacian operator.&lt;/p&gt;
&lt;p&gt;Another set of filters is used to calculate the  $ (p+q) $  th moment of an image region  $ \mathcal{R} $
$$  m_{p,q}(x,y) = \sum_{(u,v) \in \mathcal{R}} u^p v^q I(u,v)  $$&lt;/p&gt;
&lt;p&gt;Choosing the region  $ \mathcal{R} $  to be a rectangular region, we can implement this as a linear filter having the appropriate weights.&lt;/p&gt;
&lt;p&gt;Perceptually motivated methods use filters that better represent the preattentive perception in the Human Visual System (HVS). Gabor filters, which are complex exponentials having a Gaussian envelope, are a good model of simple cells in the primary visual cortex. Because these filters are both frequency and orientation selective, they are used to conduct multi-scale multi-orientation analysis of images.&lt;/p&gt;
&lt;p&gt;To derive texture features, an image to passed through a Gabor filter bank to obtain subbands  $ r_i(x,y) $ . These responses are passed through a sigmoid non-linearity  $ \sigma $  (tanh function) and used to obtain texture features
\begin{equation}
f_i(x,y) = \sum_{(u,v) \in \mathcal{R}} |\sigma(r_i(u,v))|
\end{equation}&lt;/p&gt;
&lt;p&gt;Laws [5] proposed a computationally efficient method to compute texture energies using spatially separable filters. The method uses a set of Texture Energy Metric (TEM) vectors. The outer product of each pair of vectors results in a filter. The five types of vectors (corresponding to different textures) are level, edge, spot, wave and ripple.
These TEM filters are used to filter images and compute the local texture energy, which is simply the sum of the magnitudes in a local region.&lt;/p&gt;
&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;
&lt;p&gt;[1] Micheal A. Wirth. Texture Analysis 
&lt;a href=&#34;http://www.cyto.purdue.edu/cdroms/micro2/content/education/wirth06.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2] Mihran Tuceryan and Anil K. Jain. Texture Analysis. &lt;em&gt;Handbook of Pattern Recognition and Computer Vision&lt;/em&gt; 
&lt;a href=&#34;https://www.worldscientific.com/doi/abs/10.1142/9789814343138_0010&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[3] R. M. Haralick and K. Shanmugam and I. Dinstein. Textural Features for Image Classification. &lt;em&gt;IEEE Transactions on Systems, Man, and Cybernetics, 1973&lt;/em&gt; 
&lt;a href=&#34;http://haralick.org/journals/TexturalFeatures.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[4] Hui-Hua Wu and Shanhe Wu. Various proofs of the Cauchy-Schwarz inequality. 
&lt;a href=&#34;https://www.statisticshowto.com/wp-content/uploads/2016/06/Cauchy-Schwarzinequality.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[5] Kenneth I. Laws. Rapid Texture Identification. &lt;em&gt;Optics &amp;amp; Photonics, 1980&lt;/em&gt; 
&lt;a href=&#34;https://www.spiedigitallibrary.org/conference-proceedings-of-spie/0238/0000/Rapid-Texture-Identification/10.1117/12.959169.short&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Natural Scene Statistics and Visual Information Fidelity</title>
      <link>https://abhinaukumar.github.io/post/nss-and-vif/</link>
      <pubDate>Wed, 07 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://abhinaukumar.github.io/post/nss-and-vif/</guid>
      <description>&lt;p&gt;This article is a review of a popular Image Quality Model - Visual Information Fidelity (VIF) [1] which is all based on a statistical model of natural scenes. To do this, we will first review a powerful natural scene statistics (NSS) model, the Gaussian Scale Mixture. Then, we will review some basic information theory and use these tools to derive an image quality model.&lt;/p&gt;
&lt;p&gt;Let us begin at the basics. A Random Variable (&lt;em&gt;very&lt;/em&gt; informally, a variable that takes a random value) $ X $ is said to be Gaussian or Normal and is denoted by $ X \sim \mathcal{N}(\mu, \sigma^2 )$ if its associated probability distribution function (pdf) is given by
$$ f_N(x, \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right).$$&lt;/p&gt;
&lt;p&gt;If that does not mean a lot to you, it&amp;rsquo;s fine. One does not need to know the expression of a hyperbolic paraboloid to enjoy 
&lt;a href=&#34;https://interestingengineering.com/geometry-of-pringles-crunchy-hyperbolic-paraboloid&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;chips out of a Pringles can&lt;/a&gt;. The simplest way to think about it is that it a distribution which is centred around a mean value $\mu$ and whose spread (variance) is controlled by $\sigma$. It looks like so.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;paranormal_distribution.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;This definition can be extended naturally to random vectors (RVs), whose distribution is parameterized by a vector $\mu$ and covariance matrix $\Sigma$. This distribution is called a multivariate (because many variables) Gaussian and its pdf in $d$ dimensions is given by
$$ f_N(x; \mu, \Sigma, d) = \frac{1}{\sqrt{(2\pi)^d|\Sigma|}} \exp\left(-\frac{1}{2}(x - \mu)^T \Sigma^{-1} (x - \mu))\right) $$&lt;/p&gt;
&lt;p&gt;Now that we know what a Gaussian is, we can &amp;lsquo;&amp;lsquo;mix&amp;rsquo;&amp;rsquo; Gaussians together. The way we mix Gaussians is by having a set of Gaussian RVs, picking one of them at random and using that RV to generate a sample. In a way, we add a layer of randomness on top of the existing randomness.&lt;/p&gt;
&lt;p&gt;Let us call the random variable which tells us which Gaussian RV to pick the mixing variable $ S $. A compact way of expressing the explanation above is to say that given $ S = s $, RV $X$ has the conditional distribution $X|S = s \sim \mathcal{N}(\mu(s), \Sigma(s))$. Then, with a bit of probability magic, we can show that the pdf of the Gaussian Mixture is given by&lt;/p&gt;
&lt;p&gt;$$ f(x) = E_S[f_N(x; \mu(s), \Sigma(s), d)] $$&lt;/p&gt;
&lt;p&gt;Once again, the actual math is not critical. The intuition here is that we picked Gaussian RVs at random, so the overall distribution is an average of their individual distributions.&lt;/p&gt;
&lt;p&gt;The Gaussian Scale Mixture (GSM) is a simplified version of this distribution, where we assume all the Gaussians are centred at the origin, i.e., $ \mu(s) = 0$ and that all covariance matrices are scaled versions of each other, i.e., $\Sigma(s) = s^2 \Sigma$ for some covariance matrix $\Sigma$. That is to say, the components of a GSM only differ by a scaling factor. Interestingly, we can &amp;lsquo;&amp;lsquo;decompose&amp;rsquo;&amp;rsquo; the GSM distributed random variable using a positive random variable $S$ and a Gaussian RV $U \sim \mathcal{N}(0, \Sigma)$ as
$$ X = S \cdot U $$&lt;/p&gt;
&lt;p&gt;While this model is a &amp;lsquo;&amp;lsquo;simplification&amp;rsquo;&amp;rsquo; of the Gaussian Mixture Model (GMM), it is powerful a model for natural scenes. This is mainly because unlike a Gaussian, a GSM can represent heavy-tailed distributions, i.e, distributions which decay slowly. In their seminal work [3] showed that wavelet coefficients GSM can be used to model wavelet coefficients of natural images.&lt;/p&gt;
&lt;p&gt;At this point, we should clarify and introduce a few notions. First, what are natural images? They are not images &lt;em&gt;of nature&lt;/em&gt;. A natural image is an image which has not undergone any distortions, like blur, compression. noise, etc. So, a natural image is one which &lt;em&gt;looks&lt;/em&gt; natural.&lt;/p&gt;
&lt;p&gt;Second, what are wavelets? Informally, a wavelet is a &amp;lsquo;&amp;lsquo;localized&amp;rsquo;&amp;rsquo; wave. That is, while waves (think sinusoids) are periodic and infinite, wavelets are more localized in space (or time). Much like the Heisenberg Uncertainty principle, signals also obey an uncertainty principle. Simply put, a signal cannot have an arbitrary small spread in both space and frequency. So, because a wave has zero spread in frequency (a sinusoid has only one frequency), it has an infinite spread in space. In other words, we say that a sinusoid offers perfect frequency resolution but no spatial resolution.&lt;/p&gt;
&lt;p&gt;A wavelet, by being localized in space, allows us to trade off spread in space for spread in frequency, giving us both (limited) spatial and frequency resolution. A wider wavelet has poorer spatial resolution, but better frequency resolution. Because images are two dimensional, there is also a notion of orientation resolution. Isotropic functions (functions which are identical in all directions) offer no orientation selectivity in space while very narrow functions offer high orientation selectivity. The steerable pyramid [3], used in VIF and STRRED [2], provides a set of wavelets which allows for an (overcomplete) multi-scale multi-orientation decomposition of images.&lt;/p&gt;
&lt;p&gt;Finally, let us talk about information theory. In my opinion, information theory is the most beautiful offshoot of probability theory. The goal of information theory, as one would guess, is to characterize the amount of information stored in &amp;lsquo;&amp;lsquo;sources&amp;rsquo;&amp;rsquo;, which are random variables.&lt;/p&gt;
&lt;p&gt;The amount of information in, or randomness of, a random variable $ X $ is called its entropy. Mathematically, the (Shannon) entropy of a random variable having a probability mass function $f(x)$ is given by
$$ H(X) = -E[\log f(X)] $$&lt;/p&gt;
&lt;p&gt;This function satisfies properties that we would expect a randomness measure to satisfy. First, we would like the amount of randomness to always be non-negative, which is true of Shannon Entropy (See 
&lt;a href=&#34;http://www.cs.yorku.ca/~kosta/CompVis_Notes/jensen.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jensen&amp;rsquo;s Inequality&lt;/a&gt;). Second, if a random variable is constant, i.e. $Pr[X = x] = 1$ for some $x$, then it has zero randomness and its entropy is 0. Finally, the uniform distribution has the highest entropy (among all distributions having the same size of support).&lt;/p&gt;
&lt;p&gt;Let us now bring in a friend. Let $Y$ be another random variable, which is &lt;strong&gt;not&lt;/strong&gt; independent of $X$ (I said &amp;lsquo;&amp;lsquo;friend&amp;rsquo;&#39;). We can ask the question, &amp;ldquo;how much information is in $X$ if I already know $Y$&amp;rdquo;? To answer this quantitatively, we compute the conditional entropy. To calculate this, we first consider the entropy of the conditional distribution of $X$ when we are given each possible of $Y$, i.e.,
$$H(X | Y = y) = E_{X|Y = y}\left[-\log f(x | Y = y)\right] $$&lt;/p&gt;
&lt;p&gt;But we only assumed that we knew $Y$, not that $Y$ took any particular value. So, we average this over all possible values of $Y$ to get the conditional entropy
$$ H(X|Y) = E[H(X|Y = y)] $$&lt;/p&gt;
&lt;p&gt;Based on entropy and conditional entropy, we define the mutual information (MI) of two random variables $X$ and $Y$ which, as the name suggests, characterizes the amount of information each variable has about the other. The intuition for the mathematical expression for MI is as follows. Let us say that $X$ has information (entropy) $H(X)$. But, if we are given the random variable $Y$, its information content decreases (it will not increase) to  $H(X|Y)$. Then, the difference is the amount of information about $X$ that we have obtained from $Y$. So, the MI between $X$ and $Y$ is defined as
$$ I(X;Y) = H(X) - H(Y|X) = H(Y) - H(X|Y) $$&lt;/p&gt;
&lt;p&gt;Now let us briefly look at its properties. As suggested by the expression, the MI is symmetric, i.e. the MS between $X$ and $Y$ is equal to the MI between $Y$ and $X$. After all, we called it &lt;em&gt;mutual&lt;/em&gt; information. Secondly, the MI between two random variables is non-negative. If knowing $Y$ means we know $X$ exactly, $H(X|Y)=0$ (because $X$ is known deterministically, there is no randomness) and the MI is just the information in $X$, i.e. $H(X)$. If $X$ and $Y$ are independent random variables, $H(X|Y) = H(X)$ and the MI is zero, which is what we would expect.&lt;/p&gt;
&lt;p&gt;I must confess, I played a little bait and switch routine over the last few paragraphs. I allured you with the promise of continuous random variables (random vectors, even!), but defined these information-theoretic quantities only for discrete distributions. For continuous random variables, we define these quantities in analogous ways, using the pdf instead of the probability mass functions, although some pleasant properties are lost.&lt;/p&gt;
&lt;p&gt;We are finally ready to discuss the three information-theoretic quality models in question. Let us begin with VIF. VIF assumes a statistical model of the natural source (the GSM model discussed above), a distortion channel which distorts this &amp;lsquo;&amp;lsquo;pristine&amp;rsquo;&amp;rsquo; image, and an additive noise model of the human visual system (HVS). We can illustrate the VIF model as below.&lt;/p&gt;
&lt;div class=&#34;mermaid&#34;&gt;
graph LR;
  A[Source] --&gt; B[HVS]
  B --&gt; C[Receiver]
  A --&gt; D[Distortion Channel]
  D --&gt; E[HVS]
  E --&gt; F[Receiver]
&lt;/div&gt;
&lt;p&gt;So, in line with our source model, let us define the source random variable as a GSM distributed random vector&lt;/p&gt;
&lt;p&gt;$$ C = S \cdot U $$&lt;/p&gt;
&lt;p&gt;The distortion channel is described as having a deterministic scalar gain $g$ and an additive White Gaussian Noise (AWGN) $V \sim \mathcal{N}(0, \sigma_v^2 I)$. So, the distorted random variable is given by&lt;/p&gt;
&lt;p&gt;$$ D = g \cdot C + V $$&lt;/p&gt;
&lt;p&gt;Finally, we model the neural noise of the HVS as an AWGN channel having $N, N&amp;rsquo; \sim \mathcal{N}(0, \sigma_n^2)$. The final received pristine and distorted images are given by&lt;/p&gt;
&lt;p&gt;$$ E = C + N = S \cdot U + N$$
$$ F = D + N&amp;rsquo; = g \cdot S \cdot U + V + N&amp;rsquo; $$&lt;/p&gt;
&lt;p&gt;We condition all quantities on knowing $S$ because knowing $S$ allows us to predict VIF for our particular pair of reference and test images, instead of a general average case. The intuition here is that upon distortion, lesser information about the source is retained in the distorted image. So, the VIF index is defined as
$$ VIF = \frac{I(C; E | S = s)}{I(C; F | S = s)} $$&lt;/p&gt;
&lt;p&gt;Conveniently, conditioning on $S$ results in all RVs becoming Gaussians, whose entropies are easy to compute. For an image having $N$ samples of dimension $d$, if the covariance matrix has eigenvalues $
\lambda_j$, the mutual informations can be calculated easily as
$$ I(C; E | S = s) = \sum\limits_{i=1}^{N}\sum_{j=1}^{d} \log\left(1 + \frac{s_i^2\lambda_j}{\sigma_n^2}\right) $$
$$ I(C; F | S = s) = \sum\limits_{i=1}^{N}\sum_{j=1}^{d} \log\left(1 + \frac{g_i^2 s_i^2 \lambda_j}{\sigma_v^2 + \sigma_n^2}\right) $$&lt;/p&gt;
&lt;p&gt;If you have made it this far, congratulations! You now know what natural scene statistics and information-theoretic quality models look like. We will end by briefly commenting on the implementation details. In practice, both the reference and test images are first transformed into the Wavelet domain using the Steerable Pyramid. The coefficients (which look like filtered images) are collected in 3x3 blocks and modelled as 9-dim vectors $C_i$ and $D_i$ respectively.&lt;/p&gt;
&lt;p&gt;What remains is to find the parameters of the GSM distribution and the distortion channel from these wavelet coefficients. The HVS is assumed to have a known channel noise $ \sigma_n^2 = 0.1 $. Because these parameter estimation details are not a part of the core idea behind VIF, I will just state them without proof.&lt;/p&gt;
&lt;p&gt;$$ \Sigma = \frac{1}{N} \sum C_i C_i^T $$
$$ s_i = \frac{C_i^T \Sigma^{-1} C_i}{9} $$
$$ g_i = \frac{Cov(C, D)}{Cov(D, D)} $$
$$ \sigma_{v,i}^2 = Cov(D, D) - g_i Cov(C, D)$$&lt;/p&gt;
&lt;p&gt;Note that the covariances in the last two equations are scalar covariances calculated in corresponding local neighbourhoods of wavelet coefficients. And there we have it - VIF! Admittedly, this post turned out to be &lt;em&gt;much&lt;/em&gt; longer than I had anticipated. But, it is a good thing because I will write about STRRED and SpEED-QA [5] next, which use these same concepts, but differently. So now, even though you may not realize it, you know enough to understand two more information-theoretic quality models.&lt;/p&gt;
&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;
&lt;p&gt;[1] Hamid R. Sheikh and Alan C. Bovik. Image Information and Visual Quality. &lt;em&gt;IEEE Transactions on Image Processing, 2006&lt;/em&gt; 
&lt;a href=&#34;https://live.ece.utexas.edu/publications/2004/hrs_ieeetip_2004_imginfo.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2] Rajiv Soundararajan and Alan C. Bovik, Video quality assessment by reduced reference spatio-temporal entropic differencing. &lt;em&gt;IEEE Transactions on Circuits and Systems for Video Technology, 2013&lt;/em&gt; 
&lt;a href=&#34;https://www.live.ece.utexas.edu/publications/2013/Rajiv%20%20Video-RRED%20Paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[3] Martin J. Wainwright and Eero P. Simoncelli. Scale Mixtures of Gaussians and the Statistics of Natural Images. &lt;em&gt;Proceedings of the 12th International Conference on Neural Information Processing Systems. 1999&lt;/em&gt; 
&lt;a href=&#34;https://papers.nips.cc/paper/1750-scale-mixtures-of-gaussians-and-the-statistics-of-natural-images.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[4] Eero P. Simoncelli and William T. Freeman. The Steerable Pyramid: A Flexible Architecture For Multi-Scale Derivative Computation. &lt;em&gt;IEEE Conference on Image Processing, 1995&lt;/em&gt; 
&lt;a href=&#34;https://www.cns.nyu.edu/pub/eero/simoncelli95b.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[5] Christos G. Bampis and Praful Gupta and Rajiv Soundararajan and Alan C. Bovik. SpEED-QA: Spatial Efficient Entropic Differencing for Image and Video Quality. &lt;em&gt;IEEE Signal Processing Letters, 2017&lt;/em&gt; 
&lt;a href=&#34;http://www.christosbampis.info/uploads/d4e7f73994efae5148ba6617684696b07b31.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
