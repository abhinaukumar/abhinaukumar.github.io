<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Abhinau Kumar</title>
    <link>https://nownowk.github.io/</link>
      <atom:link href="https://nownowk.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Abhinau Kumar</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sun, 09 Aug 2020 16:43:32 -0500</lastBuildDate>
    <image>
      <url>https://nownowk.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Abhinau Kumar</title>
      <link>https://nownowk.github.io/</link>
    </image>
    
    <item>
      <title>Perceptually Driven Conditional GAN for Fourier Ptychography</title>
      <link>https://nownowk.github.io/publication/fourier-ptychography/</link>
      <pubDate>Sun, 09 Aug 2020 16:43:32 -0500</pubDate>
      <guid>https://nownowk.github.io/publication/fourier-ptychography/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Better Safe than Sorry: Evidence Accumulation Allows for Safe Reinforcement Learning</title>
      <link>https://nownowk.github.io/publication/safe-reinforcement-learning/</link>
      <pubDate>Sun, 09 Aug 2020 16:38:12 -0500</pubDate>
      <guid>https://nownowk.github.io/publication/safe-reinforcement-learning/</guid>
      <description></description>
    </item>
    
    <item>
      <title>No-reference quality assessment of tone mapped High Dynamic Range (HDR) images using transfer learning</title>
      <link>https://nownowk.github.io/publication/hdr-quality-assessment/</link>
      <pubDate>Sun, 09 Aug 2020 16:34:44 -0500</pubDate>
      <guid>https://nownowk.github.io/publication/hdr-quality-assessment/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Texture Features</title>
      <link>https://nownowk.github.io/post/texture-feature/</link>
      <pubDate>Sat, 08 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://nownowk.github.io/post/texture-feature/</guid>
      <description>&lt;p&gt;This article is a review of existing texture characterization, identification and segmentation methods. These methods typically involve statistical methods, i.e. inferring from histograms or co-occurence matrices, and/or signal processing methods, either simple filtering and energy based methods, or as a method of pre-processing before applying statistical analysis.&lt;/p&gt;
&lt;p&gt;But, what is texture? Simply, &amp;lsquo;&amp;lsquo;texture&amp;rsquo;&amp;rsquo; describes the local arrangement of pixels/intensity values. For a classic example, consider two $8\times 8$ squares, one painted half white and half black, while the other has a checkerboard pattern. While they have the same mean luminance (brightness), they vary in the arrangement in the actual pixel (intensity) values. That is, they vary in &amp;lsquo;&amp;lsquo;texture&amp;rsquo;&amp;rsquo;.&lt;/p&gt;
&lt;p&gt;Texture can also be thought of as local &amp;lsquo;&amp;lsquo;complexity&amp;rsquo;&amp;rsquo;. The first square in our example had two plain regions, so it was &amp;lsquo;&amp;lsquo;simple&amp;rsquo;&amp;rsquo;, while the checkerboard pattern is more &amp;lsquo;&amp;lsquo;complex&amp;rsquo;&amp;rsquo;. Natural examples of texture include grass, fabric patterns, ripples, falling confetti, etc. Note that we have not rigorously defined the term yet. That is because texture is not a precisely defined concept, merely a notion. Even so, describing local arrangements can be very useful. Texture analysis has found great use in medical image processing, document processing and remote sensing. An example closer to my work would be that &amp;lsquo;&amp;lsquo;simple&amp;rsquo;&amp;rsquo; or &amp;lsquo;&amp;lsquo;plain&amp;rsquo;&amp;rsquo; regions can be compressed easily, while complex regions may demand higher bandwidth.&lt;/p&gt;
&lt;p&gt;What type of questions can we ask about texture? The simplest question is to identify it. Given a set of texture classes, can we identify a given texture as being one of these? This is called texture classification, and typically involves statistical methods. Another task is texture segmentation, where we wish to segment (split) an image into regions having different textures from each other. Think of the Windows XP wallpaper, but without clouds and less tidy grass. Such a method would split the image into the sky and the grass, because they have different textures.&lt;/p&gt;
&lt;p&gt;We will now look at features that we can use to describe textures. Currently, I&amp;rsquo;m not interested in specific algorithms. My goal is to find ways to &amp;lsquo;&amp;lsquo;describe&amp;rsquo;&amp;rsquo; texture. This will also not be an exhaustive review of all texture features. After all, texture is not the focus of my work. Learning this is just the means to an end, so I will only go so far as I need to.&lt;/p&gt;
&lt;p&gt;Most of my reading has used this presentation [1] and this review article [2] as jumping points to other sources wherever necessary.&lt;/p&gt;
&lt;h1 id=&#34;first-order-statistical-features&#34;&gt;First-Order Statistical Features&lt;/h1&gt;
&lt;p&gt;Statistics that only depend on individual pixel values are called first-order statistics. The local range and variance are simple first-order statistical features to describe textures. Plain regions have a smaller range (max - min) while textured regions have larger ranges because of the greater diversity in intensity values. A similar argument can be made about local variances because of which we expect textured regions to have higher variance than plain regions.&lt;/p&gt;
&lt;h1 id=&#34;gray-level-co-occurence-matrix-glcm&#34;&gt;Gray Level Co-occurence Matrix (GLCM)&lt;/h1&gt;
&lt;p&gt;The Gray Level Co-occurence Matric (GLCM) is arguably the most common method of describing textures. The GLCM records second-order statistics, because it depends on pairs of pixels, storing information about the relative positions of pixels having similar intensity values. Given an offset  $\delta = (\delta x, \delta y)$ , the GLCM is a $256 \times 256$ matrix counting the co-occurence of gray levels at an offset  $ \delta $ . That is, we construct a matrix whose entries are
$$ G_{\delta}[i,j] = \sum_x \sum_y \mathbb{1}(G[x,y] = i) \mathbb{1}(G[x+\delta y,y+\delta y] = j) $$&lt;/p&gt;
&lt;p&gt;The entries of this matrix are then normalized by the sum of all entries, giving us a normalized GLCM, say  $ P_\delta $ , which is a valid probability mass function. While the GLCM itself is not used to, say, compare textures, we derive numerical features from these which are used to describe texture. Some examples of such features are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Maximum&lt;/strong&gt; :  $ \max P_\delta[i,j] $ , i.e., the most likely pair of intensities.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Order $ k $ difference moment&lt;/strong&gt; : $ E[(i-j)^k)] $ , or its inverse  $ E[1/(i-j)^k] $ . A special case of this is the contrast, which is the 2nd difference moment, i.e.,  $ E[(i-j)^2] $ .&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Homogeneity&lt;/strong&gt;: $ E\left[\frac{1}{(1 + \lvert i-j\rvert)}\right] $ . A homogeneous image will have non-zero entries close to the principal diagonal, i.e  $ i \approx j $ , while a heterogeneous image will have a more even spread since many pairs occur.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Entropy&lt;/strong&gt;: $ E[-\log P_\delta[i,j]] $ , which is a measure of the &amp;lsquo;&amp;lsquo;spread&amp;rsquo;&amp;rsquo; or the amount of information in the distribution.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Correlation&lt;/strong&gt;: $ \frac{E[ij] - \mu_i\mu_j}{\sigma_i \sigma_j} $  which is high when pixels have a linear dependence.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Haralick [3] defined 14 texture features based on the GLCN. In a similar vein, the Gray Level Difference statistics (GLDS) are derived from a vector of 256 values, which count the number of times each difference  $ \lvert i-j\rvert $  occurs between pairs of intensity values separated by a distance  $ \delta $ .&lt;/p&gt;
&lt;p&gt;A main drawback of the GLCM and the GLDS is finding a good choice of  $ \delta $ . In the current deep learning/gradient-based optimization era, I would add that the non-differentiability of the counting process is an added drawback.&lt;/p&gt;
&lt;h1 id=&#34;autocorrelation-function&#34;&gt;Autocorrelation Function&lt;/h1&gt;
&lt;p&gt;The autocorrelation function (ACF) is a powerful signal processing method to extract repeating patterns. Given an image  $ I(x,y) $  the ACF is defined as&lt;/p&gt;
&lt;p&gt;$$ \rho(u,v) = \frac{\sum_x \sum_y I(x,y) I(x+u, y+v)}{\sum_x\sum_y I^2(x,y)} $$&lt;/p&gt;
&lt;p&gt;The auto-correlation function is a function of the &amp;lsquo;&amp;lsquo;offset&amp;rsquo;&amp;rsquo; between pairs of pixels. Given an offset, we multiply corresponding intensity values and consider the normalized sum. Why is this relevant? When the offset is close to the &amp;lsquo;&amp;lsquo;true&amp;rsquo;&amp;rsquo; offset between similar texture elements, the value of the ACF is close to 1, which is its highest value.&lt;/p&gt;
&lt;p&gt;Why is this the case? The short technical answer is &amp;lsquo;&amp;lsquo;Cauchy-Schwarz Inequality&amp;rsquo;&amp;rsquo;. More simply, we know that textures involve patterns which repeat at some intervals (although not exactly). So, it makes sense that we would like to &amp;lsquo;&amp;lsquo;test&amp;rsquo;&amp;rsquo; various offsets. When we choose the correct offset, the repeating intensities &amp;lsquo;&amp;lsquo;line up&amp;rsquo;&amp;rsquo;, so they get squared in the summation, leading to an ACF value close to 1. When we choose an &amp;lsquo;&amp;lsquo;incorrect&amp;rsquo;&amp;rsquo; offset, the repeating values do not overlap, leading to lower values. The figure below shows a visual argument for why &amp;lsquo;&amp;lsquo;lining up&amp;rsquo;&amp;rsquo; is a good thing.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;cs_ineq.png&#34; alt=&#34;Visualizing Cauchy-Scwarz Inequality&#34;&gt;&lt;/p&gt;
&lt;p&gt;At this point, much like Dumbledore to Harry on the Astronomy tower, I must ask for your trust in believing that squaring values when lining up is the &amp;lsquo;&amp;lsquo;best&amp;rsquo;&amp;rsquo; you can do (that is, lining up maximizes the ACF). Thankfully, you don&amp;rsquo;t need to wait for the death of a morally grey character to find out why this is actually the case. There are elegant proofs of the Cauchy Schwarz inequality that I would encourage you to find online. Several proofs have been reviewed in [4].&lt;/p&gt;
&lt;p&gt;Moving on, the ACF falls off slowly when the texture is coarse, because it takes a large shifts to fall out of, or out of phase with, the texture. On the other hand, fine textures cause sharp drop in the ACF.&lt;/p&gt;
&lt;h1 id=&#34;signal-processing-methods&#34;&gt;Signal Processing Methods&lt;/h1&gt;
&lt;p&gt;We begin with the simple observation that the coarseness of texture in a region is related to the density of edges in that region. Fine textures have higher edge density compared to coarse textures. To extract these features we can use edge operators like the Laplacian operator.&lt;/p&gt;
&lt;p&gt;Another set of filters is used to calculate the  $ (p+q) $  th moment of an image region  $ \mathcal{R} $
$$  m_{p,q}(x,y) = \sum_{(u,v) \in \mathcal{R}} u^p v^q I(u,v)  $$&lt;/p&gt;
&lt;p&gt;Choosing the region  $ \mathcal{R} $  to be a rectangular region, we can implement this as a linear filter having the appropriate weights.&lt;/p&gt;
&lt;p&gt;Perceptually motivated methods use filters that better represent the preattentive perception in the Human Visual System (HVS). Gabor filters, which are complex exponentials having a Gaussian envelope, are a good model of simple cells in the primary visual cortex. Because these filters are both frequency and orientation selective, they are used to conduct multi-scale multi-orientation analysis of images.&lt;/p&gt;
&lt;p&gt;To derive texture features, an image to passed through a Gabor filter bank to obtain subbands  $ r_i(x,y) $ . These responses are passed through a sigmoid non-linearity  $ \sigma $  (tanh function) and used to obtain texture features
\begin{equation}
f_i(x,y) = \sum_{(u,v) \in \mathcal{R}} |\sigma(r_i(u,v))|
\end{equation}&lt;/p&gt;
&lt;p&gt;Laws [5] proposed a computationally efficient method to compute texture energies using spatially separable filters. The method uses a set of Texture Energy Metric (TEM) vectors. The outer product of each pair of vectors results in a filter. The five types of vectors (corresponding to different textures) are level, edge, spot, wave and ripple.
These TEM filters are used to filter images and compute the local texture energy, which is simply the sum of the magnitudes in a local region.&lt;/p&gt;
&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;
&lt;p&gt;[1] Micheal A. Wirth. Texture Analysis 
&lt;a href=&#34;http://www.cyto.purdue.edu/cdroms/micro2/content/education/wirth06.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2] Mihran Tuceryan and Anil K. Jain. Texture Analysis. &lt;em&gt;Handbook of Pattern Recognition and Computer Vision&lt;/em&gt; 
&lt;a href=&#34;https://www.worldscientific.com/doi/abs/10.1142/9789814343138_0010&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[3] R. M. Haralick and K. Shanmugam and I. Dinstein. Textural Features for Image Classification. &lt;em&gt;IEEE Transactions on Systems, Man, and Cybernetics, 1973&lt;/em&gt; 
&lt;a href=&#34;http://haralick.org/journals/TexturalFeatures.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[4] Hui-Hua Wu and Shanhe Wu. Various proofs of the Cauchy-Schwarz inequality. 
&lt;a href=&#34;https://www.statisticshowto.com/wp-content/uploads/2016/06/Cauchy-Schwarzinequality.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[5] Kenneth I. Laws. Rapid Texture Identification. &lt;em&gt;Optics &amp;amp; Photonics, 1980&lt;/em&gt; 
&lt;a href=&#34;https://www.spiedigitallibrary.org/conference-proceedings-of-spie/0238/0000/Rapid-Texture-Identification/10.1117/12.959169.short&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Natural Scene Statistics and Visual Information Fidelity</title>
      <link>https://nownowk.github.io/post/nss-and-vif/</link>
      <pubDate>Wed, 07 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://nownowk.github.io/post/nss-and-vif/</guid>
      <description>&lt;p&gt;This article is a review of a popular Image Quality Model - Visual Information Fidelity (VIF) [1] which is all based on a statistical model of natural scenes. To do this, we will first review a powerful natural scene statistics (NSS) model, the Gaussian Scale Mixture. Then, we will review some basic information theory and use these tools to derive an image quality model.&lt;/p&gt;
&lt;p&gt;Let us begin at the basics. A Random Variable (&lt;em&gt;very&lt;/em&gt; informally, a variable that takes a random value) $ X $ is said to be Gaussian or Normal and is denoted by $ X \sim \mathcal{N}(\mu, \sigma^2 )$ if its associated probability distribution function (pdf) is given by
$$ f_N(x, \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right).$$&lt;/p&gt;
&lt;p&gt;If that does not mean a lot to you, it&amp;rsquo;s fine. One does not need to know the expression of a hyperbolic paraboloid to enjoy 
&lt;a href=&#34;https://interestingengineering.com/geometry-of-pringles-crunchy-hyperbolic-paraboloid&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;chips out of a Pringles can&lt;/a&gt;. The simplest way to think about it is that it a distribution which is centred around a mean value $\mu$ and whose spread (variance) is controlled by $\sigma$. It looks like so.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;paranormal_distribution.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;This definition can be extended naturally to random vectors (RVs), whose distribution is parameterized by a vector $\mu$ and covariance matrix $\Sigma$. This distribution is called a multivariate (because many variables) Gaussian and its pdf in $d$ dimensions is given by
$$ f_N(x; \mu, \Sigma, d) = \frac{1}{\sqrt{(2\pi)^d|\Sigma|}} \exp\left(-\frac{1}{2}(x - \mu)^T \Sigma^{-1} (x - \mu))\right) $$&lt;/p&gt;
&lt;p&gt;Now that we know what a Gaussian is, we can &amp;lsquo;&amp;lsquo;mix&amp;rsquo;&amp;rsquo; Gaussians together. The way we mix Gaussians is by having a set of Gaussian RVs, picking one of them at random and using that RV to generate a sample. In a way, we add a layer of randomness on top of the existing randomness.&lt;/p&gt;
&lt;p&gt;Let us call the random variable which tells us which Gaussian RV to pick the mixing variable $ S $. A compact way of expressing the explanation above is to say that given $ S = s $, RV $X$ has the conditional distribution $X|S = s \sim \mathcal{N}(\mu(s), \Sigma(s))$. Then, with a bit of probability magic, we can show that the pdf of the Gaussian Mixture is given by&lt;/p&gt;
&lt;p&gt;$$ f(x) = E_S[f_N(x; \mu(s), \Sigma(s), d)] $$&lt;/p&gt;
&lt;p&gt;Once again, the actual math is not critical. The intuition here is that we picked Gaussian RVs at random, so the overall distribution is an average of their individual distributions.&lt;/p&gt;
&lt;p&gt;The Gaussian Scale Mixture (GSM) is a simplified version of this distribution, where we assume all the Gaussians are centred at the origin, i.e., $ \mu(s) = 0$ and that all covariance matrices are scaled versions of each other, i.e., $\Sigma(s) = s^2 \Sigma$ for some covariance matrix $\Sigma$. That is to say, the components of a GSM only differ by a scaling factor. Interestingly, we can &amp;lsquo;&amp;lsquo;decompose&amp;rsquo;&amp;rsquo; the GSM distributed random variable using a positive random variable $S$ and a Gaussian RV $U \sim \mathcal{N}(0, \Sigma)$ as
$$ X = S \cdot U $$&lt;/p&gt;
&lt;p&gt;While this model is a &amp;lsquo;&amp;lsquo;simplification&amp;rsquo;&amp;rsquo; of the Gaussian Mixture Model (GMM), it is powerful a model for natural scenes. This is mainly because unlike a Gaussian, a GSM can represent heavy-tailed distributions, i.e, distributions which decay slowly. In their seminal work [3] showed that wavelet coefficients GSM can be used to model wavelet coefficients of natural images.&lt;/p&gt;
&lt;p&gt;At this point, we should clarify and introduce a few notions. First, what are natural images? They are not images &lt;em&gt;of nature&lt;/em&gt;. A natural image is an image which has not undergone any distortions, like blur, compression. noise, etc. So, a natural image is one which &lt;em&gt;looks&lt;/em&gt; natural.&lt;/p&gt;
&lt;p&gt;Second, what are wavelets? Informally, a wavelet is a &amp;lsquo;&amp;lsquo;localized&amp;rsquo;&amp;rsquo; wave. That is, while waves (think sinusoids) are periodic and infinite, wavelets are more localized in space (or time). Much like the Heisenberg Uncertainty principle, signals also obey an uncertainty principle. Simply put, a signal cannot have an arbitrary small spread in both space and frequency. So, because a wave has zero spread in frequency (a sinusoid has only one frequency), it has an infinite spread in space. In other words, we say that a sinusoid offers perfect frequency resolution but no spatial resolution.&lt;/p&gt;
&lt;p&gt;A wavelet, by being localized in space, allows us to trade off spread in space for spread in frequency, giving us both (limited) spatial and frequency resolution. A wider wavelet has poorer spatial resolution, but better frequency resolution. Because images are two dimensional, there is also a notion of orientation resolution. Isotropic functions (functions which are identical in all directions) offer no orientation selectivity in space while very narrow functions offer high orientation selectivity. The steerable pyramid [3], used in VIF and STRRED [2], provides a set of wavelets which allows for an (overcomplete) multi-scale multi-orientation decomposition of images.&lt;/p&gt;
&lt;p&gt;Finally, let us talk about information theory. In my opinion, information theory is the most beautiful offshoot of probability theory. The goal of information theory, as one would guess, is to characterize the amount of information stored in &amp;lsquo;&amp;lsquo;sources&amp;rsquo;&amp;rsquo;, which are random variables.&lt;/p&gt;
&lt;p&gt;The amount of information in, or randomness of, a random variable $ X $ is called its entropy. Mathematically, the (Shannon) entropy of a random variable having a probability mass function $f(x)$ is given by
$$ H(X) = -E[\log f(X)] $$&lt;/p&gt;
&lt;p&gt;This function satisfies properties that we would expect a randomness measure to satisfy. First, we would like the amount of randomness to always be non-negative, which is true of Shannon Entropy (See 
&lt;a href=&#34;http://www.cs.yorku.ca/~kosta/CompVis_Notes/jensen.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jensen&amp;rsquo;s Inequality&lt;/a&gt;). Second, if a random variable is constant, i.e. $Pr[X = x] = 1$ for some $x$, then it has zero randomness and its entropy is 0. Finally, the uniform distribution has the highest entropy (among all distributions having the same size of support).&lt;/p&gt;
&lt;p&gt;Let us now bring in a friend. Let $Y$ be another random variable, which is &lt;strong&gt;not&lt;/strong&gt; independent of $X$ (I said &amp;lsquo;&amp;lsquo;friend&amp;rsquo;&#39;). We can ask the question, &amp;ldquo;how much information is in $X$ if I already know $Y$&amp;rdquo;? To answer this quantitatively, we compute the conditional entropy. To calculate this, we first consider the entropy of the conditional distribution of $X$ when we are given each possible of $Y$, i.e.,
$$H(X | Y = y) = E_{X|Y = y}\left[-\log f(x | Y = y)\right] $$&lt;/p&gt;
&lt;p&gt;But we only assumed that we knew $Y$, not that $Y$ took any particular value. So, we average this over all possible values of $Y$ to get the conditional entropy
$$ H(X|Y) = E[H(X|Y = y)] $$&lt;/p&gt;
&lt;p&gt;Based on entropy and conditional entropy, we define the mutual information (MI) of two random variables $X$ and $Y$ which, as the name suggests, characterizes the amount of information each variable has about the other. The intuition for the mathematical expression for MI is as follows. Let us say that $X$ has information (entropy) $H(X)$. But, if we are given the random variable $Y$, its information content decreases (it will not increase) to  $H(X|Y)$. Then, the difference is the amount of information about $X$ that we have obtained from $Y$. So, the MI between $X$ and $Y$ is defined as
$$ I(X;Y) = H(X) - H(Y|X) = H(Y) - H(X|Y) $$&lt;/p&gt;
&lt;p&gt;Now let us briefly look at its properties. As suggested by the expression, the MI is symmetric, i.e. the MS between $X$ and $Y$ is equal to the MI between $Y$ and $X$. After all, we called it &lt;em&gt;mutual&lt;/em&gt; information. Secondly, the MI between two random variables is non-negative. If knowing $Y$ means we know $X$ exactly, $H(X|Y)=0$ (because $X$ is known deterministically, there is no randomness) and the MI is just the information in $X$, i.e. $H(X)$. If $X$ and $Y$ are independent random variables, $H(X|Y) = H(X)$ and the MI is zero, which is what we would expect.&lt;/p&gt;
&lt;p&gt;I must confess, I played a little bait and switch routine over the last few paragraphs. I allured you with the promise of continuous random variables (random vectors, even!), but defined these information-theoretic quantities only for discrete distributions. For continuous random variables, we define these quantities in analogous ways, using the pdf instead of the probability mass functions, although some pleasant properties are lost.&lt;/p&gt;
&lt;p&gt;We are finally ready to discuss the three information-theoretic quality models in question. Let us begin with VIF. VIF assumes a statistical model of the natural source (the GSM model discussed above), a distortion channel which distorts this &amp;lsquo;&amp;lsquo;pristine&amp;rsquo;&amp;rsquo; image, and an additive noise model of the human visual system (HVS). We can illustrate the VIF model as below.&lt;/p&gt;
&lt;div class=&#34;mermaid&#34;&gt;
graph LR;
  A[Source] --&gt; B[HVS]
  B --&gt; C[Receiver]
  A --&gt; D[Distortion Channel]
  D --&gt; E[HVS]
  E --&gt; F[Receiver]
&lt;/div&gt;
&lt;p&gt;So, in line with our source model, let us define the source random variable as a GSM distributed random vector&lt;/p&gt;
&lt;p&gt;$$ C = S \cdot U $$&lt;/p&gt;
&lt;p&gt;The distortion channel is described as having a deterministic scalar gain $g$ and an additive White Gaussian Noise (AWGN) $V \sim \mathcal{N}(0, \sigma_v^2 I)$. So, the distorted random variable is given by&lt;/p&gt;
&lt;p&gt;$$ D = g \cdot C + V $$&lt;/p&gt;
&lt;p&gt;Finally, we model the neural noise of the HVS as an AWGN channel having $N, N&amp;rsquo; \sim \mathcal{N}(0, \sigma_n^2)$. The final received pristine and distorted images are given by&lt;/p&gt;
&lt;p&gt;$$ E = C + N = S \cdot U + N$$
$$ F = D + N&amp;rsquo; = g \cdot S \cdot U + V + N&amp;rsquo; $$&lt;/p&gt;
&lt;p&gt;We condition all quantities on knowing $S$ because knowing $S$ allows us to predict VIF for our particular pair of reference and test images, instead of a general average case. The intuition here is that upon distortion, lesser information about the source is retained in the distorted image. So, the VIF index is defined as
$$ VIF = \frac{I(C; E | S = s)}{I(C; F | S = s)} $$&lt;/p&gt;
&lt;p&gt;Conveniently, conditioning on $S$ results in all RVs becoming Gaussians, whose entropies are easy to compute. For an image having $N$ samples of dimension $d$, if the covariance matrix has eigenvalues $
\lambda_j$, the mutual informations can be calculated easily as
$$ I(C; E | S = s) = \sum\limits_{i=1}^{N}\sum_{j=1}^{d} \log\left(1 + \frac{s_i^2\lambda_j}{\sigma_n^2}\right) $$
$$ I(C; F | S = s) = \sum\limits_{i=1}^{N}\sum_{j=1}^{d} \log\left(1 + \frac{g_i^2 s_i^2 \lambda_j}{\sigma_v^2 + \sigma_n^2}\right) $$&lt;/p&gt;
&lt;p&gt;If you have made it this far, congratulations! You now know what natural scene statistics and information-theoretic quality models look like. We will end by briefly commenting on the implementation details. In practice, both the reference and test images are first transformed into the Wavelet domain using the Steerable Pyramid. The coefficients (which look like filtered images) are collected in 3x3 blocks and modelled as 9-dim vectors $C_i$ and $D_i$ respectively.&lt;/p&gt;
&lt;p&gt;What remains is to find the parameters of the GSM distribution and the distortion channel from these wavelet coefficients. The HVS is assumed to have a known channel noise $ \sigma_n^2 = 0.1 $. Because these parameter estimation details are not a part of the core idea behind VIF, I will just state them without proof.&lt;/p&gt;
&lt;p&gt;$$ \Sigma = \frac{1}{N} \sum C_i C_i^T $$
$$ s_i = \frac{C_i^T \Sigma^{-1} C_i}{9} $$
$$ g_i = \frac{Cov(C, D)}{Cov(D, D)} $$
$$ \sigma_{v,i}^2 = Cov(D, D) - g_i Cov(C, D)$$&lt;/p&gt;
&lt;p&gt;Note that the covariances in the last two equations are scalar covariances calculated in corresponding local neighbourhoods of wavelet coefficients. And there we have it - VIF! Admittedly, this post turned out to be &lt;em&gt;much&lt;/em&gt; longer than I had anticipated. But, it is a good thing because I will write about STRRED and SpEED-QA [5] next, which use these same concepts, but differently. So now, even though you may not realize it, you know enough to understand two more information-theoretic quality models.&lt;/p&gt;
&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;
&lt;p&gt;[1] Hamid R. Sheikh and Alan C. Bovik. Image Information and Visual Quality. &lt;em&gt;IEEE Transactions on Image Processing, 2006&lt;/em&gt; 
&lt;a href=&#34;https://live.ece.utexas.edu/publications/2004/hrs_ieeetip_2004_imginfo.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2] Rajiv Soundararajan and Alan C. Bovik, Video quality assessment by reduced reference spatio-temporal entropic differencing. &lt;em&gt;IEEE Transactions on Circuits and Systems for Video Technology, 2013&lt;/em&gt; 
&lt;a href=&#34;https://www.live.ece.utexas.edu/publications/2013/Rajiv%20%20Video-RRED%20Paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[3] Martin J. Wainwright and Eero P. Simoncelli. Scale Mixtures of Gaussians and the Statistics of Natural Images. &lt;em&gt;Proceedings of the 12th International Conference on Neural Information Processing Systems. 1999&lt;/em&gt; 
&lt;a href=&#34;https://papers.nips.cc/paper/1750-scale-mixtures-of-gaussians-and-the-statistics-of-natural-images.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[4] Eero P. Simoncelli and William T. Freeman. The Steerable Pyramid: A Flexible Architecture For Multi-Scale Derivative Computation. &lt;em&gt;IEEE Conference on Image Processing, 1995&lt;/em&gt; 
&lt;a href=&#34;https://www.cns.nyu.edu/pub/eero/simoncelli95b.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[5] Christos G. Bampis and Praful Gupta and Rajiv Soundararajan and Alan C. Bovik. SpEED-QA: Spatial Efficient Entropic Differencing for Image and Video Quality. &lt;em&gt;IEEE Signal Processing Letters, 2017&lt;/em&gt; 
&lt;a href=&#34;http://www.christosbampis.info/uploads/d4e7f73994efae5148ba6617684696b07b31.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Tourist</title>
      <link>https://nownowk.github.io/post/the-tourist/</link>
      <pubDate>Thu, 01 Mar 2018 00:00:00 +0000</pubDate>
      <guid>https://nownowk.github.io/post/the-tourist/</guid>
      <description>&lt;p&gt;Through a small opening in a coat&lt;/p&gt;
&lt;p&gt;Glinted a shard of iridescent blue.&lt;/p&gt;
&lt;p&gt;The Tourist pulled it closer, clearing her throat,&lt;/p&gt;
&lt;p&gt;To shield from the cold wind that blew&lt;/p&gt;
&lt;p&gt;Sounding a low, ominous whistle&lt;/p&gt;
&lt;p&gt;As if in taunt or admiration.&lt;/p&gt;
&lt;p&gt;The sea, from behind, offered a drizzle&lt;/p&gt;
&lt;p&gt;Urging her to flaunt His generous creation.&lt;/p&gt;
&lt;p&gt;The Tourist moved ahead, steadfast&lt;/p&gt;
&lt;p&gt;Reading from a finger lined with the past.&lt;/p&gt;
&lt;p&gt;She walked through the crowded thoroughfare&lt;/p&gt;
&lt;p&gt;Each standing still with muffled screams&lt;/p&gt;
&lt;p&gt;Albeit with pressed shirts and slicked back hair&lt;/p&gt;
&lt;p&gt;Like a requiem for their forsaken dreams.&lt;/p&gt;
&lt;p&gt;She reached into her coat and chipped off from an edge;&lt;/p&gt;
&lt;p&gt;And offered it to each stationed sentry&lt;/p&gt;
&lt;p&gt;Who stood there as if bound by a pledge,&lt;/p&gt;
&lt;p&gt;As a payment for her entry.&lt;/p&gt;
&lt;p&gt;The tourist entered the city at last&lt;/p&gt;
&lt;p&gt;Having added to the finger lined with the past.&lt;/p&gt;
&lt;p&gt;To her, all these cities looked the same;&lt;/p&gt;
&lt;p&gt;Two stone walls rising on either side&lt;/p&gt;
&lt;p&gt;And yet, deserving of all their fame&lt;/p&gt;
&lt;p&gt;But none offering a place to hide.&lt;/p&gt;
&lt;p&gt;As often before, she was found by a man&lt;/p&gt;
&lt;p&gt;Offering to make her feel whole.&lt;/p&gt;
&lt;p&gt;She placed a fading blue chip on his hand&lt;/p&gt;
&lt;p&gt;Paying her debt with a piece of her soul.&lt;/p&gt;
&lt;p&gt;She walked away, hoping to be alone at last&lt;/p&gt;
&lt;p&gt;Adding again to the finger lined with her past.&lt;/p&gt;
&lt;p&gt;One might wonder, “What use is it?”&lt;/p&gt;
&lt;p&gt;Is the soul too much for the feet to carry?&lt;/p&gt;
&lt;p&gt;Why lose yourself to a cause unfit?”&lt;/p&gt;
&lt;p&gt;To you, I say, it is the contrary.&lt;/p&gt;
&lt;p&gt;Like others, she was born set into motion.&lt;/p&gt;
&lt;p&gt;So while the wind howled through each rift&lt;/p&gt;
&lt;p&gt;She rolled ahead like waves of the ocean&lt;/p&gt;
&lt;p&gt;While denying herself her own weight to lift.&lt;/p&gt;
&lt;p&gt;And with every loss she did outlast&lt;/p&gt;
&lt;p&gt;She added to the finger lined with her past.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Old Man&#39;s Tale</title>
      <link>https://nownowk.github.io/post/the-old-mans-tale/</link>
      <pubDate>Fri, 16 Dec 2016 00:00:00 +0000</pubDate>
      <guid>https://nownowk.github.io/post/the-old-mans-tale/</guid>
      <description>&lt;p&gt;The old gentleman lifted the glass of champagne in his hand&lt;br&gt;
Carrying with it the honour of being the best man,&lt;br&gt;
“To the lovely couple, I would like to raise a toast&lt;br&gt;
And tell you a story that I find most&lt;br&gt;
Entertaining, but it has nothing to do&lt;br&gt;
With our beloved bride or groom.&lt;/p&gt;
&lt;p&gt;Once upon a time, as many times before,&lt;br&gt;
I walked out into a forest that began at my door.&lt;br&gt;
For a troubled mind, it was a beautiful sight,&lt;br&gt;
And my aching feet carried me into the white.&lt;br&gt;
The forest would wrap me up in her trees,&lt;br&gt;
Providing a virgin land of undisturbed peace.&lt;br&gt;
But when the clear day became a smoky screen,&lt;br&gt;
I missed, for once, the colours of red and green.&lt;/p&gt;
&lt;p&gt;Of all the things I could have done,&lt;br&gt;
I abandoned classical notions of “fun”&lt;br&gt;
And chose the path that many dread;&lt;br&gt;
The path that only the vagabonds tread.&lt;br&gt;
The trail was dark, but uncannily pleasant;&lt;br&gt;
Calming, yet chaotic like a raging adolescent.&lt;br&gt;
The moon offered only attention, not love;&lt;br&gt;
She was a lone friend loaned from above.&lt;/p&gt;
&lt;p&gt;I admitted I was lost, as my watch struck eleven.&lt;br&gt;
It was too cold for hell, but it sure wasn&amp;rsquo;t heaven.&lt;br&gt;
The wind carried the call of an unknown beast,&lt;br&gt;
Waiting in the bushes for his Christmas feast.&lt;br&gt;
Finding myself on the ground, I tried to remember why&lt;br&gt;
And before I could look up, something dropped from the sky.&lt;br&gt;
Shivering hard, I closed my eyes shut.&lt;br&gt;
I had come for some peace; this was anything but.&lt;br&gt;
Even so, I could tell it swung around&lt;br&gt;
Waiting, as a forgotten victim, to be found.&lt;br&gt;
&amp;lsquo;That was a man’, my brain tried to reaffirm.&lt;br&gt;
‘Of course it was’. But my eyes wouldn&amp;rsquo;t confirm.&lt;/p&gt;
&lt;p&gt;In spite of the crippling fear and fatigue,&lt;br&gt;
The man&amp;rsquo;s identity was a cause of intrigue.&lt;br&gt;
Maybe he was someone I had met.&lt;br&gt;
Maybe he just couldn’t outrun his debt.&lt;br&gt;
Or a man of great wisdom and might&lt;br&gt;
Brought to the ground on his greatest flight&lt;br&gt;
Unable to share life&amp;rsquo;s sense of humour&lt;br&gt;
And gone at the dawn of the most vile rumour.&lt;br&gt;
Or maybe just a man tired of his woeful existence&lt;br&gt;
Who declined to offer any further resistance.&lt;br&gt;
Or maybe he just took a fateful drink,&lt;br&gt;
Lost his sacred ability to think,&lt;br&gt;
Thought a rope around his neck suited him best&lt;br&gt;
And allowed gravity to do the rest.&lt;/p&gt;
&lt;p&gt;You’d expect me to say I had learnt something by now&lt;br&gt;
About life, its deep meaning and how&lt;br&gt;
It was stupid of me to have stayed on the ground.&lt;br&gt;
But no, I waited till the sun came around.&lt;br&gt;
I opened my eyes as if to look up was a sin&lt;br&gt;
And there he hung, the wax mannequin.”&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>In Search of Swaraj</title>
      <link>https://nownowk.github.io/post/in-search-of-swaraj/</link>
      <pubDate>Wed, 19 Oct 2016 00:00:00 +0000</pubDate>
      <guid>https://nownowk.github.io/post/in-search-of-swaraj/</guid>
      <description>&lt;p&gt;I’ve lived a vast majority of my life in a dominion where a minority (one-fourth) of the population wielded a disproportionate amount of power over the numerical majority. Much like the British did in colonial India. I speak, of course, of the great power bestowed upon my mother, by the unmentioned powers of the universe. Anyone even mildly familiar with display of said power would know that the element of surprise is the (metaphorical) nutrient-rich petri-dish upon which the effect of said display is cultured. And there is nothing more effortlessly middle class, yet vastly effective as the sudden issue of a diktat to get a haircut.&lt;/p&gt;
&lt;p&gt;Before you judge, I am aware of those kids in Africa everyone talks about. I know it is, classically, a more socially relevant issue to write about. But, I haven’t seen any of them being threatened with surprise haircuts, so I reject the comparison. (That was a joke. Please don’t threaten to [or actually] post a status update or start a hashtag. I sympathise with them on a daily basis. Well, not daily, but you get the idea.) Anyway, back to topic.&lt;/p&gt;
&lt;p&gt;The first plan of attack is, of course, retaliation. So, refuse. And then watch them refuse to accept your refusal. And then realise that your plan never factored in history. Never has a puny child’s “no” overpowered that of that singularity of all power (read mom). (“And it will stay that way”, she says.) Back home, school on weekdays meant that Sunday was the day allotted for this last legal form of systematic, and forced, acquisition of personal property (read hair). Invariably, the day of issue would give my brother and me a couple of days to figure out who would go that Sunday and who would go the next. (Side note: All stalemates were dealt with by the age-old method of conflict resolution called “Let’s ask mom”. Side-side note: Who thought that was a good idea?) Being the one with the longer hair never worked in my favour, but I had to hold on to that as it was the last domain that I had any control over. But more on that later.&lt;/p&gt;
&lt;p&gt;So, on average, one gets three days to mentally prepare for the sacrifice. This period is usually characterized by increased mirror usage, new-found fondness for hair products and hallucinations of sounds made by electric razors (Not really, so if you could relate to that last one, you need to go see a doctor. Stat.) This is also the period of bargaining. With my mum about how short I had to get my hair cut. I’m not aware of the professionalism among barbers in other places in India, but where I come from, we use a time tested, scientifically designed scale of “short or medium”. “Medium”, stands for “shorter than you think it will be”, and “short” stands for “Stop crying, you’re not bald yet”. To put it into perspective with the bargain, the difference between the length of your hair in “medium” and “short” is that little personal space you let your parents invade. This is the one part of the whole ordeal I win at. Although it takes a great deal of effort and having to summon all of my skills of parental persuasion, I manage to get away with “medium”, which is the lesser of two evils.&lt;/p&gt;
&lt;p&gt;On that Saturday night, one learns the most important lesson anyone can hope to learn; that life has a twisted sense of humour. By then, I’ve silently resigned to my fate, and even convinced myself that my hair is an unqualified mess and therefore, must be sentenced to the equivalent of capital punishment in the world of bodily produced keratinous filaments. So, that night, I walk towards my bed after a long and emotionally exhausting week and pass by a mirror I had forgotten was even there. Turn around and look at myself in the mirror (I was really tempted to make a “how do you sleep at night” and/or a “how do you look at yourself in the mirror” joke at this point but couldn’t come up with one that was relevant) and my hair is on point. Just perfect. It’s the best it’s looked in forever, and all of that emotional foundation-laying goes to waste. But, I have no choice but to go ahead with it. The next morning, I’d embark on the journey (that one must take alone) to the hallowed ceremonial grounds (read salon) and when I’d reach there the next life lesson would await me.&lt;/p&gt;
&lt;p&gt;Barbers are a sadistic lot. I would, honestly, not be surprised if most psychopathic and sadistic serial killers worked day jobs as barbers. By the time I’ve been led up to the chair, they’ve used their acute senses to determine my willingness (or lack thereof). And, because I’m, well, me, the thinly veiled look of acute depression and loss doesn’t help. They pick their tools up and make to stand behind me. Much like this.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;joker.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;And every time, I can almost hear them think to themselves, “Oh, I’m not gonna shave your head. I’m just gonna cut your hair. Really, really, short.” I know, at this point, all this sounds like I’m reading too much into body language. Well, explain this. For those not familiar with the process, the electric razor is used to trim the hair on the sides of the head first. When that’s done, you reach a state where even looking at that figure staring at you from the mirror in front of you is difficult. Because you look like a sentient mushroom cloud. This is where the psychopathy of the barber kicks in. Right then, at your most vulnerable, he stops. And walks away. For “tea”. And as he sips on his beverage that was, conveniently, delivered just then, he looks across at you trying to take it all in; trying to come to terms with the ramifications of his not coming back. Imagine having to walk out like that! It would mean the complete and utter annihilation of any social, and therefore, self-image you had spent your life painstakingly constructing. Just before you’ve given up any hope of walking out of there without pulling off a Shia LaBeouf (Google: shia labeouf paper bag, for more details), he comes back, now a saviour. With nowhere left to go but up.&lt;/p&gt;
&lt;p&gt;If this is not oppression I don’t know what is. To convince you, I am now going to quote someone famous. Was it not Martin Luther King who said, “I have a dream that my four little children will one day live in a nation where they will not be judged by the length of their hair but by the content of their character.” It wasn’t? Well, it ought to have been. So, I’d like to send a message to all the kids out there who have been through this. Remember that countless lives have been lost to the cause of swaraj and we are not going to let them take it away from us.&lt;/p&gt;
&lt;p&gt;*This article is not, in any way, a deep and convoluted metaphor for any struggles faced by human society. I just like my hair.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Five Stages of Summer</title>
      <link>https://nownowk.github.io/post/the-five-stages-of-summer/</link>
      <pubDate>Sat, 09 Jul 2016 00:00:00 +0000</pubDate>
      <guid>https://nownowk.github.io/post/the-five-stages-of-summer/</guid>
      <description>&lt;p&gt;Because I’ve had a lot of people tell me the worst way to start an article is using a quote, I’m going to do just that.&lt;/p&gt;
&lt;p&gt;“Everything good, everything magical happens between the months of June and August.” Now, before you think I’m some literary savant who knows all quotes by anybody famous, I’d like to confess that I Googled for this after I wrote down the title of the article. And I found it on Buzzfeed, not Goodreads; so you can imagine my literary inclinations. One of the things that struck me about it, though, was that it was tagged under “Summer”. I think it should be tagged under “Guilt”.&lt;/p&gt;
&lt;p&gt;For the less informed, summer vacations at IITH start in May and extend through to the end of July. When you spend the last week of the semester writing tests and assignments, you convince yourself that it’s all worth it for the amazing three months ahead. As I stood at the exit of the hostels waiting for a bus to take me on a 14 hour journey home, I made a commitment to myself. A commitment that I’d use my holidays to do everything I ever wanted to do, because I had finally understood the value of a vacation. “Ever”, obviously, referring to the previous week because that’s the only thing I had a memory of. Who has life goals anyway, right? Thus began my journey to the discovery that the five stages of summer are, actually, the five stages of grief backwards&lt;/p&gt;
&lt;p&gt;It was a week after I sank into a sofa for the first time in two months that I realised that that moment of realisation was truly once-in-a-vacation. Cuz you ain’t gonna have one after you get home. You see, trauma-induced motivation is like that cheap deodorant you bought on a trip because you suddenly realised you needed one. You’re not going back to it once you get home. And I didn’t mind that at all. There it was; stage 1 – Acceptance.&lt;/p&gt;
&lt;p&gt;The first week had passed informing people I was in town (although, I only actually met family in the first six weeks) and catching up on the shows I thought I was tired of, spending time I thought I had nothing to do with. Of course, I was wrong on both counts.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;awesomeness_v_time.jpg&#34; alt=&#34;Awesomeness v Time&#34;&gt;&lt;/p&gt;
&lt;p&gt;As I’ve shown in, what I call, the awesomeness vs time curve up there, week 2 is when you really ease into the routine and start to appreciate it. This was the life, you know. Food laid out on a table, watching Survivor, Masterchef and House on repeat. Especially because Masterchef was on right about lunch time. I didn’t know which meal I was having but it was pretty convenient to be treated like the judges were on TV. And you’ve gotta admit, watching people fight for food on reward challenges really boosts the ego about having it for free.&lt;/p&gt;
&lt;p&gt;Stage 2, depression, began about midway into third week with Survivor going off air, leaving a vacuum that could only be filled in by rational thought. I began to realise what a waste of a day it was to watch other people make a million dollars. The excitement just dropped, and it was all downhill from there. I’d explain in detail, but given that almost all of you who’re reading this have been through JEE, you’d have been acquainted with it at some point. To sum it up, I spent all my time complaining that I couldn’t do the things I wanted to do, because I was too bored, because I had nothing to do. (It made sense at the time)&lt;/p&gt;
&lt;p&gt;Then began the bargaining. I figured if I could spend about an hour a day doing something productive, it’d atone for how I spent the other 23. It went on for a couple of days but I ran out of easy things to do really fast. (That’s when I began this article, by the way) Then, mathematics took over and I figured I didn’t have to be productive all at once, and I could work on something in parts spread out between the TV shows I was addicted to by now, and still do the same amount of work.&lt;/p&gt;
&lt;p&gt;By the end of that week I realised that I wasn’t the only one who noticed I hadn’t been much use to anybody in a while. And the others began to feel they should, at least, make me aware. It was hard to make a case to them because even I knew it was true but I just didn’t want to do much else about it. Couple of days of being annoyed and I realized I was at the anger stage. So I decided to do the first sensible thing I’d done in almost a month and set up for when I’d get some sense knocked into me, because I figured it probably wasn’t that far away anyway.&lt;/p&gt;
&lt;p&gt;As much as it pains the nonconformist in me to admit, I did what I was supposed to do and set up some achievable goals and got myself productive again. Which brings us to the last and final stage – denial. Denial of what, you ask? Nothing much, really. Just that the first four weeks ever happened.&lt;/p&gt;
&lt;p&gt;So, I guess people say moments of redemption are spread far and wide in our lives. But I had one every week; every Sunday before I went to bed (although, technically, that would make it Monday) when I realised I just lost another week to make myself productive. Luckily, third time was a charm. Remember that guilt I wrote about at the beginning? Turns out it’s a potent getter-off-your-butt-er. &lt;em&gt;So&lt;/em&gt; (That’s just me stressing on the word) potent, in fact, that it got me back to finish the second half of this article. Twice. So wherever you are in this process, remember that May sets it up for you and the more guilt you accumulate the better. Because that’ll fuel you through the five stages of summer.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>He Who Derived His Excellence From That Of Others</title>
      <link>https://nownowk.github.io/post/harsha-bhogle/</link>
      <pubDate>Tue, 12 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://nownowk.github.io/post/harsha-bhogle/</guid>
      <description>&lt;p&gt;Ever since the game of cricket began to be telecast to crowds, live commentary has played a great role in bringing the game closer to laypeople. This, they achieved, not just by their knowledge of the game, but sometimes also simply by their choice of how to present a situation to the audience. When Mansur Ali Khan Pataudi was dismissed for one run against England at Bombay in 1973, the commentator, Bobby Talyarkhan remarked on the radio, “Pataudi is out 99 runs short of an expected century!”&lt;/p&gt;
&lt;p&gt;For long, Indians hadn’t been considered great commentators. But then came one man who, at his peak, had played for his college and was best appreciated for getting off strike when he batted. And he took Indian cricket commentary to new heights.&lt;/p&gt;
&lt;p&gt;Harsha Bhogle started out at the age of nineteen, at the All India Radio. He became the first Indian to be invited by the Australian Broadcasting Corporation to cover India’s tour of Australia before the 1992 World Cup.  The catch was that the ABC would not pay the foreign presenter, and it was up to the home broadcasting stations to pay the presenter. And the AIR did not even know he went to Australia. But being signed did not mean respect and that is probably the thing he handled best. Instead of competing against other cricketer-turned-commentators, he adopted a different style of commentary. A non-technical flavour, and that captivated the audience like no one had before.&lt;/p&gt;
&lt;p&gt;He gained the support and respect of active cricketers at that time, because he made them feel comfortable during their interviews. And for someone who is constantly being judged on his performance and worth, there is nothing more refreshing than being spoken to, not as authority, but as an admirer.&lt;/p&gt;
&lt;p&gt;In 2005, he visited his alma mater and gave a pretty cool speech on excellence. And his opening line went something like this. “I am here, not as someone who has achieved excellence in life, but as someone who has seen excellence first-hand” It is this kind of modesty that made him the most popular cricket commentator in the world. And as there weren’t many “great” presenters at that time, he sought inspiration from the cameramen and technicians he worked with and tried to emulate the kind of perfection they brought to their professions.&lt;/p&gt;
&lt;p&gt;And he had a brilliant way of putting things. During the recently concluded Cricket World Cup, when playing against India, AB de Villiers missed a rather simple run out chance. While the others were ruing the missed opportunity or criticizing the undue risk taken by the Indian batsmen, Harsha Bhogle chose to see the positive side. “The one good thing we can all take from this run out is that AB de Villiers has proved to the world that he too is human and that he too can make mistakes” Rewind back to a warm up game before the World Cup. The Indian Team dropped a considerable number of catches in that match. Once again, optimism had not seen a more faithful patron, till that night. “It’s okay. The Indians can drop as many as they want now and let the law of averages catch up.” In another ODI game, Sachin Tendulkar and MS Dhoni were playing at the end of the innings. Dhoni had dismissed the ball to all sides of the park. Getting in on the act, Sachin caressed one to the boundary. In all this mayhem, Harsha Bhogle’s voice was the only one that left the commentary box. “We have a surgeon at one end and a butcher at the other”.&lt;/p&gt;
&lt;p&gt;Test games are known for their long-drawn out nature, and it is in such situations that the role of a commentator doubles up as an entertainer to keep the readers hooked to the game. In a game against India, Alastair Cook had scored one run in 52 balls. The next ball, he scored a boundary. Seizing the opportunity, Bhogle remarked, “80% of his runs came off that one ball!” His commentary wasn’t limited to what happened on the field at that time. Speaking about the legend that is Sachin Tendulkar, he said. “Eruption of joy at an Indian wicket can only mean one thing – that Sachin is next to the crease.” And how good is a commentator who focuses on just his home country? When Misbah-ul-Haq came under criticism from players like Shoaib Akthar and other Pakistani players, he commented, “I find this criticism of Misbah very strange. It is like a family of ten complaining that the sole breadwinner isn’t doing enough. Misbah is rated far higher outside Pakistan than within. Afridi is rated much higher in Pakistan that outside!” Sometimes, even the most optimistic of people must bow down to reality. And Harsha Bhogle did so gracefully. When Ian Chappell asked him if Narendra Hirwani can bat, he said, “If you make a team with all the No.11s of all the teams, Narendra Hirwani would still be the No.11.” On Rahul Dravid being bowled on his last international appearance, “In a career that is marked by grace, style and beautiful batsmanship, it’s a slog that’s ended Rahul Dravid‘s career. But once again, it was what THE TEAM needed.”&lt;/p&gt;
&lt;p&gt;Starting out as an outlaw in a profession dominated by former cricketers, he went on to carve a niche for himself, applying one cardinal rule he claimed to have learnt from journalism. “Don’t let the truth get in the way of a good story.”. Everywhere he went, he was consistently asked one question; “How many games have you played?” Initially determined to prove himself as one of them, he later realized that there were too many of “them” anyway, and that he shouldn’t try to “examine Sachin Tendulkar’s cover drive” or “explain the art of reverse swing with Wasim Akram beside me in the box”.&lt;/p&gt;
&lt;p&gt;And I think the success of Harsha Bhogle lies in that moment of realization. He embraced his lack of technical knowledge and replaced it with his supreme presenting skills, and that has made all the difference.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Man Who Sits Under The Banyan Tree</title>
      <link>https://nownowk.github.io/post/banyan-tree/</link>
      <pubDate>Sat, 15 Aug 2015 00:00:00 +0000</pubDate>
      <guid>https://nownowk.github.io/post/banyan-tree/</guid>
      <description>&lt;p&gt;There was once a man, quite old was he,&lt;br&gt;
He spent his day under a Banyan tree.&lt;br&gt;
Men would come, and men would go&lt;br&gt;
Up to twilight, from the cock’s crow.&lt;br&gt;
They came from all walks of life,&lt;br&gt;
Seemingly struck by mental strife.&lt;br&gt;
When they left, they left with a smile,&lt;br&gt;
No wonder he had people waiting in a file.&lt;br&gt;
And I wonder, “So knowledgeable is he,&lt;br&gt;
The Man Who Sits Under The Banyan Tree?”&lt;/p&gt;
&lt;p&gt;And where would he spend the night in?&lt;br&gt;
For he didn’t seem to have any next of kin.&lt;br&gt;
Back then, I was new to the city,&lt;br&gt;
And so, all this was quite the mystery.&lt;br&gt;
I asked around, and answers I got,&lt;br&gt;
Which were contrary to what I thought.&lt;br&gt;
And I mused, “What a fool he must be,&lt;br&gt;
The Man Who Sits Under The Banyan Tree?&amp;rdquo;&lt;/p&gt;
&lt;p&gt;As he was an imbecile, he was thrown&lt;br&gt;
Out to the streets, by his very own.&lt;br&gt;
The rest of it is an urban lore,&lt;br&gt;
Bottom line being, he found a door&lt;br&gt;
That fed him well, and made him gain&lt;br&gt;
In health, only to get back on the streets again.&lt;br&gt;
And I ask myself, “What do people see,&lt;br&gt;
In The Man Who Sits Under The Banyan Tree?”&lt;/p&gt;
&lt;p&gt;Atop a pedestal, he sat on a bed,&lt;br&gt;
And so, I couldn’t listen to what he said.&lt;br&gt;
But, I wanted to see what pulled such a crowd&lt;br&gt;
Which made the whole street buzz aloud.&lt;br&gt;
After pushing, pulling, a minor tussle&lt;br&gt;
And two hours of being asked to hustle,&lt;br&gt;
I got to where he was, eventually,&lt;br&gt;
To meet The Man Who Sits Under The Banyan Tree.&lt;/p&gt;
&lt;p&gt;“All my life, Sir, what have I done?”&lt;br&gt;
“You have done a good job, my son.”&lt;br&gt;
So, I told myself, “This man, here,&lt;br&gt;
He must be a great seer.”&lt;br&gt;
And then, I realized, for whatever I said,&lt;br&gt;
“You’ve done a good job, son; go ahead.”&lt;br&gt;
Looking back - Irrespective of the crowd there,&lt;br&gt;
The locals never did seem to care.&lt;br&gt;
So, let me give you some advice for free,&lt;br&gt;
Don’t fall for The Man Who Sits Under The Banyan Tree.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
