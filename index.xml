<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Abhinau Kumar</title>
    <link>https://abhinaukumar.github.io/</link>
      <atom:link href="https://abhinaukumar.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Abhinau Kumar</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Thu, 10 Dec 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://abhinaukumar.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Abhinau Kumar</title>
      <link>https://abhinaukumar.github.io/</link>
    </image>
    
    <item>
      <title>Film HDR Videos to Support My Research!</title>
      <link>https://abhinaukumar.github.io/hdr-videos-call/</link>
      <pubDate>Sat, 11 Dec 2021 00:00:00 +0100</pubDate>
      <guid>https://abhinaukumar.github.io/hdr-videos-call/</guid>
      <description>&lt;h4 id=&#34;update-now-accepting-samsung-phones-in-addition-to-iphones-clarifying-that-videos-must-be-in-landscape-mode-and-updated-duration-requirements-to-20-seconds-from-10-seconds-please-read-the-text-for-more-details&#34;&gt;UPDATE: Now accepting Samsung phones in addition to iPhones, clarifying that videos must be in landscape mode, and updated duration requirements to 20 seconds from 10 seconds. Please read the text for more details.&lt;/h4&gt;
&lt;p&gt;Hello! My name is Abhinau and I am a Graduate Research Assistant at UT Austin. I am conducting a study to understand the impact of tone-mapping on the quality of High Dynamic Range (HDR) videos. Simply put, HDR videos are capable of capturing a wide range of brightness (think bright clouds and shadows at the same time) and colors, in comparison to the usual Standard Dynamic Range (SDR) videos. However, the vast majority of displays currently in use are only capable of displaying SDR content. As a result, HDR videos must be “down-converted” to SDR using a process called tone-mapping, thereby “degrading” their quality. The goal of this study is to understand and model these effects.&lt;/p&gt;
&lt;p&gt;As part of the study, I will be creating a database of tone-mapped videos, obtained from a set of HDR video clips. It is to collect this set of HDR clips that I am reaching out to you for your help. If you own a phone capable of recording HDR video (e.g. iPhones 12 and 13, and Samsung Galaxy S10 and S10+), please consider signing up 
&lt;a href=&#34;https://docs.google.com/forms/d/e/1FAIpQLSe10jSGzzbRick_mr61cSHH6yWCsyLKvUaBoquIeNtYFG_QhQ/viewform&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; to record and submit any number of HDR video clips. Since each clip is expected to be around 10 seconds in length, this will likely not be a great demand on your time. Having said that, because we want to isolate the degradation in quality due to tone-mapping, please note that the video clips will need to be “pristine”, and follow certain guidelines. In any case, prior to filming, ensure that HDR video recording is turned on (
&lt;a href=&#34;https://support.apple.com/en-in/guide/iphone/iph2cafe2ebc/ios&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;iPhone&lt;/a&gt;, 
&lt;a href=&#34;https://www.samsung.com/us/support/answer/ANS00086003/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Samsung&lt;/a&gt;).&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Please be sure to film either moving objects (like people walking around, trees shaking in the wind, etc.) or pan the camera slowly as you film. While it is important to avoid rapid camera movements, since they lead to motion blur, videos with almost no motion are not particularly useful since they are not very different from images. Aim to strike a balance so that the video contains motion but no motion blur.&lt;/li&gt;
&lt;li&gt;Film at a resolution of 1080p (landscape mode) and the highest available frame rate. The duration of the video must be at least 20 seconds long. Filming at the highest available framerate reduces the chances of motion blur. Filming at higher resolutions than 1080p is okay, but the video will be downscaled to 1080p at our end before any processing.&lt;/li&gt;
&lt;li&gt;Finally, follow these instructions to export your videos if you are using an 
&lt;a href=&#34;https://support.apple.com/en-us/HT211950&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;iPhone&lt;/a&gt; (using iMovie may be the easiest choice). If you are using a Samsung phone, share the video from the &amp;ldquo;My Files&amp;rdquo; app. In both cases, do not share directly from the gallery/photos app - this will not export an HDR video. Furthermore, if you are given the option, export at the highest possible quality setting - do not compress! If you are not given any option regarding quality, follow the usual steps.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In addition to being &amp;ldquo;pristine&amp;rdquo;, we would like to ensure that the content of the video is such that it highlights the strengths of HDR, i.e., it exhibits a wide range of brightness (i.e., video contains both bright and dark areas) and/or is colorful. To this end, you may find the following content suggestions helpful.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Colorful scenes are preferred - this includes lights, trees, flowers, etc.&lt;/li&gt;
&lt;li&gt;Popular choices of HDR videos include nature/sceneries such as hills, a beautiful sunrise/sunset, trees in the fall.&lt;/li&gt;
&lt;li&gt;Night-time scenes with colorful lights, such as city walkthroughs are another popular choice.&lt;/li&gt;
&lt;li&gt;Snow in winter would be another great choice, since snow is very bright. It is also a good opportunity to capture colorful winter clothes, winter/Christmas lights, etc.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For examples of HDR videos, see 
&lt;a href=&#34;https://www.youtube.com/watch?v=N1-Jmq7BLFE&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; and 
&lt;a href=&#34;https://www.youtube.com/watch?v=0nTO4zSEpOs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;; several others are available on YouTube. Note that, unlike these examples, the audio that is captured along with your videos will be completely ignored, since we are interested in purely the visual quality. Finally, should you submit one or more videos to the study, we will ask that you sign an agreement allowing us to freely distribute them to the public. So, please do not submit any private, proprietary, or confidential videos.&lt;/p&gt;
&lt;p&gt;If you have any questions, please feel free to contact me at 
&lt;a href=&#34;mailto:abhinaukumar@utexas.edu&#34;&gt;abhinaukumar@utexas.edu&lt;/a&gt;. I hope you will spend a small amount of your time supporting this line of research. Our findings will directly help in developing the next generation of video processing tools for delivering high-quality videos, making your contributions immensely valuable!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Analyzing Model Tradeoffs in Predicting Length of Stay (LOS) in eICU Patients</title>
      <link>https://abhinaukumar.github.io/post/eicu-tradeoffs/</link>
      <pubDate>Thu, 10 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://abhinaukumar.github.io/post/eicu-tradeoffs/</guid>
      <description>&lt;p&gt;Patients with prolonged stay often account for high resource consumption [1], and so, are a potential loss of revenue from a hospital management perspective. This is especially important in the case of Intensive Care Unit (ICU) and electronic ICU (eICU) patients due to their intensive need for care resources like nurses, drugs, surgeons, X-ray machines, etc., for continuous monitoring.  Data mining and predicting/projecting the remaining length of stay (rLOS) for a patient will help allocate resources efficiently and provide timely services to patients.&lt;/p&gt;
&lt;p&gt;The challenge for developing such a predictive model arises from the incomplete understanding of the complex clinical factors that may be involved and their interactions or relationships that lead to a specific LOS. This is a potential application for neural networks, which are considered universal function approximators. Since we are handling time-series data in the form of patient records, we mainly use CNNs and RNNs (and related architectures) to predict rLOS.&lt;/p&gt;
&lt;p&gt;In addition to having a predictive model, an accurate understanding of the factors that are closely associated with LOS can allow for efficient hospital resource management, savings to a healthcare system, and better patient care by reducing readmissions. In this regard, neural networks fall short, as they are not interpretable. This “black box” behavior of neural networks leads us to the first tradeoff that we consider in this blog – performance vs. interpretability.&lt;/p&gt;
&lt;p&gt;Also, the dataset is sensitive for the patients and must be protected. All countries have laws to protect privacy, leading to difficulties in the data collection phase. Many hospitals may refuse to share healthcare data without any Health Insurance Portability and Accountability Act (HIPAA) agreements or data governance in place. Therefore, we also investigated a Federated Learning framework to address such privacy issues.  This distributed learning framework leads us to the second tradeoff – performance vs. privacy security.&lt;/p&gt;
&lt;p&gt;This post is part of my term project for the Data Mining (EE 380L) class with Nishamathi Kumaraswamy, Zhengzhong Tu, Parveen Kumari, and Huancheng Chen who are all students of UT Austin.&lt;/p&gt;
&lt;h1 id=&#34;the-eicu-database&#34;&gt;The eICU Database&lt;/h1&gt;
&lt;p&gt;The eICU Collaborative Research Database [2] is a multi-center intensive care unit database with high granularity data for over 200,000 admissions to ICUs monitored by eICU programs across the United States. The database is publicly available 
&lt;a href=&#34;https://physionet.org/content/eicu-crd/2.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;, and comprises 200,859 patient unit encounters for 139,367 unique patients admitted between 2014 and 2015 to 208 hospitals located throughout the US.&lt;/p&gt;
&lt;p&gt;The database is de-identified and includes vital sign measurements, care plan documentation, severity of illness measures, diagnosis information, treatment information, and more. The data is publicly available after registration, including completion of a training course on research with human subjects and signing of a data use agreement mandating responsible handling of the data and adhering to the principle of collaborative research. We would like to thank PhysioNet for granting accesss to the eICU database, which forms the backbone of all of our experiments.&lt;/p&gt;
&lt;h1 id=&#34;preprocessing-the-data&#34;&gt;Preprocessing the Data&lt;/h1&gt;
&lt;p&gt;To clean and preprocess the data, we use the same procedure used in [3]. To clean the data, we use several exclusion criteria.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;We first only consider patients who are “adults,” i.e., patients who are between 18 and 89 years old.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We only consider patients who have one ICU stay on record because we are interested in ICU length of stay, and we want to avoid ambiguity.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We exclude patients who have less than 15 or more than 200 records to keep the length of inputs significant but not excessive.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We exclude patients whose gender or hospital/ICU discharge status (alive vs. expired) are unknown.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Finally, we only include records that lie between the times of admission and discharge, which corresponds to having positive offset and rLOS.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As a side note, we use the term “record” to refer to a set of 21 features collected at a time step, which is called an “offset.” After cleaning the data, we bin the offsets into hours. Within each hour, we consider the first record, and we impute its missing values with the average value over the hour. Often, features are missing over the entire hour, so we will need to impute values again at the hour level. We will discuss this “sparsity” of the database in further detail below. To handle these missing values, we follow the literature and use “typical” values of each feature.&lt;/p&gt;
&lt;p&gt;After cleaning and binning the data, we have a total of 74686 patients, leading to a total of just over 3.1 million records. We then split the data randomly to use 80% of the patients for training and 20% for testing. It is important to note that the records of each patient are assigned either to the training or the test set, but not both.&lt;/p&gt;
&lt;p&gt;We have several categorical variables, and some can take several values. For example, admission diagnosis can take almost 400 different values. So, we cannot use the one-hot encoding to represent these variables without adding a large number of features.&lt;/p&gt;
&lt;p&gt;Instead, we handle categorical variables by a method called “Target Encoding.” In target encoding, each value of a categorical variable is replaced by the average value of the target variable corresponding to all examples which have this value. More formally, the value $v$ of a categorical variable $X_j$ is replaced by&lt;/p&gt;
&lt;p&gt;$$E[Y | X_j = v]$$&lt;/p&gt;
&lt;p&gt;In some cases, we may see new values of the categorical variable in the test data that we have not encountered. In this case, we default to using the average value of the target variable instead of such a conditional average.&lt;/p&gt;
&lt;p&gt;In our code, we created a class TargetEncoder to carry this out, which adheres to the Scikit-Learn API format. So, in practice, target encoding variables in a dataset is as simple as&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;encoder = TargetEncoder()
encoder.fit(X, y, [&#39;column1&#39;, &#39;column2&#39;, &#39;column3&#39;])
X = encoder.transform(X)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we applied a “Min-Max Scaler” which maps the values of each feature to the range $[-1, 1]$. This kind of scaling helps improve the convergence of many machine learning models, especially those that use gradient-based optimization methods.&lt;/p&gt;
&lt;h1 id=&#34;understanding-the-data&#34;&gt;Understanding the Data&lt;/h1&gt;
&lt;h2 id=&#34;sparsity&#34;&gt;Sparsity&lt;/h2&gt;
&lt;p&gt;Before we begin training the models, it is useful to understand the nature of the database. As we have mentioned above, a notable feature of the database is its sparsity. This is not surprising because not every feature is recorded at each time step. Often, the frequency at which, say, oxygen saturation is measured may be higher than that at which blood glucose level or pH are measured. This is because oxygen saturation is easier to measure, and is not a diagnostic test specific to any disease.&lt;/p&gt;
&lt;p&gt;In the following figure, we show the fraction of missing values of each feature.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;missing_barplot.png&#34; alt=&#34;Fraction of missing values&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this plot, we plot the histogram of the number of missing features in each record.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;row_missing_histogram.png&#34; alt=&#34;Number of missing features in each record&#34;&gt;&lt;/p&gt;
&lt;p&gt;From these two plots, it’s clear that the dataset is quite sparse, with almost most records missing at least three features. Also, the features that are missing the most often are the pH, glucose, and FiO2, which are likely measured only for specific patients, depending on the necessity. So, we don’t expect them to be missing completely at random (MCAR), and we cannot omit any records or features. As a result, imputation becomes a crucial data processing step. Also, because the data is time-series, imputing using the mean or median, which are common choices, can give undue weight to patients who have more records. So, to avoid these complex considerations, using “typical” values is a good practical choice.&lt;/p&gt;
&lt;h2 id=&#34;visualizing-features-and-targets&#34;&gt;Visualizing Features and Targets&lt;/h2&gt;
&lt;p&gt;After target encoding, we have converted all variables into numerical variables, so we can visualize their dependencies using the correlation plot below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;corrplot.png&#34; alt=&#34;Pairwise correlation between features&#34;&gt;&lt;/p&gt;
&lt;p&gt;Looking at the plot, it is not surprising that the height and the weight are highly correlated. Another interesting observation is that the features obtained from the Glasgow Coma Score (GCS) suite - GCS Total, Eyes, Verbal, Motor are all highly correlated. It is important to note that these features were originally categorical variables, so these correlations are over a small set of unique values. Other than these, the variables are weakly correlated, so we can expect that they provide diverse information about the LOS/rLOS targets.&lt;/p&gt;
&lt;p&gt;To round out the data visualization exercise, we show the histogram of lengths of stays below. Since most values lie in the range 0-20, we restrict the histogram to this range.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;los_histogram.png&#34; alt=&#34;Histogram of LOS (in days)&#34;&gt;&lt;/p&gt;
&lt;p&gt;In the histogram, we can notice several local modes corresponding to integer values of LOS. We think this shows a natural bias of hospitals to discharge patients in whole numbers of days, perhaps due to administrative convenience.&lt;/p&gt;
&lt;h2 id=&#34;looking-deeper&#34;&gt;Looking Deeper&lt;/h2&gt;
&lt;p&gt;Beyond these summaries, we want to look a little deeper into the data. Specifically, we are interested in finding the effect of gender and ethnicity on the LOS and the discharge status - alive vs. expired. Such an analysis will help us uncover biases (conscious or otherwise) in the healthcare system that need to be addressed.&lt;/p&gt;
&lt;p&gt;To find the effect of gender on the LOS, we will compare the average LOS between male and female patients. Visually, we can illustrate the values in the form of the following box plot.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;gender_los_boxplot.png&#34; alt=&#34;LOS by gender&#34;&gt;&lt;/p&gt;
&lt;p&gt;Analytically, because we want to compare the means of two groups, we will use a statistical test called the “independent t-test.” We will skip the details here, but 
&lt;a href=&#34;https://conjointly.com/kb/statistical-student-t-test/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this&lt;/a&gt; blog post is a good resource to learn about the t-test. The t-test returned a “p-value” of about $10^{-3}$. Loosely speaking, a low p-value means that our observations are very unlikely if the means were the same. So, we say that the difference in the means is “significant,” i.e., the mean LOS in ICUs depends on gender. We see from the data that men have a larger average LOS, and from the box plot, we see that men are also more likely to have abnormally high LOS when compared to women.&lt;/p&gt;
&lt;p&gt;We can ask the same question of ethnicity. The dataset contains patients of the following ethnicities - Asian, African American, Caucasian, Hispanic, Native American, Other/Unknown. Once again, we can visualize the data using a box plot.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;ethnicity_los_boxplot.png&#34; alt=&#34;LOS by ethnicity&#34;&gt;&lt;/p&gt;
&lt;p&gt;But now that we have more than two groups, we use an F-test (also called a one-way analysis of variance), which compares the means of several groups. The p-value returned by the F-test was $10^{-17}$, so once again, the test is “significant,” and the mean LOS varies with ethnicity. From the data, we see that African-American patients have the longest stays, while Native Americans have the shortest. We also see from the box plot that Caucasian patients are more likely to have abnormally high LOS when compared to Asian, Hispanic, or Native American patients.&lt;/p&gt;
&lt;p&gt;To investigate the effect of gender on discharge status, we can tabulate the number of men and women who were discharged alive or expired. Such a table is called a “contingency table.”&lt;/p&gt;
&lt;style&gt;
.basic-styling td,
.basic-styling th {
  border: 1px solid #999;
  padding: 0.5rem;
}
&lt;/style&gt;
&lt;div class=&#34;ox-hugo-table basic-styling&#34;&gt;
&lt;div&gt;&lt;/div&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Alive&lt;/th&gt;
&lt;th&gt;Expired&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Female&lt;/td&gt;
&lt;td&gt;31943 (94%)&lt;/td&gt;
&lt;td&gt;2054 (6%)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Male&lt;/td&gt;
&lt;td&gt;38146 (93.8%)&lt;/td&gt;
&lt;td&gt;2543 (6.2%)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
Analytically, we want to test if the discharge status depends on gender. Because we are testing the dependence between two categorical variables, we use a $\chi^2$ test of indepence. For a $\chi^2$ test, the p-value is the probability of observing the data if the two quantities (gender and discharge status, in our case) are independent. So, a low p-value means that the two quantities are dependent. The p-value returned by the $\chi^2$ test was 0.245, which means that we did not find a significant dependence between discharge status and gender.
&lt;p&gt;As above, we can compute a similar contingency table to analyze the effect of ethnicity on discharge status.&lt;/p&gt;
&lt;style&gt;
.basic-styling td,
.basic-styling th {
  border: 1px solid #999;
  padding: 0.5rem;
}
&lt;/style&gt;
&lt;div class=&#34;ox-hugo-table basic-styling&#34;&gt;
&lt;div&gt;&lt;/div&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Ethnicity / Discharge Status&lt;/th&gt;
&lt;th&gt;Alive&lt;/th&gt;
&lt;th&gt;Expired&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Unknown/Other&lt;/td&gt;
&lt;td&gt;4075 (93.3%)&lt;/td&gt;
&lt;td&gt;291 (6.7%)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Asian&lt;/td&gt;
&lt;td&gt;1136 (94.3%)&lt;/td&gt;
&lt;td&gt;69 (5.7%)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;African American&lt;/td&gt;
&lt;td&gt;7667 (94.5%)&lt;/td&gt;
&lt;td&gt;442 (5.4%)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Caucasian&lt;/td&gt;
&lt;td&gt;54007 (93.7%)&lt;/td&gt;
&lt;td&gt;3609 (6.3%)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Hispanic&lt;/td&gt;
&lt;td&gt;2818 (94.9%)&lt;/td&gt;
&lt;td&gt;151 (5.1%)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Native American&lt;/td&gt;
&lt;td&gt;386 (91.7%)&lt;/td&gt;
&lt;td&gt;35 (8.3%)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Applying a $\chi^2$ test again, the p-value that we obtained was $10^{-3}$, which means that the test was significant; the discharge status does depend on ethnicity. From the contingency table, we see that Native Americans are significantly more likely to die in ICUs than patients of other ethnicities. This may explain the shorter ICU stays.&lt;/p&gt;
&lt;p&gt;A more detailed exploratory analysis of the dataset can be found in [4].&lt;/p&gt;
&lt;h1 id=&#34;related-work&#34;&gt;Related Work&lt;/h1&gt;
&lt;p&gt;Our proposed work follows the footsteps of other researchers who answer similar problems using the same, or similar, data. Harutyunyan et al. [5] trained a multi-task RNN model on the MIMIC-III [6] database to perform&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;In-hospital mortality prediction, based on the first 48 hours of ICU stay&lt;/li&gt;
&lt;li&gt;Decompensation prediction, as an early-warning system to predict the rapid
deterioration of the patient over the next 24 hours&lt;/li&gt;
&lt;li&gt;Length of stay prediction&lt;/li&gt;
&lt;li&gt;Phenotype classification, classifying which of 25 acute care conditions are present in the patient’s record&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The use of a multi-task model leads to significant feature reuse and a reduction in the total computational load during inference.&lt;/p&gt;
&lt;p&gt;Rajabalizadeh et al. [4] conducted an exploratory analysis of the eICU dataset, quantifying the distribution of frequencies of visits, co-presence of diagnoses, racial bias, etc. They also show that the Acute Physiology Age Chronic Health Evaluation (APACHE) III system does not predict LOS well.&lt;/p&gt;
&lt;p&gt;Sheikhalishahi et al. [3] proposed a Bidirectional Long Short Term Memory (BiLSTM) model to perform the same four tasks. An important difference between the two models, aside from being trained on different databases, is that Sheikhalishahi et al. use a single-task BiLSTM, while Rajabalizadeh et al. use a multi-task RNN model.&lt;/p&gt;
&lt;p&gt;Other than unsupervised and supervised methods, reinforcement learning (RL) models have also been developed using the eICU database. Komorowski et al. Komorowsksi et al. [7] trained an RL agent to learn an optimal “clinician policy,” to reduce the mortality risk of patients. The agent was trained and validated using the MIMIC-III database, while the eICU database was used for model testing.&lt;/p&gt;
&lt;h1 id=&#34;the-models-we-tested&#34;&gt;The Models We Tested&lt;/h1&gt;
&lt;p&gt;We can broadly divide the different trained models into three broad categories, baseline, advanced, and federated models. The baseline models consist of standard regression models like decision trees, linear regression, and support vector regressors, and ensemble models built on these base regressors.&lt;/p&gt;
&lt;p&gt;Among the advanced models, we have Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and an interpretable RNN called the Reverse Time Attention (RETAIN) model.&lt;/p&gt;
&lt;p&gt;We use the RETAIN and Federated Learning models to understand the tradeoffs in performance when enforcing interpretability and privacy, respectively. We report the performance of all our models using three metrics - the Mean Absolute Error (MAE), the (Root) Mean Squared Error (RMSE / MSE), and the $R^2$ score. While the MAE and the (R)MSE are self-explanatory, the $R^2$ score warrants explanation.&lt;/p&gt;
&lt;p&gt;The $R^2$ score is the fraction of the variance in targets that is explained by the regression model. So, if the predictions are perfect, all the variance in the data is explained by the model, and the $R^2$ score achieves its maximum value of 1. If the model is naive and predicts the mean value of the target, irrespective of input, none of the variation is explained by the model and the $R^2$ score is 0. Note that the $R^2$ score can be negative, which means that the model performs worse than a naive constant-prediction model.&lt;/p&gt;
&lt;h2 id=&#34;baseline-models&#34;&gt;Baseline Models&lt;/h2&gt;
&lt;p&gt;We considered the following baseline regression models in our experiments.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Decision trees&lt;/li&gt;
&lt;li&gt;Regularized linear models (Ridge, LASSO)&lt;/li&gt;
&lt;li&gt;Support Vector Machines (SVM)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We also considered the following ensemble models&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Random forest regressor (RF)&lt;/li&gt;
&lt;li&gt;Extreme gradient boosting (XGB)&lt;/li&gt;
&lt;li&gt;Light gradient boosting machines (LGBM).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We chose this set of models because of their simplicity and the ease of output interpretation. Many of these models are also commonly used in the bio-sciences and medical communities, thus providing a robust comparison of the tradeoffs we explore in this blog. We will explain and compare these models in subsequent sections.&lt;/p&gt;
&lt;h3 id=&#34;decision-trees-dt&#34;&gt;Decision Trees (DT)&lt;/h3&gt;
&lt;p&gt;The main idea behind Decision Trees (DTs) is to create a model that predicts the target variable (rLOS or LOS) by learning simple decision rules deduced from the 21 data features. Three models (each with a different set of inputs) were explored with decision trees. We will call them BMDT1, BMDT2, and BMDT3 for purposes of model tracking in this blog.&lt;/p&gt;
&lt;p&gt;We used the Scikit-Learn library module &lt;code&gt;Decision Tree Regressor&lt;/code&gt; to build our DT regressors. The default criterion of &amp;ldquo;mean square error&amp;rdquo; was used as the objective. This criterion minimizes the L2 loss when using the mean from each terminal node.&lt;/p&gt;
&lt;p&gt;We trained the BMDT1 model on the entire database to predict the rLOS, considering each record as an independent sample. We also trained an optimized version of this model, where the best parameters were found using a grid search.&lt;/p&gt;
&lt;p&gt;The parameter ranges for grid search were varied based on the $R^2$ results achieved from predicting the training data ground truth values. The BMDT1 model was the worst-performing DT model based on our comparison metrics ($R^2$ on test data). We also used this model to test for feature robustness by using features without target encoding or feature scaling.&lt;/p&gt;
&lt;p&gt;To train the BMDT2 models, we used either the first or the last record of each patient as a proxy for the whole time-series data. The BMDT2 models also differ from BMDT1 because they predict the LOS instead of the rLOS.&lt;/p&gt;
&lt;p&gt;We modify the BMDT2 models by using the number of records (offsets) for a patient as an input feature, instead of the offset itself. This engineered feature is a way for baseline models to include some information about the time-series portion of the data. We call this model BMDT3.&lt;/p&gt;
&lt;p&gt;It is natural to expect that that offset of the last record is very indicative of the LOS of the patient. So, we would expect the baseline models that were trained on the last record to perform very well. We will show the impact of using the last record, in our results.&lt;/p&gt;
&lt;h3 id=&#34;more-baselines-ridge-lasso-svm-rf-xgb-lgbm&#34;&gt;More Baselines (Ridge, Lasso, SVM, RF, XGB, LGBM)&lt;/h3&gt;
&lt;p&gt;Until now, we have just tried various features and different training methods using the decision tree regressor as an example. Here we branch out to try more popular baseline regression models to get a sense of how these regressors perform on this dataset. These models include 
&lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ridge&lt;/a&gt;, 
&lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lasso&lt;/a&gt;, 
&lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SVM&lt;/a&gt;, 
&lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RF&lt;/a&gt;, 
&lt;a href=&#34;https://xgboost.readthedocs.io/en/latest/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;XGB&lt;/a&gt;, and 
&lt;a href=&#34;https://lightgbm.readthedocs.io/en/latest/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LGBM&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We used the best two groups of features we observed in decision trees - one is to predict the LOS using the features of the first record and an additional feature, the number of records, for each patient; another one is to predict the LOS using the last record (including the last offset as the 21st feature) of features.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Linear Models&lt;/strong&gt;: A linear model fits a multiple linear regression with coefficents $w=(w_1,…,w_p)$ to minimize the residual sum of squares, $||Xw-y||^2_2$, between the observed targets in the dataset and the predictions. Ridge is a variant of multiple linear regression that imposes an $L_2$ penalty on the size of the coefficents: $||Xw-y||^2_2+\alpha||w||^2_2$. On the other hand, Lasso imposes an $L_1$ penalty on the size of the coeffiicents: $||Xw-y||^2_2+\alpha||w||_1$. As a result, Lasso produces sparser results as compared to Ridge, i.e., it typically has a higher number of zero coefficients.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Support Vector Regression&lt;/strong&gt;: SVM is a maximum-margin method that only penalizes the points that lie closest to the decision boundary of the classifier. SVR is an extension of SVM to the regression problem, where the model depends only on a subset of the training data because the cost function ignores samples whose prediction is close to their target. Since SVR with non-linear kernels scales poorly with data and feature dimension, we stuck to a Linear SVR.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Random Forest&lt;/strong&gt;: RF is an ensemble of many decision trees that are built from bootstrap samples from the training set. A bootstrap sample is obtained by randomly sampling the training set with replacement.
Another layer of randomness is introduced by fitting each node during the construction of a tree on a random subset of features. RF has been shown to have very good generalization properties compared to many other models.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;XGBoost&lt;/strong&gt;: XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible, and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. XGBoost scales well to run on major distributed environments like Hadoop, SGE, and MPI and can solve problems beyond billions of examples.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Light GBM&lt;/strong&gt;: LightGBM is a gradient boosting framework that uses tree-based learning algorithms. The main advantages of LightGBM are faster training speed, higher efficiency, lower memory usage, better accuracy, support of parallel and GPU learning, and the capability to handle large-scale data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Hyperparameter tuning&lt;/strong&gt;: The performance of regressors is greatly dependent on their hyperparameters. So, we adopted a randomized grid search cross-validation on the training set to search for the best hyperparameters. However, there is no standard on how to select the parameter range for each regression model. We referred to both official 
&lt;a href=&#34;https://scikit-learn.org/stable/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scikit-learn&lt;/a&gt; documents and unofficial blogs to get an empirical parameter search space for each model. The final parameter search spaces are as follows:
Ridge:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;param_grid = {&#39;alpha&#39;: np.logspace(-3, 3, 10)}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lasso:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;param_grid = {&#39;alpha&#39;: np.logspace(-5, 0, 10)}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;LinearSVR&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;param_grid = {&#39;C&#39;: np.logspace(-5, 15, 15, base=2)}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;RF&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;param_grid = {&#39;n_estimators&#39;: [100, 200, 300, 400, 500],
                &#39;max_features&#39;: [&#39;auto&#39;, &#39;sqrt&#39;],
                &#39;max_depth&#39;: [3, 4, 5, 6, 7, 9],
                &#39;min_samples_split&#39;: [2, 5, 10, 15],
                &#39;min_samples_leaf&#39;: [1, 2, 5],
                &#39;bootstrap&#39;: [True, False]}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;XGBoost&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;param_grid = {&#39;max_depth&#39;: range(3,12),
                &#39;min_child_weight&#39;: range(1,10),
                &#39;gamma&#39;: list([i/10.0 for i in range(0,5)]),
                &#39;subsample&#39;: list([i/10.0 for i in range(6,10)]),
                &#39;colsample_bytree&#39;: list([i/10.0 for i in range(6,10)]),
                &#39;reg_alpha&#39;:[1e-5, 1e-2, 0.1, 1, 100]}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;LightGBM&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;param_grid = {&#39;num_leaves&#39;: [7, 15, 31, 61, 81, 127],
                    &#39;max_depth&#39;: [3, 4, 5, 6, 7, 9, 11, -1],
                   &#39;learning_rate&#39;: [0.001, 0.005, 0.01, 0.05, 0.1, 0.3, 0.5],
                   &#39;n_estimators&#39;: [100, 200, 300, 400, 500],
                   &#39;boosting_type&#39;: [&#39;gbdt&#39;, &#39;dart&#39;],
                   &#39;class_weight&#39;: [None, &#39;balanced&#39;],
                   &#39;min_child_samples&#39;: [10, 20, 40, 60, 80, 100, 200],
                   &#39;subsample&#39;: [0.5, 0.7, 0.8, 0.9, 1.0],
                   &#39;reg_alpha&#39;:[1e-5, 1e-2, 0.1, 1, 10, 100],
                   &#39;reg_lambda&#39;: [1e-5, 1e-2, 0.1, 1, 10, 100]}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;convolutional-neural-networks-cnns&#34;&gt;Convolutional Neural Networks (CNNs)&lt;/h2&gt;
&lt;p&gt;It is well known that CNNs are highly noise-resistant models, and they can extract very informative features that capture local contexts. While CNNs have typically been used in image processing and computer vision, in this section, we will examine in detail how we can apply a 1-D CNN to time-series data.&lt;/p&gt;
&lt;p&gt;At the first layer, the convolution kernels (windows) always have the same depth as the number of features in time series, while their width can be varied. This way, the kernel moves in one direction from the beginning of a time series towards its end, performing convolution. In other words, we mapped each feature to a channel of the convolution layer.&lt;/p&gt;
&lt;p&gt;Each element of the kernel is multiplied by the corresponding element of the time-series input that it covers at a given point. The results of this multiplication are added together, and a nonlinear activation function (like ReLU) is applied to the value. The resulting value becomes an element of a new “filtered” time series, and the kernel moves forward along the time series to produce the next value. The depth of the new “filtered” time series is the same as the number of convolution kernels. Depending on the width of the kernel, different aspects, properties, and “features” of the initial time series get captured in each of the new filtered series.&lt;/p&gt;
&lt;p&gt;The baseline model is a Multi-Layered Perceptron (MLP), which processes only one record at a time. Point-wise convolutions compute higher-level features from interactions in the feature domain.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;cnn1.png&#34; alt=&#34;Pointwise convolutions&#34;&gt;&lt;/p&gt;
&lt;p&gt;We include temporal information by processing all the records corresponding to a patient together and having wider kernel sizes. In these temporal models, wider kernels compute interaction features from the feature set at each time step.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;cnn2.png&#34; alt=&#34;Temporal convolutions&#34;&gt;&lt;/p&gt;
&lt;p&gt;We report the results for the following models.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Kernel size = 1 for all layers&lt;/li&gt;
&lt;li&gt;Kernel size = 3 for all layers&lt;/li&gt;
&lt;li&gt;Kernel size = 15 for all layers&lt;/li&gt;
&lt;li&gt;Variable kermel size: Kernel size = 3 for the first two layers and 15 for the last layer&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;recurrent-neural-networks-rnns&#34;&gt;Recurrent Neural Networks (RNNs)&lt;/h2&gt;
&lt;p&gt;While CNNs can extract features based on neighboring records in time, they do not truly address the time-series nature of the data. Also, they are unable to extract long-range dependencies between inputs. Recurrent Neural Networks (RNNs) were designed to handle such time-series data and have found great success, especially in the field of natural language processing (NLP).&lt;/p&gt;
&lt;p&gt;The basic unit that makes a neural network “recurrent” is called a recurrent layer. There are several types of recurrent layers, but we can describe a general recurrent layer by a function that satisfies the relationship,
$$v_t, c_t = RNN(x_t, v_{t-1}, c_{t-1})$$&lt;/p&gt;
&lt;p&gt;In simple terms, the output (also called the hidden state) $v$ and “cell state” $c$ at every time instant is a function of the current input, the previous output, and the previous cell state. We can illustrate this as&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;general_rnn.png&#34; alt=&#34;General RNN block&#34;&gt;&lt;/p&gt;
&lt;p&gt;$v_{t-1}$ and $c_{t-1}$ themselves can be written in terms of the previous outputs. So,
$$v_t, c_t = RNN(x_t, f(x_{t-1}, v_{t-2}, c_{t-2})) = RNN(x_t, f(x_{t-1}, f(x_{t-2}, v_{t-3}, c_{t-3}))) \dots $$&lt;/p&gt;
&lt;p&gt;In RNN literature, this is called “unrolling” because we can represent this using the block diagram&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;general_rnn_unrolled.png&#34; alt=&#34;Unrolled RNN block&#34;&gt;&lt;/p&gt;
&lt;p&gt;which is an “unrolled” version of the previous figure. We can repeat this process endlessly, so is it turtles all the way down? Not really. In practice, we can assume that the input $x$ starts at t = 1, and initialize $v_0 = c_0 = 0$ for simplicity. This choice is arbitrary. You could use 0, 1, $\pi$, $e$, $e^\pi$, or anything else, depending on how creative you feel. Another common choice for the “initial state” $(v_0, c_0)$ is random noise, which adds a layer of stochasticity to the model.&lt;/p&gt;
&lt;p&gt;There is some 
&lt;a href=&#34;https://r2rt.com/non-zero-initial-states-for-recurrent-neural-networks.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;interesting evidence&lt;/a&gt; that shows that learning the initial state like a parameter can lead to an improvement in performance. This can be taken a step further by predicting the hidden state as a function of the input. Such models are called “Contextual” RNNs [8]&lt;/p&gt;
&lt;p&gt;Those with a background in Digital Signal Processing (DSP) might find the above expression for a general recurrent layer familiar. The time-domain description of an Infinite Impulse Response (IIR) filter
$$y[t] = \sum_{k=0}^{p} b_k x[n-k] - \sum_{k=1}^{q} a_k y[n-k] $$
resembles the description of a kind of “linear” recurrent layer, when $p = q = 1$. In time series analysis, this is called the Auto-Regressive Moving Average (ARMA) model, where the $\sum a_k y[n-k]$ is the AR part, and $\sum b_k x[n-k]$ is the MA part. In practice, however, recurrent layers are often non-linear in nature, to build more complex time-series models.&lt;/p&gt;
&lt;p&gt;While many recurrent layers exist, we used the Long Short Term Memory (LSTM) cell. The LSTM is one of the most popular choices of recurrent layers, due to its ability to capture long-range dependencies.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;lstm_block.png&#34; alt=&#34;LSTM Block&#34;&gt;
&lt;em&gt;Source: 
&lt;a href=&#34;https://www.researchgate.net/figure/Block-diagram-of-the-recurrent-module-of-an-LSTM-network_fig1_319235738&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ResearchGate&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;A detailed explanation of LSTMs warrants a blog post of its own, of which 
&lt;a href=&#34;https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this&lt;/a&gt; is a fantastic example. The broad idea is that the line that runs through the LSTM offers a pathway for information from the past to be passed to the future easily. This gives the RNN cell a sort of “memory,” and the cell state is modified by the input at every time step. Finally, the output is a function of the cell state and the input. The pointwise multiplications are interpreted as “gates” that decide the amount of “attention” (in the range $[0, 1]$ or $[-1, 1]$) given to each feature.&lt;/p&gt;
&lt;h2 id=&#34;reverse-time-attention-models-retain&#34;&gt;Reverse Time Attention Models (RETAIN)&lt;/h2&gt;
&lt;p&gt;The Reverse Time Attention (RETAIN) model is an interpretable alternative to RNNs which was proposed in [9]. In this blog, we will look at a simplified version of the model, which worked better than the one proposed in the paper. We will also investigate the effect of the direction of time.&lt;/p&gt;
&lt;p&gt;Specifically, we will feed the input to both the traditional RNN and the RETAIN model in three ways - forward, which is the standard form of the input, backward, by reversing the order of records in the input, and bidirectional (we call this BiRETAIN), by feeding in both the forward and reverse directions. Nevertheless, we will call all these models “RETAIN” as they are based on the original RETAIN architecture.&lt;/p&gt;
&lt;p&gt;The RETAIN model can be illustrated by the figure&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;retain_arch.png&#34; alt=&#34;RETAIN Model&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we had mentioned in the previous section, a core concept of RNNs is “gating”, which is achieved by “attention”. RETAIN provides an interpretable model by explicitly finding record-level (corresponding to each time step) and feature-level (corresponding to each feature at a time step) attention values, as a function of the “embedding” v, which is a linear function of the input features. As with all attention, these values are multiplied pointwise to the values of the embedding, and each gated embedding is mapped to the output, which is the rLOS in our case. Formally, we can write these as the following set of equations
$$v_t = W_{emb}x_t + b_{emb}, \quad 1 \leq t \leq T$$&lt;/p&gt;
&lt;p&gt;That is, the embedding at each time step is a linear (technically, affine) function of the input features. This is equivalent to a single layer perceptron with no non-linear activation.&lt;/p&gt;
&lt;p&gt;$$z_{\alpha1} \dots z_{\alpha T} = RNN_\alpha(v_1 \dots v_T) $$
$$z_{\beta1} \dots z_{\beta T} = RNN_\beta(v_1 \dots v_T)$$&lt;/p&gt;
&lt;p&gt;These two networks obtain feature vectors which are used to compute&lt;/p&gt;
&lt;p&gt;$$\alpha_t = \sigma(w_\alpha^Tz_{\alpha t} + b_\alpha)$$
$$\beta_t = \tanh(W_\beta z_{\beta t} + b_\beta)$$&lt;/p&gt;
&lt;p&gt;The $\sigma$ and $\tanh$ are the sigmoid and tanh activations, which respectively map inputs to the range $[0,1]$ and $[-1, 1]$. These $\alpha_t$ and $\beta_t$ values are our attention values. We can now multiply these pointwise with the embeddings, to obtain the gated embeddings&lt;/p&gt;
&lt;p&gt;$$\tilde{v}_t = \alpha_t \beta_t \odot v_t$$&lt;/p&gt;
&lt;p&gt;where $\odot$ is called the “Hadamard product” which is just a fancy term for element-wise multiplication.&lt;/p&gt;
&lt;p&gt;Finally, the gated embedding is mapped to the output using the linear (again, technically, affine) mapping
$$y_t = w^T \tilde{v_t} + b$$&lt;/p&gt;
&lt;p&gt;This final mapping is just linear regression, but now, the features $\tilde{v_t}$ are learned from data. We will show an even stronger relationship with linear regression when we discuss the interpretability of RETAIN.&lt;/p&gt;
&lt;p&gt;As mentioned earlier, a neat implementation trick that can be very useful is that we can apply an MLP (of which a linear function is a very special case) to a set of points using 1D convolutions. We can do this by mapping the features to the channels of the convolution and using a kernel size of 1. Using this trick, we can avoid looping over inputs to apply the MLP, which can be very inefficient.&lt;/p&gt;
&lt;p&gt;Now, we look at the most interesting part of RETAIN - how do we interpret the predictions? As hinted earlier, the interpretability of RETAIN comes from its relationship to linear regression. To understand this connection, let us take another look at the equations that describe RETAIN.&lt;/p&gt;
&lt;p&gt;Substituting the value of the gated embeddings,
$$y_t = w^T (\alpha_t \beta_t \odot v_t) + b$$&lt;/p&gt;
&lt;p&gt;We can expand this further by writing the embeddings as a function of inputs, i.e.
$$
\begin{aligned}
y_t &amp;amp;= w^T (\alpha_t \beta_t \odot (W_{emb} x_t + b_{emb})) + b \\
&amp;amp;= (\alpha_t (\beta_t \odot w)^T W_{emb}) x_t + (\alpha_t (\beta_t \odot w)^T b_{emb} + b)
\end{aligned}
$$
Even though the math looks complicated, we have basically written $y_t$ as a function of the form
$$y_t = \tilde{w_t}^T x_t + \tilde{b_t}$$&lt;/p&gt;
&lt;p&gt;This resembles the linear regression formulation, so we can interpret the coefficients $\tilde{w_t}$ as weights given to each feature. So, the RETAIN model is a method by which the weights of a linear regression model are predicted as a function of the input, and these weights represent the contribution of each feature.&lt;/p&gt;
&lt;h2 id=&#34;federated-learning&#34;&gt;Federated Learning&lt;/h2&gt;
&lt;p&gt;Standard machine learning approaches like the baseline model and the interpretable models require centralizing the training data on one machine or in a data center. Because our dataset is from the healthcare area, we need to consider the important problems of protecting patients’ privacy and the business secrets of hospitals. To address this, we use a Federated Learning [10] model for distributed learning, which does not require collecting all data in a center. So, the data providers do not need to share their data, making the training process more secure.&lt;/p&gt;
&lt;p&gt;In our federated learning model, we assume that our eICU data originated from different hospitals. To simulate this, we divided our original dataset into $k$ sub-datasets, representing $k$ hospitals. We called these hospitals clients in the rest of the blog. As the figure shows, all the clients train their models locally on their sub-datasets. In every epoch, all the clients can get their local model, and they only upload the models instead of data to the server. The server integrates all the local models into the global model and then sends it back to all clients. All the clients update their models to the updated global model and search for better weights. Because all the clients only share their models instead of original data, federated learning addresses privacy problems.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;federated_learning.png&#34; alt=&#34;Federated Learning&#34;&gt;
&lt;em&gt;Source: 
&lt;a href=&#34;https://developer.nvidia.com/blog/federated-learning-clara/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NVIDIA&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;However, it is important to note that the federated learning model has the following  disadvantages:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Training local models with sub-datasets will decrease the accuracy of the global model because the global model does not use whole data to search for optimal weights of the network. Instead, the local models are merged in a weighted way, which influences the accuracy of the global model.&lt;/li&gt;
&lt;li&gt;If differences between local models trained on sub-datasets are large, the global model suffers from the worst local model.&lt;/li&gt;
&lt;li&gt;Communication between clients, where the local models reside, and the server, where the global model resides, takes a significant amount of time.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Nevertheless, we are interested in using federated learning to introduce a privacy constraint to model building. In this experiment, we divided the original training set into five equal-sized parts. For comparison, we chose BiLSTM and BiRETAIN models as our baseline models. These two models have the lowest RMSE and highest $R^2$ among all the models in previous sections, making them good candidates for illustrating the effect of the federated learning model.&lt;/p&gt;
&lt;h3 id=&#34;iid-data-group&#34;&gt;IID data group&lt;/h3&gt;
&lt;p&gt;In this section, we randomly divide the training set into five parts. So, all the features in these sub-training sets have similar distributions, which helps reduce the variation in performance between local models.  With these five independent and identically distributed (IID) training sets, we have two ways of training the local models:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Full training&lt;/strong&gt;: At every epoch, we train all the local models and update the weights of the global model with these five models.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Partial training&lt;/strong&gt;: At every epoch, we randomly choose two of the five local models. Then, we only train these models and update the weights of the global model using these two.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;non-iid-data-group&#34;&gt;Non-IID data group&lt;/h3&gt;
&lt;p&gt;In this section, we divide the training set into five equal-sized parts according to three features - height, weight, and age. As a result, the features in these sub-training sets have different distributions. The criteria used to construct the sub-training sets are shown below.&lt;/p&gt;
&lt;style&gt;
.basic-styling td,
.basic-styling th {
  border: 1px solid #999;
  padding: 0.5rem;
}
&lt;/style&gt;
&lt;div class=&#34;ox-hugo-table basic-styling&#34;&gt;
&lt;div&gt;&lt;/div&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Features/Group&lt;/th&gt;
&lt;th&gt;Group A&lt;/th&gt;
&lt;th&gt;Group B&lt;/th&gt;
&lt;th&gt;Group C&lt;/th&gt;
&lt;th&gt;Group D&lt;/th&gt;
&lt;th&gt;Group E&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Height (cm)&lt;/td&gt;
&lt;td&gt;100 - 160&lt;/td&gt;
&lt;td&gt;161 - 167&lt;/td&gt;
&lt;td&gt;168 - 173&lt;/td&gt;
&lt;td&gt;174 - 180&lt;/td&gt;
&lt;td&gt;181 - 218&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Weight (kg)&lt;/td&gt;
&lt;td&gt;30 - 65&lt;/td&gt;
&lt;td&gt;66 - 75&lt;/td&gt;
&lt;td&gt;76 - 86&lt;/td&gt;
&lt;td&gt;87 - 102&lt;/td&gt;
&lt;td&gt;103 - 272&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Age (years)&lt;/td&gt;
&lt;td&gt;18 - 53&lt;/td&gt;
&lt;td&gt;54 - 64&lt;/td&gt;
&lt;td&gt;65 - 71&lt;/td&gt;
&lt;td&gt;72 - 78&lt;/td&gt;
&lt;td&gt;79 - 89&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;For convenience, we only use the partial training strategy on non-IID data.&lt;/p&gt;
&lt;h1 id=&#34;results&#34;&gt;Results&lt;/h1&gt;
&lt;h2 id=&#34;decision-trees&#34;&gt;Decision Trees&lt;/h2&gt;
&lt;p&gt;Below are the optimized parameter values for the best decision tree.&lt;/p&gt;
&lt;center&gt;
&lt;style&gt;
.basic-styling td,
.basic-styling th {
  border: 1px solid #999;
  padding: 0.5rem;
}
&lt;/style&gt;
&lt;div class=&#34;ox-hugo-table basic-styling&#34;&gt;
&lt;div&gt;&lt;/div&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Parameter&lt;/th&gt;
&lt;th&gt;Value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;DT Maximum depth&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DT Max leaf nodes&lt;/td&gt;
&lt;td&gt;200&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DT Max sample leaf&lt;/td&gt;
&lt;td&gt;100&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DT Min samples split&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/center&gt;
&lt;p&gt;We found that the optimized decision tree that used the last record of each patient performed the best with an $R^2$ value of 0.96 as shown in the table below.&lt;/p&gt;
&lt;center&gt;
&lt;style&gt;
.basic-styling td,
.basic-styling th {
  border: 1px solid #999;
  padding: 0.5rem;
}
&lt;/style&gt;
&lt;div class=&#34;ox-hugo-table basic-styling&#34;&gt;
&lt;div&gt;&lt;/div&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;MAE&lt;/th&gt;
&lt;th&gt;MSE&lt;/th&gt;
&lt;th&gt;$R^2$&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;BMDT1&lt;/td&gt;
&lt;td&gt;rLOS&lt;/td&gt;
&lt;td&gt;1.696&lt;/td&gt;
&lt;td&gt;6.944&lt;/td&gt;
&lt;td&gt;-0.834&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;BMDT1 – Optimized&lt;/td&gt;
&lt;td&gt;rLOS&lt;/td&gt;
&lt;td&gt;1.237&lt;/td&gt;
&lt;td&gt;3.46&lt;/td&gt;
&lt;td&gt;0.084&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;BMDT2 – Optimized (first record)&lt;/td&gt;
&lt;td&gt;LOS&lt;/td&gt;
&lt;td&gt;1.268&lt;/td&gt;
&lt;td&gt;3.779&lt;/td&gt;
&lt;td&gt;0.092&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;BMDT2 – Optimized (last record)&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;LOS&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;0.072&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;0.150&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;0.964&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;BMDT3 – Optimized (first record and # of offsets)&lt;/td&gt;
&lt;td&gt;LOS&lt;/td&gt;
&lt;td&gt;0.552&lt;/td&gt;
&lt;td&gt;1.603&lt;/td&gt;
&lt;td&gt;0.615&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;BMDT3 – Optimized  (last record and # of offsets)&lt;/td&gt;
&lt;td&gt;LOS&lt;/td&gt;
&lt;td&gt;0.584&lt;/td&gt;
&lt;td&gt;1.671&lt;/td&gt;
&lt;td&gt;0.599&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/center&gt;
&lt;h2 id=&#34;overall-baseline-model-performance&#34;&gt;Overall Baseline model Performance&lt;/h2&gt;
&lt;p&gt;Using the first record plus the number of offsets for each patient to predict LOS, the performance is as follows.&lt;/p&gt;
&lt;center&gt;
&lt;style&gt;
.basic-styling td,
.basic-styling th {
  border: 1px solid #999;
  padding: 0.5rem;
}
&lt;/style&gt;
&lt;div class=&#34;ox-hugo-table basic-styling&#34;&gt;
&lt;div&gt;&lt;/div&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;MAE&lt;/th&gt;
&lt;th&gt;MSE&lt;/th&gt;
&lt;th&gt;$R^2$&lt;/th&gt;
&lt;th&gt;Time (sec)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Decision Tree&lt;/td&gt;
&lt;td&gt;0.552&lt;/td&gt;
&lt;td&gt;1.603&lt;/td&gt;
&lt;td&gt;0.615&lt;/td&gt;
&lt;td&gt;0.222&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Ridge&lt;/td&gt;
&lt;td&gt;0.623&lt;/td&gt;
&lt;td&gt;1.735&lt;/td&gt;
&lt;td&gt;0.583&lt;/td&gt;
&lt;td&gt;0.008&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Lasso&lt;/td&gt;
&lt;td&gt;0.623&lt;/td&gt;
&lt;td&gt;1.735&lt;/td&gt;
&lt;td&gt;0.583&lt;/td&gt;
&lt;td&gt;0.016&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;LinearSVR&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;0.492&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1.942&lt;/td&gt;
&lt;td&gt;0.533&lt;/td&gt;
&lt;td&gt;1.483&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;RF&lt;/td&gt;
&lt;td&gt;0.559&lt;/td&gt;
&lt;td&gt;1.598&lt;/td&gt;
&lt;td&gt;0.616&lt;/td&gt;
&lt;td&gt;50.591&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;XGBoost&lt;/td&gt;
&lt;td&gt;0.537&lt;/td&gt;
&lt;td&gt;1.521&lt;/td&gt;
&lt;td&gt;0.634&lt;/td&gt;
&lt;td&gt;5.914&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;LightGBM&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;0.533&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;1.500&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;0.639&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;9.239&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/center&gt;
&lt;p&gt;From this table, we can see that using the first record plus the number of offsets for each patient, we can get reasonable performance. The best model in terms of MAE is the LinearSVR, while in terms of MSE and $R^2$, LightGBM ranks first. We also observed that linear models trained very fast, while tree-based models were generally slow.&lt;/p&gt;
&lt;p&gt;Using the last record for each patient to predict LOS, we obtain the performance&lt;/p&gt;
&lt;center&gt;
&lt;style&gt;
.basic-styling td,
.basic-styling th {
  border: 1px solid #999;
  padding: 0.5rem;
}
&lt;/style&gt;
&lt;div class=&#34;ox-hugo-table basic-styling&#34;&gt;
&lt;div&gt;&lt;/div&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;MAE&lt;/th&gt;
&lt;th&gt;MSE&lt;/th&gt;
&lt;th&gt;$R^2$&lt;/th&gt;
&lt;th&gt;Time (sec)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Decision Tree&lt;/td&gt;
&lt;td&gt;0.072&lt;/td&gt;
&lt;td&gt;0.150&lt;/td&gt;
&lt;td&gt;0.964&lt;/td&gt;
&lt;td&gt;0.204&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Ridge&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;0.057&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;0.010&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;0.997&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;0.014&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Lasso&lt;/td&gt;
&lt;td&gt;0.057&lt;/td&gt;
&lt;td&gt;0.011&lt;/td&gt;
&lt;td&gt;0.997&lt;/td&gt;
&lt;td&gt;0.013&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;LinearSVR&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;0.052&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;0.011&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;0.997&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;7.017&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;RF&lt;/td&gt;
&lt;td&gt;0.058&lt;/td&gt;
&lt;td&gt;0.024&lt;/td&gt;
&lt;td&gt;0.994&lt;/td&gt;
&lt;td&gt;62.924&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;XGBoost&lt;/td&gt;
&lt;td&gt;0.059&lt;/td&gt;
&lt;td&gt;0.016&lt;/td&gt;
&lt;td&gt;0.996&lt;/td&gt;
&lt;td&gt;3.634&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;LightGBM&lt;/td&gt;
&lt;td&gt;0.074&lt;/td&gt;
&lt;td&gt;0.073&lt;/td&gt;
&lt;td&gt;0.982&lt;/td&gt;
&lt;td&gt;0.581&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/center&gt;
From this table, we see that using the last record with the time offset, all the models achieved nearly perfect predictions. Specifically, the Ridge and Linear SVR models performed the best in terms of MAE and MSE, respectively, and both models achieved the highest $R^2$.
&lt;h2 id=&#34;convolutional-neural-networks&#34;&gt;Convolutional Neural Networks&lt;/h2&gt;
&lt;p&gt;The results of our experiments with baseline MLP and CNNs having various kernel sizes are shown below.&lt;/p&gt;
&lt;center&gt;
&lt;style&gt;
.basic-styling td,
.basic-styling th {
  border: 1px solid #999;
  padding: 0.5rem;
}
&lt;/style&gt;
&lt;div class=&#34;ox-hugo-table basic-styling&#34;&gt;
&lt;div&gt;&lt;/div&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;MAE&lt;/th&gt;
&lt;th&gt;RMSE&lt;/th&gt;
&lt;th&gt;R2&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Baseline MLP&lt;/td&gt;
&lt;td&gt;1.35&lt;/td&gt;
&lt;td&gt;2.14&lt;/td&gt;
&lt;td&gt;-0.02&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Kernel size = 1&lt;/td&gt;
&lt;td&gt;1.19&lt;/td&gt;
&lt;td&gt;1.97&lt;/td&gt;
&lt;td&gt;-0.03&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Kernel size = 3&lt;/td&gt;
&lt;td&gt;1.08&lt;/td&gt;
&lt;td&gt;1.873&lt;/td&gt;
&lt;td&gt;0.073&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Kernel size = 15&lt;/td&gt;
&lt;td&gt;1.41&lt;/td&gt;
&lt;td&gt;1.99&lt;/td&gt;
&lt;td&gt;-0.05&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Variable kernel size&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;0.97&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;1.74&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;0.19&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/center&gt;
&lt;p&gt;From these results, we can make the following observations. The most complex model, which has a kernel of size 15 performs the worst. So there is some gap between our intuition and how the CNNs behave with this kind of dataset.&lt;/p&gt;
&lt;p&gt;Also, while CNNs are computationally less intensive as compared to RNNs, they perform poorly because the features extracted from the time-series data are shallow, in the sense that only closely localized relationships between a few neighbors are factored into the feature representations.&lt;/p&gt;
&lt;h2 id=&#34;recurrent-neural-networks-and-retain&#34;&gt;Recurrent Neural Networks and RETAIN&lt;/h2&gt;
&lt;p&gt;The performance of various RNN and the corresponding RETAIN models have been listed in the table below.&lt;/p&gt;
&lt;center&gt;
&lt;style&gt;
.basic-styling td,
.basic-styling th {
  border: 1px solid #999;
  padding: 0.5rem;
}
&lt;/style&gt;
&lt;div class=&#34;ox-hugo-table basic-styling&#34;&gt;
&lt;div&gt;&lt;/div&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;MAE&lt;/th&gt;
&lt;th&gt;RMSE&lt;/th&gt;
&lt;th&gt;R2 Score&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;LSTM&lt;/td&gt;
&lt;td&gt;1.166&lt;/td&gt;
&lt;td&gt;1.927&lt;/td&gt;
&lt;td&gt;0.019&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;LSTM (Reversed)&lt;/td&gt;
&lt;td&gt;0.149&lt;/td&gt;
&lt;td&gt;0.325&lt;/td&gt;
&lt;td&gt;0.972&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;BiLSTM&lt;/td&gt;
&lt;td&gt;0.108&lt;/td&gt;
&lt;td&gt;0.316&lt;/td&gt;
&lt;td&gt;0.973&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;RETAIN&lt;/td&gt;
&lt;td&gt;1.169&lt;/td&gt;
&lt;td&gt;1.927&lt;/td&gt;
&lt;td&gt;0.019&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;RETAIN (Reversed)&lt;/td&gt;
&lt;td&gt;0.157&lt;/td&gt;
&lt;td&gt;0.383&lt;/td&gt;
&lt;td&gt;0.961&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;BiRETAIN&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;0.085&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;0.255&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;0.983&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/center&gt;
&lt;p&gt;From this table, we see that the two bidirectional models outperform all other models, and the BiRETAIN model, in particular, performs the best. This makes sense because the bidirectional models can capture dependencies in both forward and reversed time. More importantly, all RETAIN models achieve a similar performance as their corresponding RNN counterparts. This shows that we can offer interpretability without incurring any cost in performance.
As explained above, we interpret the RETAIN model by examining the coefficients of the effective linear regression model. We visualize the change in the coefficients over time for an example from the test set below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;biretain_interpretation.png&#34; alt=&#34;Interpreting the RETAIN model&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the figure, we see that the weights of the offset and the admission diagnosis change rapidly, while the other weights remain approximately constant. This makes sense because the admission diagnosis may become less important as time progresses, and the change in rLOS over time is fuelled by the offset.&lt;/p&gt;
&lt;h2 id=&#34;federated-learning-1&#34;&gt;Federated Learning&lt;/h2&gt;
&lt;h3 id=&#34;performance-on-iid-data&#34;&gt;Performance on IID Data&lt;/h3&gt;
&lt;style&gt;
.basic-styling td,
.basic-styling th {
  border: 1px solid #999;
  padding: 0.5rem;
}
&lt;/style&gt;
&lt;div class=&#34;ox-hugo-table basic-styling&#34;&gt;
&lt;div&gt;&lt;/div&gt;
&lt;div class=&#34;table-caption&#34;&gt;
&lt;l&gt;&lt;b&gt;Baseline&lt;/b&gt;&lt;/l&gt;
&lt;/div&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Time (min.sec)&lt;/th&gt;
&lt;th&gt;MAE&lt;/th&gt;
&lt;th&gt;MSE&lt;/th&gt;
&lt;th&gt;$R^2$&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;BiLSTM&lt;/td&gt;
&lt;td&gt;3.24&lt;/td&gt;
&lt;td&gt;0.108&lt;/td&gt;
&lt;td&gt;0.316&lt;/td&gt;
&lt;td&gt;0.973&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;BiRETAIN&lt;/td&gt;
&lt;td&gt;7.48&lt;/td&gt;
&lt;td&gt;0.085&lt;/td&gt;
&lt;td&gt;0.255&lt;/td&gt;
&lt;td&gt;0.983&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;style&gt;
.basic-styling td,
.basic-styling th {
  border: 1px solid #999;
  padding: 0.5rem;
}
&lt;/style&gt;
&lt;div class=&#34;ox-hugo-table basic-styling&#34;&gt;
&lt;div&gt;&lt;/div&gt;
&lt;div class=&#34;table-caption&#34;&gt;
&lt;l&gt;&lt;b&gt;IID - Full Training&lt;/b&gt;&lt;/l&gt;
&lt;/div&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Time (min.sec)&lt;/th&gt;
&lt;th&gt;MAE&lt;/th&gt;
&lt;th&gt;MSE&lt;/th&gt;
&lt;th&gt;$R^2$&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;BiLSTM&lt;/td&gt;
&lt;td&gt;4.27&lt;/td&gt;
&lt;td&gt;0.2442&lt;/td&gt;
&lt;td&gt;0.3947&lt;/td&gt;
&lt;td&gt;0.9605&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;BiRETAIN&lt;/td&gt;
&lt;td&gt;8.55&lt;/td&gt;
&lt;td&gt;0.1791&lt;/td&gt;
&lt;td&gt;0.3757&lt;/td&gt;
&lt;td&gt;0.9654&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;style&gt;
.basic-styling td,
.basic-styling th {
  border: 1px solid #999;
  padding: 0.5rem;
}
&lt;/style&gt;
&lt;div class=&#34;ox-hugo-table basic-styling&#34;&gt;
&lt;div&gt;&lt;/div&gt;
&lt;div class=&#34;table-caption&#34;&gt;
&lt;l&gt;&lt;b&gt;IID - Partial Training&lt;/b&gt;&lt;/l&gt;
&lt;/div&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Time (min.sec)&lt;/th&gt;
&lt;th&gt;MAE&lt;/th&gt;
&lt;th&gt;MSE&lt;/th&gt;
&lt;th&gt;$R^2$&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;BiLSTM&lt;/td&gt;
&lt;td&gt;1.45&lt;/td&gt;
&lt;td&gt;0.2970&lt;/td&gt;
&lt;td&gt;0.3516&lt;/td&gt;
&lt;td&gt;0.9437&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;BiRETAIN&lt;/td&gt;
&lt;td&gt;3.40&lt;/td&gt;
&lt;td&gt;0.2644&lt;/td&gt;
&lt;td&gt;0.3903&lt;/td&gt;
&lt;td&gt;0.9307&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;From the tables above, we can see the federated learning model with IID sub-datasets doesn’t affect the performance of the baseline models much.&lt;/p&gt;
&lt;p&gt;Under the full-training regime, the training time for BiLSTM and BiRETAIN federated models is considerably more (about 30% for BiLSTM and 15% BiRETAIN) than the corresponding baseline models because of the communication cost mentioned before. The performance of the federated BiLSTM model decreases slightly, and $R^2$ doesn’t change much. In contrast, the federated BiRETAIN model is affected significantly, with $R^2$ decreasing from 0.9819 to 0.9654.&lt;/p&gt;
&lt;p&gt;Under the partial-training regime, we see that the training time decreases dramatically because only 40% of the data is used at each epoch. In this regime, the MAEs for both models increase dramatically while the $R^2$ decreases. These two partially-trained models are affected by the federated framework, but the performances are still acceptable.&lt;/p&gt;
&lt;h3 id=&#34;performance-on-non-iid-data&#34;&gt;Performance on non IID Data&lt;/h3&gt;
&lt;style&gt;
.basic-styling td,
.basic-styling th {
  border: 1px solid #999;
  padding: 0.5rem;
}
&lt;/style&gt;
&lt;div class=&#34;ox-hugo-table basic-styling&#34;&gt;
&lt;div&gt;&lt;/div&gt;
&lt;div class=&#34;table-caption&#34;&gt;
&lt;l&gt;&lt;b&gt;Grouping by Height&lt;/b&gt;&lt;/l&gt;
&lt;/div&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Time (min.sec)&lt;/th&gt;
&lt;th&gt;MAE&lt;/th&gt;
&lt;th&gt;MSE&lt;/th&gt;
&lt;th&gt;$R^2$&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;BiLSTM&lt;/td&gt;
&lt;td&gt;1.56&lt;/td&gt;
&lt;td&gt;0.2579&lt;/td&gt;
&lt;td&gt;0.3529&lt;/td&gt;
&lt;td&gt;0.9474&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;BiRETAIN&lt;/td&gt;
&lt;td&gt;3.44&lt;/td&gt;
&lt;td&gt;0.3114&lt;/td&gt;
&lt;td&gt;0.4017&lt;/td&gt;
&lt;td&gt;0.9149&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;style&gt;
.basic-styling td,
.basic-styling th {
  border: 1px solid #999;
  padding: 0.5rem;
}
&lt;/style&gt;
&lt;div class=&#34;ox-hugo-table basic-styling&#34;&gt;
&lt;div&gt;&lt;/div&gt;
&lt;div class=&#34;table-caption&#34;&gt;
&lt;l&gt;&lt;b&gt;Grouping by Weight&lt;/b&gt;&lt;/l&gt;
&lt;/div&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Time (min.sec)&lt;/th&gt;
&lt;th&gt;MAE&lt;/th&gt;
&lt;th&gt;MSE&lt;/th&gt;
&lt;th&gt;$R^2$&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;BiLSTM&lt;/td&gt;
&lt;td&gt;1.58&lt;/td&gt;
&lt;td&gt;0.2921&lt;/td&gt;
&lt;td&gt;0.3868&lt;/td&gt;
&lt;td&gt;0.9327&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;BiRETAIN&lt;/td&gt;
&lt;td&gt;3.52&lt;/td&gt;
&lt;td&gt;0.3120&lt;/td&gt;
&lt;td&gt;0.4976&lt;/td&gt;
&lt;td&gt;0.8774&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;style&gt;
.basic-styling td,
.basic-styling th {
  border: 1px solid #999;
  padding: 0.5rem;
}
&lt;/style&gt;
&lt;div class=&#34;ox-hugo-table basic-styling&#34;&gt;
&lt;div&gt;&lt;/div&gt;
&lt;div class=&#34;table-caption&#34;&gt;
&lt;l&gt;&lt;b&gt;Grouping by Age&lt;/b&gt;&lt;/l&gt;
&lt;/div&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Time (min.sec)&lt;/th&gt;
&lt;th&gt;MAE&lt;/th&gt;
&lt;th&gt;MSE&lt;/th&gt;
&lt;th&gt;$R^2$&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;BiLSTM&lt;/td&gt;
&lt;td&gt;1.45&lt;/td&gt;
&lt;td&gt;0.3275&lt;/td&gt;
&lt;td&gt;0.3824&lt;/td&gt;
&lt;td&gt;0.9319&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;BiRETAIN&lt;/td&gt;
&lt;td&gt;3.31&lt;/td&gt;
&lt;td&gt;0.2873&lt;/td&gt;
&lt;td&gt;0.4071&lt;/td&gt;
&lt;td&gt;0.9269&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;From the tables above, we see that all the models in the non-IID setting perform worse than the corresponding partially-trained model on IID datasets. Models in height and age groups show a small decline in $R^2$, while the models in the weight group show a steeper decline. This is especially the case for the BiRETAIN model, with $R^2$ decreasing from 0.9307 to 0.8774.&lt;/p&gt;
&lt;h2 id=&#34;comprehensive-results&#34;&gt;Comprehensive Results&lt;/h2&gt;
&lt;p&gt;In the table below, we compare the best performing models of each type. We omit the federated learning models, because they will naturally achieve a lower performance, while the real benefit is being able to protect patient privacy.&lt;/p&gt;
&lt;style&gt;
.basic-styling td,
.basic-styling th {
  border: 1px solid #999;
  padding: 0.5rem;
}
&lt;/style&gt;
&lt;div class=&#34;ox-hugo-table basic-styling&#34;&gt;
&lt;div&gt;&lt;/div&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;MAE&lt;/th&gt;
&lt;th&gt;RMSE&lt;/th&gt;
&lt;th&gt;$R^2$&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;BMDT2 (last record)&lt;/td&gt;
&lt;td&gt;LOS&lt;/td&gt;
&lt;td&gt;0.072&lt;/td&gt;
&lt;td&gt;0.388&lt;/td&gt;
&lt;td&gt;0.964&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Ridge (last record)&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;LOS&lt;/td&gt;
&lt;td&gt;0.057&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;0.100&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;0.997&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Linear SVR (last record)&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;LOS&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;0.052&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;0.105&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;0.997&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Variable size CNN&lt;/td&gt;
&lt;td&gt;rLOS&lt;/td&gt;
&lt;td&gt;0.97&lt;/td&gt;
&lt;td&gt;1.74&lt;/td&gt;
&lt;td&gt;0.19&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;BiRETAIN&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;rLOS&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;0.085&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;0.255&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;0.983&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Because the targets predicted by the baselines models are different from those predicted by the advanced models, knowing the LOS is equivalent to know the rLOS, because the current offset is always known. So, even though we cannot directly compare the values, it is clear that simple regression models are able to match, if not exceed, the performance of sophisticated neural networks by having access to the last data. This means that almost all of the predictive power of a patient&amp;rsquo;s records is in the last record.&lt;/p&gt;
&lt;h1 id=&#34;what-have-we-learnt&#34;&gt;What Have We Learnt?&lt;/h1&gt;
&lt;p&gt;Based on our experiments, we have the following key takeways.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Simple regressors that have access to the last record achieve almost perfect predictions.&lt;/li&gt;
&lt;li&gt;CNNs are unable to capture long-term dependencies, so they perform poorly in spite of having access to all records.&lt;/li&gt;
&lt;li&gt;The RETAIN model is able to match, and even out perform, traditional RNN models, with the added benefit of interpretability.&lt;/li&gt;
&lt;li&gt;In most cases, the federated learning model can guarantee the confidentiality of patient data, while incurring a small sacrifice in performance.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The code for all our experiments can be found at 
&lt;a href=&#34;https://github.com/parvenkumari/datamining&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this GitHub repository&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;some-caveats&#34;&gt;Some Caveats&lt;/h1&gt;
&lt;p&gt;The eICU database is large and comprises of routinely collected clinical data in an eICU setting. However, the database also contains non-ICU stays (e.g., step down units SDUs). This means that some patients (patient unit stay ids) in our dataset could be from SDUs, rather than an actual ICU unit stay. Nevertheless, we do not expect this to affect our results because all our models use the same data, and we only analyze tradeoffs.&lt;/p&gt;
&lt;p&gt;An important observation about our results is that models that did not have access to all records (and so, the last record), performed very poorly. Having access to all records is a profoundly impractical assumption, because simply knowing that we have all the records means knowing that the patient&amp;rsquo;s stay has ended, which renders predicting the LOS useless.&lt;/p&gt;
&lt;p&gt;So, even though we have shown that using interpretable and confidential models does not affect performance too much, the base problem of predicting rLOS in a causal way, meaning that we do not have access to future records, remains a very hard problem.&lt;/p&gt;
&lt;p&gt;Moreover, while our experiments with federated learning showed good results, there is still room for improvement in the following areas:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Our main goal is regression, which is simpler than classification and other advanced tasks. So, separation of original data doesn&amp;rsquo;t affect the results much, and we may not have stretched our federated learning models to see its side effects.&lt;/li&gt;
&lt;li&gt;Even though our non-IID groups had different height, weight, and age distributions, all the sub-datasets were of the same size. Therefore, we did not get the results under the extreme situation that one of the sub-training set is significantly larger than another. In such a situation, we expect the performance of the federated models to decrease dramatically.&lt;/li&gt;
&lt;li&gt;All clients have the same form of data, which only differ in their values. This homogeneity might be inflating the results of our non-IID models.&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;
&lt;p&gt;[1] Rapoport J, Teres D, Zhao Y, Lemeshow S: Length of stay data as a guide to hospital economic performance for ICU patients. Med Care. 2003, 41: 386-397. 10.1097/00005650-200303000-00007.&lt;/p&gt;
&lt;p&gt;[2] Pollard, T. J., Johnson, A. E., Raffa, J. D., Celi, L. A., Mark, R. G., &amp;amp; Badawi, O. (2018). The eICU Collaborative Research Database, a freely available multi-center database for critical care research. Scientific data, 5, 180178.&lt;/p&gt;
&lt;p&gt;[3] Sheikhalishahi, S., Balaraman, V., &amp;amp; Osmani, V. (2020). Benchmarking machine learning models on multi-centre eICU critical care dataset. Plos one, 15(7), e0235424.&lt;/p&gt;
&lt;p&gt;[4] Rajabalizadeh A, Nia N, Safaei N, Talafidaryani M, Bijari R, Zarindast A, Fotouhi F, Salehi M, Moqri M (2020). Exploratory Analysis of Electronic Intensive Care Unit (eICU) Database.  medRxiv 2020.03.29.20042028; doi: 
&lt;a href=&#34;https://doi.org/10.1101/2020.03.29.20042028&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1101/2020.03.29.20042028&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[5] Harutyunyan, H., Khachatrian, H., Kale, D. C., Ver Steeg, G., &amp;amp; Galstyan, A. (2019). Multitask learning and benchmarking with clinical time series data. Scientific data, 6(1), 1-18.&lt;/p&gt;
&lt;p&gt;[6] Johnson, A. E., Pollard, T. J., Shen, L., Li-Wei, H. L., Feng, M., Ghassemi, M., Moody, B., Szolovits, P., Celi, L. A., Mark, R. G. (2016). MIMIC-III, a freely accessible critical care database. Scientific data, 3(1), 1-9.&lt;/p&gt;
&lt;p&gt;[7] Komorowski, M., Celi, L.A., Badawi, O. et al. The Artificial Intelligence Clinician learns optimal treatment strategies for sepsis in intensive care. Nat Med 24, 1716–1720 (2018). 
&lt;a href=&#34;https://doi.org/10.1038/s41591-018-0213-5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1038/s41591-018-0213-5&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[8] Wenke, S., &amp;amp; Fleming, J. (2019). Contextual Recurrent Neural Networks. arXiv preprint arXiv:1902.03455.&lt;/p&gt;
&lt;p&gt;Smirnova, E., &amp;amp; Vasile, F. (2017, August). Contextual sequence modeling for recommendation with recurrent neural networks. In Proceedings of the 2nd Workshop on Deep Learning for Recommender Systems (pp. 2-9).&lt;/p&gt;
&lt;p&gt;[9] Choi, E., Bahadori, M. T., Sun, J., Kulas, J., Schuetz, A., &amp;amp; Stewart, W. (2016). Retain: An interpretable predictive model for healthcare using reverse time attention mechanism. In Advances in Neural Information Processing Systems (pp. 3504-3512).&lt;/p&gt;
&lt;p&gt;[10] Yang, Q., Liu, Y., Chen, T., &amp;amp; Tong, Y. (2019). Federated machine learning: Concept and applications. ACM Transactions on Intelligent Systems and Technology (TIST), 10(2), 1-19.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Information Theoretic Reduced Reference Quality Models</title>
      <link>https://abhinaukumar.github.io/post/reduced-reference/</link>
      <pubDate>Sun, 16 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://abhinaukumar.github.io/post/reduced-reference/</guid>
      <description>&lt;p&gt;In this post, we will introduce two broad categories of image/video quality models, and discuss two algorithms that fall under a third, hybrid, category. We will heavily borrow from concepts of Natural Scene Statics (NSS) and Information theory discussed in my earlier post on 
&lt;a href=&#34;post/nss-and-vif&#34;&gt;VIF&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Consider the following scenario. You go on a hike and come back with a collection of high-quality pictures on your favourite DSLR camera. You now want to share these pictures, but the &amp;ldquo;raw&amp;rdquo; files on your camera are massive. So, you need to compress them, which leads to some data loss, changing the image ever so slightly. We want to measure how this &amp;ldquo;distortion&amp;rdquo; affects the quality of the image. We can ask this question in two ways.&lt;/p&gt;
&lt;p&gt;You hold the original &amp;ldquo;pristine&amp;rdquo; image and this compressed image side by side and compare them. This way, you will be able to identify exactly where the image has been changed, and by how much. Or, as in our case, you could use an algorithm which takes both the pristine image, called the reference, and the compressed image, called the test image, and predicts how good/poor you perceive the compressed image to be. This is called a Full Reference (FR) quality metric because it compares the test image against a gold-standard reference image.&lt;/p&gt;
&lt;p&gt;Another way one could ask this question is to only consider the compressed image. Instead of measuring the change due to compression, we only predict how good the final product looks. Because we do not compare the test image with any gold-standard, we call this a No Reference (NR) quality model. In the case of compression, this does not seem like a great idea because we are ignoring information about the gold standard even though we have access to it.&lt;/p&gt;
&lt;p&gt;However, NR models are very useful in other settings where a pristine image/video does not exist. The most important use of NR models is in estimating the quality of User Generated Content (UGC). While large studios create movies and advertisements with professional camera work and editing, a large volume of content on the internet is created by individuals, often with hand-held cameras. Such images/videos often suffer from degradations like noise, blur, low exposure. So, even though there is no &amp;ldquo;gold standard&amp;rdquo; to compare against, it is clear that such videos are of poor quality. NR models help us capture exactly that.&lt;/p&gt;
&lt;p&gt;Naturally, because have access to more information, FR models typically predict subjective quality better than NR models. But, we are not restricted to this all-or-nothing binary. For example, let us consider the case of transmitting a video over a lossy network. As the service provider, we would like to measure the quality of the video at the user&amp;rsquo;s end.&lt;/p&gt;
&lt;p&gt;Of course, we cannot use an FR model. If we could have a copy of the pristine video at the user&amp;rsquo;s end, we could declare victory and move on. On the other hand, no reference models do not perform very well. So, we find a middle ground. We can transmit &lt;em&gt;some&lt;/em&gt; side information about the pristine video over the network and use this to calculate quality. Such models are called reduced-reference (RR) models and they occupy the region between FR and NR models.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;info-spectrum.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now that we know what a RR algorithm should look like, let us discuss the first of two very similar algorithms - Spatio-Temporal Reduced Reference Entropic Differencing (ST-RRED) [1]. As in 
&lt;a href=&#34;post/nss-and-vif&#34;&gt;VIF&lt;/a&gt; [2], this model also uses the Gaussian Scale Mixture (GSM) to model the statistics of wavelet coefficients. However, this model deviates from VIF in two important ways. First, instead of modelling a distortion channel, frames from both the reference and the test videos are modelled as GSM random vectors (RVs). This is because modelling the distortion channel requires both reference and test data, which is only possible in an FR setting. Secondly, ST-RRED extends the statistical model to the temporal dimension by using GSM to also model the statistics of frame differences. This allows the model to determine both the spatial and temporal quality of videos.&lt;/p&gt;
&lt;p&gt;The following few paragraphs contain some tedious notation but it will all be worth it in the end. We first consider corresponding frames from the reference and distorted videos and calculate their wavelet coefficients using the Steerable Pyramid [3] decomposition. Coefficients from each subband (corresponding to one scale and one orientation) are split into $M \times M$ blocks. Let us denote the $m^{th}$ coefficient in the $k^{th}$ subband of frame $f$ in the reference and distorted videos by $C_{mkfr}$ and $C_{mkfd}$ respectively.&lt;/p&gt;
&lt;p&gt;Because we model these coefficients as a GSM, we can express them as the product of a non-negative scalar random variable $S$ and a Gaussian RV $U$. That is,
$$ C_{mkfr} = S_{mkfr} U_{mkfr}, \ C_{mkfd} = S_{mkfd} U_{mkfd}$$&lt;/p&gt;
&lt;p&gt;Similarly, to measure the temporal quality, we consider the wavelet coefficients of the differences between successive frames and model them also as GSM RVs.
$$ D_{mkfr} = T_{mkfr} V_{mkfr}, \ D_{mkfd} = T_{mkfd} V_{mkfd}$$&lt;/p&gt;
&lt;p&gt;These are passed through an Additive White Gaussian Noise Channel (AWGN) which models neural noise in the Human Visual System (HVS) to get
$$C_{mkfr}&#39; = C_{mkfr} + W_{mkfr}, \ C_{mkfd}&#39; = C_{mkfd} + W_{mkfd}$$ $$D_{mkfr}&#39; = D_{mkfr} + Z_{mkfr}, \ D_{mkfd}&#39; = D_{mkfd} + Z_{mkfd}$$&lt;/p&gt;
&lt;p&gt;where the noise RVs $W_{mkfr}$ and $W_{mkfd}$ are distributed as $\mathcal{N}(0, \sigma_w^2 I)$, and $Z_{mkfr}$ and $Z_{mkfd}$ are distributed as $\mathcal{N}(0, \sigma_z^2 I)$.&lt;/p&gt;
&lt;p&gt;As in VIF, we estimate the covariance of the &amp;ldquo;underlying&amp;rdquo; Gaussian in each case, denoting them by $K_{U_{kfr}}$, $K_{V_{kfd}}$, $ K_{V_{kfr}}$, and the value of the scalar multipliers at each point (also called the variance field), denoting them by $s_{mkfr}$, $s_{mkfd}$, $t_{mkfr}$, and $t_{mkfd}$.&lt;/p&gt;
&lt;p&gt;Using these quantities, we can compute the conditional differential entropy of the $d = M^2$ dimensional GSM vectors $C$ and $D$ given $s$ and $t$ respectively as
$$ h_{C_{mkfr}} = h\left(C_{mkfr} | S_{mkfr} = s_{mkfr}\right) = \frac{1}{2} \log\left[(2 \pi e)^d \left\lvert s_{mkfr}^2 K_{U_{kfr}} + \sigma_w^2I\right\rvert\right]$$ $$ h_{D_{mkfr}} = h\left(D_{mkfr} | T_{mkfr} = t_{mkfr}\right) = \frac{1}{2} \log\left[(2 \pi e)^d \left\lvert t_{mkfr}^2 K_{V_{kfr}} + \sigma_z^2I\right\rvert\right]$$&lt;/p&gt;
&lt;p&gt;and similarly for coefficients from the test images. A weighting term is added to these entropy values, which assigns higher weights to regions having higher local variance. This is done because areas having higher spatial information (variance) are perceptually more important. The spatial and temporal scaling factors are computed from the local variance estimates as
$$\gamma_{mkfr} = \log\left(1 + s_{mkfr}^2\right)$$ $$\delta_{mkfr} = \log\left(1 + s_{mkfr}^2\right)\log\left(1 + t_{mkfr}^2\right)$$&lt;/p&gt;
&lt;p&gt;and similarly in the distorted coefficients. The Spatial RRED index for a video having $F$ frames, in subband $k$, which has $M_k$ samples is defined as the average difference between weighted entropies
$$ SRRED_{k}^{M_k} = \frac{1}{F M_k} \sum_{f=1}^{F}\sum_{m=1}^{M_k} \lvert \gamma_{mkfr} h_{C_{mkfr}} - \gamma_{mkfd} h_{C_{mkfd}} \rvert$$&lt;/p&gt;
&lt;p&gt;Note that to calculate this, all $M_k$ values of $\gamma_{mkfr} h_{C_{mkfr}}$ must be available. This is effectively a FR model, because it has access to all the available information. However, we can reduce the number of scalars by summing up neighbouring values. As an extreme case, the SRRED index using just one scalar is obtained by summing up all the scalars in each subband, giving
$$ SRRED_{k}^{1} = \frac{1}{F M_k} \sum_{f=1}^{F} \left\lvert \sum_{m=1}^{M_k} \gamma_{mkfr} h_{C_{mkfr}} - \sum_{m=1}^{M_k} \gamma_{mkfd} h_{C_{mkfd}} \right\rvert $$&lt;/p&gt;
&lt;p&gt;We can choose to transmit any intermediate number of scalars, say $N$, by summing up $M_k/N$ neighbouring values&lt;/p&gt;
&lt;p&gt;$$ SRRED_{k}^{N} = \frac{1}{F M_k} \sum_{f=1}^{F} \sum_{n=1}^{N} \left\lvert \sum_{m=\frac{(n-1)M_k}{N}}^{\frac{nM_k}{N}} \gamma_{mkfr} h_{C_{mkfr}} - \sum_{m=\frac{(n-1)M_k}{N}}^{\frac{nM_k}{N}} \gamma_{mkfd} h_{C_{mkfd}} \right\rvert $$&lt;/p&gt;
&lt;p&gt;In exactly the same manner, we can define the temporal RRED index using $N$ scalars in the $k^{th}$ subband as
$$ TRRED_{k}^{N} = \frac{1}{F M_k} \sum_{f=1}^{F} \sum_{n=1}^{N} \left\lvert \sum_{m=\frac{(n-1)M_k}{N}}^{\frac{nM_k}{N}} \delta_{mkfr} h_{D_{mkfr}} - \sum_{m=\frac{(n-1)M_k}{N}}^{\frac{nM_k}{N}} \delta_{mkfd} h_{D_{mkfd}} \right\rvert $$&lt;/p&gt;
&lt;p&gt;Finally, the STRRED index is obtained as the product
$$ STRRED_k = SRRED_k \cdot TRRED_k$$&lt;/p&gt;
&lt;p&gt;So, we have developed an information-theoretic model of subjective quality, where the amount of information available at the receiver&amp;rsquo;s end can be controlled by choosing the number of scalars to be transmitted. Accordingly, the performance of the quality model also varies. In their experiments, the authors showed that ST-RRED  reported higher performance than Multi-Scale SSIM (MS-SSIM) and PSNR using just 0.1% of the number of scalars at the receiver.&lt;/p&gt;
&lt;p&gt;The Spatial Efficient Entropic Differences (SpEED-QA) [4] model applies the same algorithm, but in the spatial domain. This is done by replacing the wavelet transform with a simple local mean subtraction. This significantly reduces the computational cost with a small decrease (sometimes increase!) in performance.&lt;/p&gt;
&lt;p&gt;So, there we have it! By modelling reference and test images independently and comparing derived quantities, we can control the amount of information that is available at the receiver and leverage the resulting tradeoff.&lt;/p&gt;
&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;
&lt;p&gt;[1] Rajiv Soundararajan and Alan C. Bovik. Video Quality Assessment by Reduced Reference Spatio-Temporal Entropic Differencing. &lt;em&gt;IEEE Transactions on Circuits and Systems for Video Technology, 2013&lt;/em&gt; 
&lt;a href=&#34;https://www.live.ece.utexas.edu/publications/2013/Rajiv%20%20Video-RRED%20Paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2] Hamid R. Sheikh and Alan C. Bovik. Image Information and Visual Quality. &lt;em&gt;IEEE Transactions on Image Processing, 2006&lt;/em&gt; 
&lt;a href=&#34;https://live.ece.utexas.edu/publications/2004/hrs_ieeetip_2004_imginfo.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[3] Eero P. Simoncelli and William T. Freeman. The Steerable Pyramid: A Flexible Architecture For Multi-Scale Derivative Computation. &lt;em&gt;IEEE Conference on Image Processing, 1995&lt;/em&gt; 
&lt;a href=&#34;https://www.cns.nyu.edu/pub/eero/simoncelli95b.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[4] Christos G. Bampis, Praful Gupta, Rajiv Soundararajan and Alan C. Bovik. SpEED-QA: Spatial Efficient Entropic Differencing for Image and Video Quality. &lt;em&gt;IEEE Signal Processing Letters, 2017&lt;/em&gt; 
&lt;a href=&#34;https://ieeexplore.ieee.org/document/7979533&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Perceptually Driven Conditional GAN for Fourier Ptychography</title>
      <link>https://abhinaukumar.github.io/publication/fourier-ptychography/</link>
      <pubDate>Sun, 09 Aug 2020 16:43:32 -0500</pubDate>
      <guid>https://abhinaukumar.github.io/publication/fourier-ptychography/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Better Safe than Sorry: Evidence Accumulation Allows for Safe Reinforcement Learning</title>
      <link>https://abhinaukumar.github.io/publication/safe-reinforcement-learning/</link>
      <pubDate>Sun, 09 Aug 2020 16:38:12 -0500</pubDate>
      <guid>https://abhinaukumar.github.io/publication/safe-reinforcement-learning/</guid>
      <description></description>
    </item>
    
    <item>
      <title>No-reference quality assessment of tone mapped High Dynamic Range (HDR) images using transfer learning</title>
      <link>https://abhinaukumar.github.io/publication/hdr-quality-assessment/</link>
      <pubDate>Sun, 09 Aug 2020 16:34:44 -0500</pubDate>
      <guid>https://abhinaukumar.github.io/publication/hdr-quality-assessment/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Texture Features</title>
      <link>https://abhinaukumar.github.io/post/texture-feature/</link>
      <pubDate>Sat, 08 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://abhinaukumar.github.io/post/texture-feature/</guid>
      <description>&lt;p&gt;This article is a review of existing texture characterization, identification and segmentation methods. These methods typically involve statistical methods, i.e. inferring from histograms or co-occurrence matrices, and/or signal processing methods, either simple filtering and energy-based methods, or as a method of pre-processing before applying statistical analysis.&lt;/p&gt;
&lt;p&gt;But, what is texture? Simply, &amp;lsquo;&amp;lsquo;texture&amp;rsquo;&amp;rsquo; describes the local arrangement of pixels/intensity values. For a classic example, consider two $8\times 8$ squares, one painted half white and half black, while the other has a checkerboard pattern. While they have the same mean luminance (brightness), they vary in the arrangement in the actual pixel (intensity) values. That is, they vary in &amp;lsquo;&amp;lsquo;texture&amp;rsquo;&amp;rsquo;.&lt;/p&gt;
&lt;p&gt;Texture can also be thought of as local &amp;lsquo;&amp;lsquo;complexity&amp;rsquo;&amp;rsquo;. The first square in our example had two plain regions, so it was &amp;lsquo;&amp;lsquo;simple&amp;rsquo;&amp;rsquo;, while the checkerboard pattern is more &amp;lsquo;&amp;lsquo;complex&amp;rsquo;&amp;rsquo;. Natural examples of texture include grass, fabric patterns, ripples, falling confetti, etc. Note that we have not rigorously defined the term yet. That is because texture is not a precisely defined concept, merely a notion. Even so, describing local arrangements can be very useful. Texture analysis has found great use in medical image processing, document processing and remote sensing. An example closer to my work would be that &amp;lsquo;&amp;lsquo;simple&amp;rsquo;&amp;rsquo; or &amp;lsquo;&amp;lsquo;plain&amp;rsquo;&amp;rsquo; regions can be compressed easily, while complex regions may demand higher bandwidth.&lt;/p&gt;
&lt;p&gt;What type of questions can we ask about texture? The simplest question is to identify it. Given a set of texture classes, can we identify a given texture as being one of these? This is called texture classification and typically involves statistical methods. Another task is texture segmentation, where we wish to segment (split) an image into regions having different textures from each other. Think of the Windows XP wallpaper, but without clouds and less tidy grass. Such a method would split the image into the sky and the grass because they have different textures.&lt;/p&gt;
&lt;p&gt;We will now look at features that we can use to describe textures. Currently, I&amp;rsquo;m not interested in specific algorithms. My goal is to find ways to &amp;lsquo;&amp;lsquo;describe&amp;rsquo;&amp;rsquo; texture. This will also not be an exhaustive review of all texture features. After all, texture is not the focus of my work. Learning this is just the means to an end, so I will only go so far as I need to.&lt;/p&gt;
&lt;p&gt;Most of my reading has used this presentation [1] and this review article [2] as jumping points to other sources wherever necessary.&lt;/p&gt;
&lt;h1 id=&#34;first-order-statistical-features&#34;&gt;First-Order Statistical Features&lt;/h1&gt;
&lt;p&gt;Statistics that only depend on individual pixel values are called first-order statistics. The local range and variance are simple first-order statistical features to describe textures. Plain regions have a smaller range (max - min) while textured regions have larger ranges because of the greater diversity in intensity values. A similar argument can be made about local variances because of which we expect textured regions to have higher variance than plain regions.&lt;/p&gt;
&lt;h1 id=&#34;gray-level-co-occurence-matrix-glcm&#34;&gt;Gray Level Co-occurence Matrix (GLCM)&lt;/h1&gt;
&lt;p&gt;The Gray Level Co-occurence Matric (GLCM) is arguably the most common method of describing textures. The GLCM records second-order statistics, because it depends on pairs of pixels, storing information about the relative positions of pixels having similar intensity values. Given an offset  $\delta = (\delta x, \delta y)$ , the GLCM is a $256 \times 256$ matrix counting the co-occurence of gray levels at an offset  $ \delta $ . That is, we construct a matrix whose entries are
$$ G_{\delta}[i,j] = \sum_x \sum_y \mathbb{1}(G[x,y] = i) \mathbb{1}(G[x+\delta y,y+\delta y] = j) $$&lt;/p&gt;
&lt;p&gt;The entries of this matrix are then normalized by the sum of all entries, giving us a normalized GLCM, say  $ P_\delta $, which is a valid probability mass function. While the GLCM itself is not used to, say, compare textures, we derive numerical features from these which are used to describe texture. Some examples of such features are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Maximum&lt;/strong&gt; :  $ \max P_\delta[i,j] $ , i.e., the most likely pair of intensities.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Order $ k $ difference moment&lt;/strong&gt; : $ E[(i-j)^k] $ , or its inverse  $ E[1/(i-j)^k] $ . A special case of this is the contrast, which is the 2nd difference moment, i.e.,  $ E[(i-j)^2] $ .&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Homogeneity&lt;/strong&gt;: $ E\left[\frac{1}{(1 + \lvert i-j\rvert)}\right] $ . A homogeneous image will have non-zero entries close to the principal diagonal, i.e  $ i \approx j $ , while a heterogeneous image will have a more even spread since many pairs occur.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Entropy&lt;/strong&gt;: $ E[-\log P_\delta[i,j]] $ , which is a measure of the &amp;lsquo;&amp;lsquo;spread&amp;rsquo;&amp;rsquo; or the amount of information in the distribution.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Correlation&lt;/strong&gt;: $ \frac{E[ij] - \mu_i\mu_j}{\sigma_i \sigma_j} $  which is high when pixels have a linear dependence.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Haralick [3] defined 14 texture features based on the GLCN. In a similar vein, the Gray Level Difference statistics (GLDS) are derived from a vector of 256 values, which count the number of times each difference  $ \lvert i-j\rvert $  occurs between pairs of intensity values separated by a distance  $ \delta $.&lt;/p&gt;
&lt;p&gt;A main drawback of the GLCM and the GLDS is finding a good choice of  $ \delta $. In the current deep learning/gradient-based optimization era, I would add that the non-differentiability of the counting process is an added drawback.&lt;/p&gt;
&lt;h1 id=&#34;autocorrelation-function&#34;&gt;Autocorrelation Function&lt;/h1&gt;
&lt;p&gt;The autocorrelation function (ACF) is a powerful signal processing method to extract repeating patterns. Given an image  $ I(x,y) $,  the ACF is defined as&lt;/p&gt;
&lt;p&gt;$$ \rho(u,v) = \frac{\sum_x \sum_y I(x,y) I(x+u, y+v)}{\sum_x\sum_y I^2(x,y)} $$&lt;/p&gt;
&lt;p&gt;The auto-correlation function is a function of the &amp;lsquo;&amp;lsquo;offset&amp;rsquo;&amp;rsquo; between pairs of pixels. Given an offset, we multiply corresponding intensity values and consider the normalized sum. Why is this relevant? When the offset is close to the &amp;lsquo;&amp;lsquo;true&amp;rsquo;&amp;rsquo; offset between similar texture elements, the value of the ACF is close to 1, which is its highest value.&lt;/p&gt;
&lt;p&gt;Why is this the case? The short technical answer is &amp;lsquo;&amp;lsquo;Cauchy-Schwarz Inequality&amp;rsquo;&amp;rsquo;. More simply, we know that textures involve patterns which repeat at some intervals (although not exactly). So, it makes sense that we would like to &amp;lsquo;&amp;lsquo;test&amp;rsquo;&amp;rsquo; various offsets. When we choose the correct offset, the repeating intensities &amp;lsquo;&amp;lsquo;line up&amp;rsquo;&amp;rsquo;, so they get squared in the summation, leading to an ACF value close to 1. When we choose an &amp;lsquo;&amp;lsquo;incorrect&amp;rsquo;&amp;rsquo; offset, the repeating values do not overlap, leading to lower values. The figure below shows a visual argument for why &amp;lsquo;&amp;lsquo;lining up&amp;rsquo;&amp;rsquo; is a good thing.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;cs_ineq.png&#34; alt=&#34;Visualizing Cauchy-Schwarz Inequality&#34;&gt;&lt;/p&gt;
&lt;p&gt;At this point, much like Dumbledore to Harry on the Astronomy Tower, I must ask for your trust in believing that squaring values when lining up is the &amp;lsquo;&amp;lsquo;best&amp;rsquo;&amp;rsquo; you can do (that is, lining up maximizes the ACF). Thankfully, you don&amp;rsquo;t need to wait for the death of a morally grey character to find out why this is actually the case. There are elegant proofs of the Cauchy Schwarz inequality that I would encourage you to find online. Several proofs have been reviewed in [4].&lt;/p&gt;
&lt;p&gt;Moving on, the ACF falls off slowly when the texture is coarse, because it takes a large shift to fall out of, or out of phase with, the texture. On the other hand, fine textures cause a sharp drop in the ACF.&lt;/p&gt;
&lt;h1 id=&#34;signal-processing-methods&#34;&gt;Signal Processing Methods&lt;/h1&gt;
&lt;p&gt;We begin with the simple observation that the coarseness of texture in a region is related to the density of edges in that region. Fine textures have a higher edge density compared to coarse textures. To extract these features we can use edge operators like the Laplacian operator.&lt;/p&gt;
&lt;p&gt;Another set of filters is used to calculate the  $ (p+q) $  th moment of an image region  $ \mathcal{R} $
$$  m_{p,q}(x,y) = \sum_{(u,v) \in \mathcal{R}} u^p v^q I(u,v)  $$&lt;/p&gt;
&lt;p&gt;Choosing the region  $ \mathcal{R} $  to be a rectangular region, we can implement this as a linear filter having the appropriate weights.&lt;/p&gt;
&lt;p&gt;Perceptually motivated methods use filters that better represent the preattentive perception in the Human Visual System (HVS). Gabor filters, which are complex exponentials having a Gaussian envelope, are a good model of simple cells in the primary visual cortex. Because these filters are both frequency and orientation-selective, they are used to conduct a multi-scale multi-orientation analysis of images.&lt;/p&gt;
&lt;p&gt;To derive texture features, an image to passed through a Gabor filter bank to obtain subbands  $ r_i(x,y) $ . These responses are passed through a sigmoid non-linearity  $ \sigma $  (tanh function) and used to obtain texture features
\begin{equation}
f_i(x,y) = \sum_{(u,v) \in \mathcal{R}} |\sigma(r_i(u,v))|
\end{equation}&lt;/p&gt;
&lt;p&gt;Laws [5] proposed a computationally efficient method to compute texture energies using spatially separable filters. The method uses a set of Texture Energy Metric (TEM) vectors. The outer product of each pair of vectors results in a filter. The five types of vectors (corresponding to different textures) are level, edge, spot, wave and ripple.
These TEM filters are used to filter images and compute the local texture energy, which is simply the sum of the magnitudes in a local region.&lt;/p&gt;
&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;
&lt;p&gt;[1] Micheal A. Wirth. Texture Analysis 
&lt;a href=&#34;http://www.cyto.purdue.edu/cdroms/micro2/content/education/wirth06.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2] Mihran Tuceryan and Anil K. Jain. Texture Analysis. &lt;em&gt;Handbook of Pattern Recognition and Computer Vision&lt;/em&gt; 
&lt;a href=&#34;https://www.worldscientific.com/doi/abs/10.1142/9789814343138_0010&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[3] R. M. Haralick and K. Shanmugam and I. Dinstein. Textural Features for Image Classification. &lt;em&gt;IEEE Transactions on Systems, Man, and Cybernetics, 1973&lt;/em&gt; 
&lt;a href=&#34;http://haralick.org/journals/TexturalFeatures.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[4] Hui-Hua Wu and Shanhe Wu. Various proofs of the Cauchy-Schwarz inequality. 
&lt;a href=&#34;https://www.statisticshowto.com/wp-content/uploads/2016/06/Cauchy-Schwarzinequality.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[5] Kenneth I. Laws. Rapid Texture Identification. &lt;em&gt;Optics &amp;amp; Photonics, 1980&lt;/em&gt; 
&lt;a href=&#34;https://www.spiedigitallibrary.org/conference-proceedings-of-spie/0238/0000/Rapid-Texture-Identification/10.1117/12.959169.short&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Natural Scene Statistics and Visual Information Fidelity</title>
      <link>https://abhinaukumar.github.io/post/nss-and-vif/</link>
      <pubDate>Wed, 07 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://abhinaukumar.github.io/post/nss-and-vif/</guid>
      <description>&lt;p&gt;This article is a review of a popular Image Quality Model - Visual Information Fidelity (VIF) [1] which is all based on a statistical model of natural scenes. To do this, we will first review a powerful natural scene statistics (NSS) model, the Gaussian Scale Mixture. Then, we will review some basic information theory and use these tools to derive an image quality model.&lt;/p&gt;
&lt;p&gt;Let us begin at the basics. A Random Variable (&lt;em&gt;very&lt;/em&gt; informally, a variable that takes a random value) $ X $ is said to be Gaussian or Normal and is denoted by $ X \sim \mathcal{N}(\mu, \sigma^2 )$ if its associated probability distribution function (pdf) is given by
$$ f_N(x, \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right).$$&lt;/p&gt;
&lt;p&gt;If that does not mean a lot to you, it&amp;rsquo;s fine. One does not need to know the expression of a hyperbolic paraboloid to enjoy 
&lt;a href=&#34;https://interestingengineering.com/geometry-of-pringles-crunchy-hyperbolic-paraboloid&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;chips out of a Pringles can&lt;/a&gt;. The simplest way to think about it is that it a distribution which is centred around a mean value $\mu$ and whose spread (variance) is controlled by $\sigma$. It looks like so.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;paranormal_distribution.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;This definition can be extended naturally to random vectors (RVs), whose distribution is parameterized by a vector $\mu$ and covariance matrix $\Sigma$. This distribution is called a multivariate (because many variables) Gaussian and its pdf in $d$ dimensions is given by
$$ f_N(x; \mu, \Sigma, d) = \frac{1}{\sqrt{(2\pi)^d|\Sigma|}} \exp\left(-\frac{1}{2}(x - \mu)^T \Sigma^{-1} (x - \mu))\right) $$&lt;/p&gt;
&lt;p&gt;Now that we know what a Gaussian is, we can &amp;lsquo;&amp;lsquo;mix&amp;rsquo;&amp;rsquo; Gaussians together. The way we mix Gaussians is by having a set of Gaussian RVs, picking one of them at random and using that RV to generate a sample. In a way, we add a layer of randomness on top of the existing randomness.&lt;/p&gt;
&lt;p&gt;Let us call the random variable which tells us which Gaussian RV to pick the mixing variable $ S $. A compact way of expressing the explanation above is to say that given $ S = s $, RV $X$ has the conditional distribution $X|S = s \sim \mathcal{N}(\mu(s), \Sigma(s))$. Then, with a bit of probability magic, we can show that the pdf of the Gaussian Mixture is given by&lt;/p&gt;
&lt;p&gt;$$ f(x) = E_S[f_N(x; \mu(s), \Sigma(s), d)] $$&lt;/p&gt;
&lt;p&gt;Once again, the actual math is not critical. The intuition here is that we picked Gaussian RVs at random, so the overall distribution is an average of their individual distributions.&lt;/p&gt;
&lt;p&gt;The Gaussian Scale Mixture (GSM) is a simplified version of this distribution, where we assume all the Gaussians are centred at the origin, i.e., $ \mu(s) = 0$ and that all covariance matrices are scaled versions of each other, i.e., $\Sigma(s) = s^2 \Sigma$ for some covariance matrix $\Sigma$. That is to say, the components of a GSM only differ by a scaling factor. Interestingly, we can &amp;lsquo;&amp;lsquo;decompose&amp;rsquo;&amp;rsquo; the GSM distributed random variable using a positive random variable $S$ and a Gaussian RV $U \sim \mathcal{N}(0, \Sigma)$ as
$$ X = S \cdot U $$&lt;/p&gt;
&lt;p&gt;While this model is a &amp;lsquo;&amp;lsquo;simplification&amp;rsquo;&amp;rsquo; of the Gaussian Mixture Model (GMM), it is powerful a model for natural scenes. This is mainly because unlike a Gaussian, a GSM can represent heavy-tailed distributions, i.e, distributions which decay slowly. In their seminal work [3] showed that wavelet coefficients GSM can be used to model wavelet coefficients of natural images.&lt;/p&gt;
&lt;p&gt;At this point, we should clarify and introduce a few notions. First, what are natural images? They are not images &lt;em&gt;of nature&lt;/em&gt;. A natural image is an image which has not undergone any distortions, like blur, compression. noise, etc. So, a natural image is one which &lt;em&gt;looks&lt;/em&gt; natural.&lt;/p&gt;
&lt;p&gt;Second, what are wavelets? Informally, a wavelet is a &amp;lsquo;&amp;lsquo;localized&amp;rsquo;&amp;rsquo; wave. That is, while waves (think sinusoids) are periodic and infinite, wavelets are more localized in space (or time). Much like the Heisenberg Uncertainty principle, signals also obey an uncertainty principle. Simply put, a signal cannot have an arbitrary small spread in both space and frequency. So, because a wave has zero spread in frequency (a sinusoid has only one frequency), it has an infinite spread in space. In other words, we say that a sinusoid offers perfect frequency resolution but no spatial resolution.&lt;/p&gt;
&lt;p&gt;A wavelet, by being localized in space, allows us to trade off spread in space for spread in frequency, giving us both (limited) spatial and frequency resolution. A wider wavelet has poorer spatial resolution, but better frequency resolution. Because images are two dimensional, there is also a notion of orientation resolution. Isotropic functions (functions which are identical in all directions) offer no orientation selectivity in space while very narrow functions offer high orientation selectivity. The steerable pyramid [3], used in VIF and STRRED [2], provides a set of wavelets which allows for an (overcomplete) multi-scale multi-orientation decomposition of images.&lt;/p&gt;
&lt;p&gt;Finally, let us talk about information theory. In my opinion, information theory is the most beautiful offshoot of probability theory. The goal of information theory, as one would guess, is to characterize the amount of information stored in &amp;lsquo;&amp;lsquo;sources&amp;rsquo;&amp;rsquo;, which are random variables.&lt;/p&gt;
&lt;p&gt;The amount of information in, or randomness of, a random variable $ X $ is called its entropy. Mathematically, the (Shannon) entropy of a random variable having a probability mass function $f(x)$ is given by
$$ H(X) = -E[\log f(X)] $$&lt;/p&gt;
&lt;p&gt;This function satisfies properties that we would expect a randomness measure to satisfy. First, we would like the amount of randomness to always be non-negative, which is true of Shannon Entropy (See 
&lt;a href=&#34;http://www.cs.yorku.ca/~kosta/CompVis_Notes/jensen.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jensen&amp;rsquo;s Inequality&lt;/a&gt;). Second, if a random variable is constant, i.e. $Pr[X = x] = 1$ for some $x$, then it has zero randomness and its entropy is 0. Finally, the uniform distribution has the highest entropy (among all distributions having the same size of support).&lt;/p&gt;
&lt;p&gt;Let us now bring in a friend. Let $Y$ be another random variable, which is &lt;strong&gt;not&lt;/strong&gt; independent of $X$ (I said &amp;lsquo;&amp;lsquo;friend&amp;rsquo;&#39;). We can ask the question, &amp;ldquo;how much information is in $X$ if I already know $Y$&amp;rdquo;? To answer this quantitatively, we compute the conditional entropy. To calculate this, we first consider the entropy of the conditional distribution of $X$ when we are given each possible of $Y$, i.e.,
$$H(X | Y = y) = E_{X|Y = y}\left[-\log f(x | Y = y)\right] $$&lt;/p&gt;
&lt;p&gt;But we only assumed that we knew $Y$, not that $Y$ took any particular value. So, we average this over all possible values of $Y$ to get the conditional entropy
$$ H(X|Y) = E[H(X|Y = y)] $$&lt;/p&gt;
&lt;p&gt;Based on entropy and conditional entropy, we define the mutual information (MI) of two random variables $X$ and $Y$ which, as the name suggests, characterizes the amount of information each variable has about the other. The intuition for the mathematical expression for MI is as follows. Let us say that $X$ has information (entropy) $H(X)$. But, if we are given the random variable $Y$, its information content decreases (it will not increase) to  $H(X|Y)$. Then, the difference is the amount of information about $X$ that we have obtained from $Y$. So, the MI between $X$ and $Y$ is defined as
$$ I(X;Y) = H(X) - H(Y|X) = H(Y) - H(X|Y) $$&lt;/p&gt;
&lt;p&gt;Now let us briefly look at its properties. As suggested by the expression, the MI is symmetric, i.e. the MS between $X$ and $Y$ is equal to the MI between $Y$ and $X$. After all, we called it &lt;em&gt;mutual&lt;/em&gt; information. Secondly, the MI between two random variables is non-negative. If knowing $Y$ means we know $X$ exactly, $H(X|Y)=0$ (because $X$ is known deterministically, there is no randomness) and the MI is just the information in $X$, i.e. $H(X)$. If $X$ and $Y$ are independent random variables, $H(X|Y) = H(X)$ and the MI is zero, which is what we would expect.&lt;/p&gt;
&lt;p&gt;I must confess, I played a little bait and switch routine over the last few paragraphs. I allured you with the promise of continuous random variables (random vectors, even!), but defined these information-theoretic quantities only for discrete distributions. For continuous random variables, we define these quantities in analogous ways, using the pdf instead of the probability mass functions, although some pleasant properties are lost.&lt;/p&gt;
&lt;p&gt;We are finally ready to discuss the three information-theoretic quality models in question. Let us begin with VIF. VIF assumes a statistical model of the natural source (the GSM model discussed above), a distortion channel which distorts this &amp;lsquo;&amp;lsquo;pristine&amp;rsquo;&amp;rsquo; image, and an additive noise model of the human visual system (HVS). We can illustrate the VIF model as below.&lt;/p&gt;
&lt;div class=&#34;mermaid&#34;&gt;
graph LR;
  A[Source] --&gt; B[HVS]
  B --&gt; C[Receiver]
  A --&gt; D[Distortion Channel]
  D --&gt; E[HVS]
  E --&gt; F[Receiver]
&lt;/div&gt;
&lt;p&gt;So, in line with our source model, let us define the source random variable as a GSM distributed random vector&lt;/p&gt;
&lt;p&gt;$$ C = S \cdot U $$&lt;/p&gt;
&lt;p&gt;The distortion channel is described as having a deterministic scalar gain $g$ and an additive White Gaussian Noise (AWGN) $V \sim \mathcal{N}(0, \sigma_v^2 I)$. So, the distorted random variable is given by&lt;/p&gt;
&lt;p&gt;$$ D = g \cdot C + V $$&lt;/p&gt;
&lt;p&gt;Finally, we model the neural noise of the HVS as an AWGN channel having $N, N&amp;rsquo; \sim \mathcal{N}(0, \sigma_n^2)$. The final received pristine and distorted images are given by&lt;/p&gt;
&lt;p&gt;$$ E = C + N = S \cdot U + N$$
$$ F = D + N&#39; = g \cdot S \cdot U + V + N&#39; $$&lt;/p&gt;
&lt;p&gt;We condition all quantities on knowing $S$ because knowing $S$ allows us to predict VIF for our particular pair of reference and test images, instead of a general average case. The intuition here is that upon distortion, lesser information about the source is retained in the distorted image. So, the VIF index is defined as
$$ VIF = \frac{I(C; E | S = s)}{I(C; F | S = s)} $$&lt;/p&gt;
&lt;p&gt;Conveniently, conditioning on $S$ results in all RVs becoming Gaussians, whose entropies are easy to compute. For an image having $N$ samples of dimension $d$, if the covariance matrix has eigenvalues $
\lambda_j$, the mutual informations can be calculated easily as
$$ I(C; E | S = s) = \sum\limits_{i=1}^{N}\sum_{j=1}^{d} \log\left(1 + \frac{s_i^2\lambda_j}{\sigma_n^2}\right) $$
$$ I(C; F | S = s) = \sum\limits_{i=1}^{N}\sum_{j=1}^{d} \log\left(1 + \frac{g_i^2 s_i^2 \lambda_j}{\sigma_v^2 + \sigma_n^2}\right) $$&lt;/p&gt;
&lt;p&gt;If you have made it this far, congratulations! You now know what natural scene statistics and information-theoretic quality models look like. We will end by briefly commenting on the implementation details. In practice, both the reference and test images are first transformed into the Wavelet domain using the Steerable Pyramid. The coefficients (which look like filtered images) are collected in 3x3 blocks and modelled as 9-dim vectors $C_i$ and $D_i$ respectively.&lt;/p&gt;
&lt;p&gt;What remains is to find the parameters of the GSM distribution and the distortion channel from these wavelet coefficients. The HVS is assumed to have a known channel noise $ \sigma_n^2 = 0.1 $. Because these parameter estimation details are not a part of the core idea behind VIF, I will just state them without proof.&lt;/p&gt;
&lt;p&gt;$$ \Sigma = \frac{1}{N} \sum C_i C_i^T $$
$$ s_i = \frac{C_i^T \Sigma^{-1} C_i}{9} $$
$$ g_i = \frac{Cov(C, D)}{Cov(D, D)} $$
$$ \sigma_{v,i}^2 = Cov(D, D) - g_i Cov(C, D)$$&lt;/p&gt;
&lt;p&gt;Note that the covariances in the last two equations are scalar covariances calculated in corresponding local neighbourhoods of wavelet coefficients. And there we have it - VIF! Admittedly, this post turned out to be &lt;em&gt;much&lt;/em&gt; longer than I had anticipated. But, it is a good thing because I will write about STRRED and SpEED-QA [5] next, which use these same concepts, but differently. So now, even though you may not realize it, you know enough to understand two more information-theoretic quality models.&lt;/p&gt;
&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;
&lt;p&gt;[1] Hamid R. Sheikh and Alan C. Bovik. Image Information and Visual Quality. &lt;em&gt;IEEE Transactions on Image Processing, 2006&lt;/em&gt; 
&lt;a href=&#34;https://live.ece.utexas.edu/publications/2004/hrs_ieeetip_2004_imginfo.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2] Rajiv Soundararajan and Alan C. Bovik, Video quality assessment by reduced reference spatio-temporal entropic differencing. &lt;em&gt;IEEE Transactions on Circuits and Systems for Video Technology, 2013&lt;/em&gt; 
&lt;a href=&#34;https://www.live.ece.utexas.edu/publications/2013/Rajiv%20%20Video-RRED%20Paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[3] Martin J. Wainwright and Eero P. Simoncelli. Scale Mixtures of Gaussians and the Statistics of Natural Images. &lt;em&gt;Proceedings of the 12th International Conference on Neural Information Processing Systems. 1999&lt;/em&gt; 
&lt;a href=&#34;https://papers.nips.cc/paper/1750-scale-mixtures-of-gaussians-and-the-statistics-of-natural-images.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[4] Eero P. Simoncelli and William T. Freeman. The Steerable Pyramid: A Flexible Architecture For Multi-Scale Derivative Computation. &lt;em&gt;IEEE Conference on Image Processing, 1995&lt;/em&gt; 
&lt;a href=&#34;https://www.cns.nyu.edu/pub/eero/simoncelli95b.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[5] Christos G. Bampis and Praful Gupta and Rajiv Soundararajan and Alan C. Bovik. SpEED-QA: Spatial Efficient Entropic Differencing for Image and Video Quality. &lt;em&gt;IEEE Signal Processing Letters, 2017&lt;/em&gt; 
&lt;a href=&#34;http://www.christosbampis.info/uploads/d4e7f73994efae5148ba6617684696b07b31.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Tourist</title>
      <link>https://abhinaukumar.github.io/post/the-tourist/</link>
      <pubDate>Thu, 01 Mar 2018 00:00:00 +0000</pubDate>
      <guid>https://abhinaukumar.github.io/post/the-tourist/</guid>
      <description>&lt;p&gt;Through a small opening in a coat&lt;br&gt;
Glinted a shard of iridescent blue.&lt;br&gt;
The Tourist pulled it closer, clearing her throat,&lt;br&gt;
To shield from the cold wind that blew&lt;br&gt;
Sounding a low, ominous whistle&lt;br&gt;
As if in taunt or admiration.&lt;br&gt;
The sea, from behind, offered a drizzle&lt;br&gt;
Urging her to flaunt His generous creation.&lt;br&gt;
The Tourist moved ahead, steadfast&lt;br&gt;
Reading from a finger lined with the past.&lt;/p&gt;
&lt;p&gt;She walked through the crowded thoroughfare&lt;br&gt;
Each standing still with muffled screams&lt;br&gt;
Albeit with pressed shirts and slicked back hair&lt;br&gt;
Like a requiem for their forsaken dreams.&lt;br&gt;
She reached into her coat and chipped off from an edge;&lt;br&gt;
And offered it to each stationed sentry&lt;br&gt;
Who stood there as if bound by a pledge,&lt;br&gt;
As a payment for her entry.&lt;br&gt;
The tourist entered the city at last&lt;br&gt;
Having added to the finger lined with the past.&lt;/p&gt;
&lt;p&gt;To her, all these cities looked the same;&lt;br&gt;
Two stone walls rising on either side&lt;br&gt;
And yet, deserving of all their fame&lt;br&gt;
But none offering a place to hide.&lt;br&gt;
As often before, she was found by a man&lt;br&gt;
Offering to make her feel whole.&lt;br&gt;
She placed a fading blue chip on his hand&lt;br&gt;
Paying her debt with a piece of her soul.&lt;br&gt;
She walked away, hoping to be alone at last&lt;br&gt;
Adding again to the finger lined with her past.&lt;/p&gt;
&lt;p&gt;One might wonder, “What use is it?”&lt;br&gt;
Is the soul too much for the feet to carry?&lt;br&gt;
Why lose yourself to a cause unfit?”&lt;br&gt;
To you, I say, it is the contrary.&lt;br&gt;
Like others, she was born set into motion.&lt;br&gt;
So while the wind howled through each rift&lt;br&gt;
She rolled ahead like waves of the ocean&lt;br&gt;
While denying herself her own weight to lift.&lt;br&gt;
And with every loss she did outlast&lt;br&gt;
She added to the finger lined with her past.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Old Man&#39;s Tale</title>
      <link>https://abhinaukumar.github.io/post/the-old-mans-tale/</link>
      <pubDate>Fri, 16 Dec 2016 00:00:00 +0000</pubDate>
      <guid>https://abhinaukumar.github.io/post/the-old-mans-tale/</guid>
      <description>&lt;p&gt;The old gentleman lifted the glass of champagne in his hand &lt;br&gt;
Carrying with it the honour of being the best man, &lt;br&gt;
“To the lovely couple, I would like to raise a toast &lt;br&gt;
And tell you a story that I find most &lt;br&gt;
Entertaining, but it has nothing to do &lt;br&gt;
With our beloved bride or groom.&lt;/p&gt;
&lt;p&gt;Once upon a time, as many times before, &lt;br&gt;
I walked out into a forest that began at my door.&lt;br&gt;
For a troubled mind, it was a beautiful sight,&lt;br&gt;
And my aching feet carried me into the white.&lt;br&gt;
The forest would wrap me up in her trees, &lt;br&gt;
Providing a virgin land of undisturbed peace. &lt;br&gt;
But when the clear day became a smoky screen, &lt;br&gt;
I missed, for once, the colours of red and green.&lt;/p&gt;
&lt;p&gt;Of all the things I could have done,&lt;br&gt;
I abandoned classical notions of “fun”&lt;br&gt;
And chose the path that many dread; &lt;br&gt;
The path that only the vagabonds tread.&lt;br&gt;
The trail was dark but uncannily pleasant; &lt;br&gt;
Calming, yet chaotic like a raging adolescent. &lt;br&gt;
The moon offered only attention, not love; &lt;br&gt;
She was a lone friend loaned from above.&lt;/p&gt;
&lt;p&gt;I admitted I was lost, as my watch struck eleven.&lt;br&gt;
It was too cold for hell, but it sure wasn&amp;rsquo;t heaven.&lt;br&gt;
The wind carried the call of an unknown beast, &lt;br&gt;
Waiting in the bushes for his Christmas feast. &lt;br&gt;
Finding myself on the ground, I tried to remember why &lt;br&gt;
And before I could look up, something dropped from the sky. &lt;br&gt;
Shivering hard, I closed my eyes shut.&lt;br&gt;
I had come for some peace; this was anything but.&lt;br&gt;
Even so, I could tell it swung around &lt;br&gt;
Waiting, as a forgotten victim, to be found.&lt;br&gt;
&amp;lsquo;That was a man’, my brain tried to reaffirm.&lt;br&gt;
‘Of course, it was’. But my eyes wouldn&amp;rsquo;t confirm.&lt;/p&gt;
&lt;p&gt;In spite of the crippling fear and fatigue, &lt;br&gt;
The man&amp;rsquo;s identity was a cause of intrigue.&lt;br&gt;
Maybe he was someone I had met. &lt;br&gt;
Maybe he just couldn’t outrun his debt. &lt;br&gt;
Or a man of great wisdom and might&lt;br&gt;
Brought to the ground on his greatest flight&lt;br&gt;
Unable to share life&amp;rsquo;s sense of humour&lt;br&gt;
And gone at the dawn of the vilest rumour. &lt;br&gt;
Or maybe just a man tired of his woeful existence&lt;br&gt;
Who declined to offer any further resistance.&lt;br&gt;
Or maybe he just took a fateful drink,&lt;br&gt;
Lost his sacred ability to think,&lt;br&gt;
Thought a rope around his neck suited him best&lt;br&gt;
And allowed gravity to do the rest.&lt;/p&gt;
&lt;p&gt;You’d expect me to say I had learnt something by now&lt;br&gt;
About life, its deep meaning and how&lt;br&gt;
It was stupid of me to have stayed on the ground.&lt;br&gt;
But no, I waited till the sun came around.&lt;br&gt;
I opened my eyes as if to look up was a sin&lt;br&gt;
And there he hung, the wax mannequin.”&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>In Search of Swaraj</title>
      <link>https://abhinaukumar.github.io/post/in-search-of-swaraj/</link>
      <pubDate>Wed, 19 Oct 2016 00:00:00 +0000</pubDate>
      <guid>https://abhinaukumar.github.io/post/in-search-of-swaraj/</guid>
      <description>&lt;p&gt;I’ve lived a vast majority of my life in a dominion where a minority (one-fourth) of the population wielded a disproportionate amount of power over the numerical majority. Much like the British did in colonial India. I speak, of course, of the great power bestowed upon my mother, by the unmentioned powers of the universe. Anyone even mildly familiar with the display of said power would know that the element of surprise is the (metaphorical) nutrient-rich petri-dish upon which the effect of said display is cultured. And there is nothing more effortlessly middle class, yet vastly effective as the sudden issue of a diktat to get a haircut.&lt;/p&gt;
&lt;p&gt;Before you judge, I am aware of those kids in Africa everyone talks about. I know it is, classically, a more socially relevant issue to write about. But, I haven’t seen any of them being threatened with surprise haircuts, so I reject the comparison. (That was a joke. Please don’t threaten to [or actually] post a status update or start a hashtag. I sympathise with them on a daily basis. Well, not daily, but you get the idea.) Anyway, let&amp;rsquo;s get back on topic.&lt;/p&gt;
&lt;p&gt;The first plan of attack is, of course, retaliation. So, refuse. And then watch them refuse to accept your refusal. And then realise that your plan never factored in history. Never has a puny child’s “no” overpowered that of that singularity of all power (read mom). (“And it will stay that way”, she says.) Back home, school on weekdays meant that Sunday was the day allotted for this last legal form of systematic, and forced, acquisition of personal property (read hair). Invariably, the day of issue would give my brother and me a couple of days to figure out who would go that Sunday and who would go the next. (Side note: All stalemates were dealt with by the age-old method of conflict resolution called “Let’s ask mom”. Side-side note: Who thought that was a good idea?) Being the one with the longer hair never worked in my favour, but I had to hold on to that as it was the last domain that I had any control over. But more on that later.&lt;/p&gt;
&lt;p&gt;So, on average, one gets three days to mentally prepare for the sacrifice. This period is usually characterized by increased mirror usage, new-found fondness for hair products and hallucinations of sounds made by electric razors (Not really, so if you could relate to that last one, you need to go see a doctor. Stat.) This is also the period of bargaining. With my mum about how short I had to get my hair cut. I’m not aware of the professionalism among barbers in other places in India, but where I come from, we use a time-tested scientifically-designed scale of “short or medium”. “Medium”, stands for “shorter than you think it will be”, and “short” stands for “Stop crying, you’re not bald yet”. To put it into perspective with the bargain, the difference between the length of your hair in “medium” and “short” is that little personal space you let your parents invade. This is the one part of the whole ordeal I win at. Although it takes a great deal of effort and having to summon all of my skills of parental persuasion, I manage to get away with “medium”, which is the lesser of two evils.&lt;/p&gt;
&lt;p&gt;On that Saturday night, one learns the most important lesson anyone can hope to learn; that life has a twisted sense of humour. By then, I’ve silently resigned to my fate and even convinced myself that my hair is an unqualified mess and therefore, must be sentenced to the equivalent of capital punishment in the world of bodily produced keratinous filaments. So, that night, I walk towards my bed after a long and emotionally exhausting week and pass by a mirror I had forgotten was even there. Turn around and look at myself in the mirror (I was really tempted to make a “how do you sleep at night” and/or a “how do you look at yourself in the mirror” joke at this point but couldn’t come up with one that was relevant) and my hair is on point. Just perfect. It’s the best it’s looked in forever, and all of that emotional foundation-laying goes to waste. But, I have no choice but to go ahead with it. The next morning, I’d embark on the journey (that one must take alone) to the hallowed ceremonial grounds (read salon) and when I’d reach there the next life lesson would await me.&lt;/p&gt;
&lt;p&gt;Barbers are a sadistic lot. I would, honestly, not be surprised if most psychopathic and sadistic serial killers worked day jobs as barbers. By the time I’ve been led up to the chair, they’ve used their acute senses to determine my willingness (or lack thereof). And, because I’m, well, me, the thinly veiled look of acute depression and loss doesn’t help. They pick their tools up and make to stand behind me. Much like this.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;joker.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;And every time, I can almost hear them think to themselves, “Oh, I’m not gonna shave your head. I’m just gonna cut your hair. Really, really, short.” I know, at this point, all this sounds like I’m reading too much into body language. Well, explain this. For those not familiar with the process, the electric razor is used to trim the hair on the sides of the head first. When that’s done, you reach a state where even looking at that figure staring at you from the mirror in front of you is difficult. Because you look like a sentient mushroom cloud. This is where the psychopathy of the barber kicks in. Right then, at your most vulnerable, he stops. And walks away. For “tea”. And as he sips on his beverage that was, conveniently, delivered just then, he looks across at you trying to take it all in; trying to come to terms with the ramifications of his not coming back. Imagine having to walk out like that! It would mean the complete and utter annihilation of any social, and therefore, self-image you had spent your life painstakingly constructing. Just before you’ve given up any hope of walking out of there without pulling off a Shia LaBeouf (Google: Shia LaBeouf paper bag, for more details), he comes back, now a saviour. With nowhere left to go but up.&lt;/p&gt;
&lt;p&gt;If this is not oppression I don’t know what is. To convince you, I am now going to quote someone famous. Was it not Martin Luther King who said, “I have a dream that my four little children will one day live in a nation where they will not be judged by the length of their hair but by the content of their character.” It wasn’t? Well, it ought to have been. So, I’d like to send a message to all the kids out there who have been through this. Remember that countless lives have been lost to the cause of swaraj and we are not going to let them take it away from us.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Five Stages of Summer</title>
      <link>https://abhinaukumar.github.io/post/the-five-stages-of-summer/</link>
      <pubDate>Sat, 09 Jul 2016 00:00:00 +0000</pubDate>
      <guid>https://abhinaukumar.github.io/post/the-five-stages-of-summer/</guid>
      <description>&lt;p&gt;Because I’ve had a lot of people tell me the worst way to start an article is using a quote, I’m going to do just that.&lt;/p&gt;
&lt;p&gt;“Everything good, everything magical happens between the months of June and August.” Now, before you think I’m some literary savant who knows all quotes by anybody famous, I’d like to confess that I Googled for this after I wrote down the title of the article. And I found it on Buzzfeed, not Goodreads; so you can imagine my literary inclinations. One of the things that struck me about it, though, was that it was tagged under “Summer”. I think it should be tagged under “Guilt”.&lt;/p&gt;
&lt;p&gt;For the less informed, summer vacations at IITH start in May and extend through to the end of July. When you spend the last week of the semester writing tests and assignments, you convince yourself that it’s all worth it for the amazing three months ahead. As I stood at the exit of the hostels waiting for a bus to take me on a 14-hour journey home, I made a commitment to myself. A commitment that I’d use my holidays to do everything I ever wanted to do because I had finally understood the value of a vacation. “Ever”, obviously, referring to the previous week because that’s the only thing I had a memory of. Who has life goals anyway, right? Thus began my journey to the discovery that the five stages of summer are, actually, the five stages of grief backwards&lt;/p&gt;
&lt;p&gt;It was a week after I sank into a sofa for the first time in two months that I realised that that moment of realisation was truly once-in-a-vacation. Cuz you ain’t gonna have one after you get home. You see, trauma-induced motivation is like that cheap deodorant you bought on a trip because you suddenly realised you needed one. You’re not going back to it once you get home. And I didn’t mind that at all. There it was; stage 1 – Acceptance.&lt;/p&gt;
&lt;p&gt;The first week had passed informing people I was in town (although, I only actually met family in the first six weeks) and catching up on the shows I thought I was tired of, spending time I thought I had nothing to do with. Of course, I was wrong on both counts.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;awesomeness_v_time.jpg&#34; alt=&#34;Awesomeness v Time&#34;&gt;&lt;/p&gt;
&lt;p&gt;As I’ve shown in, what I call, the awesomeness vs time curve up there, week 2 is when you really ease into the routine and start to appreciate it. This was the life, you know. Food laid out on a table, watching Survivor, Masterchef and House on repeat. Especially because Masterchef was on right about lunchtime. I didn’t know which meal I was having but it was pretty convenient to be treated like the judges were on TV. And you’ve gotta admit, watching people fight for food on reward challenges really boosts the ego about having it for free.&lt;/p&gt;
&lt;p&gt;Stage 2, depression, began about midway into the third week with Survivor going off-air, leaving a vacuum that could only be filled in by rational thought. I began to realise what a waste of a day it was to watch other people make a million dollars. The excitement just dropped, and it was all downhill from there. I’d explain in detail, but given that almost all of you who’re reading this have been through JEE, you’d have been acquainted with it at some point. To sum it up, I spent all my time complaining that I couldn’t do the things I wanted to do because I was too bored because I had nothing to do. (It made sense at the time)&lt;/p&gt;
&lt;p&gt;Then began the bargaining. I figured if I could spend about an hour a day doing something productive, it’d atone for how I spent the other 23. It went on for a couple of days but I ran out of easy things to do very soon. (That’s when I began this article, by the way) Then, mathematics took over and I figured I didn’t have to be productive all at once, and I could work on something in parts spread out between the TV shows I was addicted to by now, and still do the same amount of work.&lt;/p&gt;
&lt;p&gt;By the end of that week, I realised that I wasn’t the only one who noticed I hadn’t been much use to anybody in a while. And the others began to feel they should, at least, make me aware. It was hard to make a case to them because even I knew it was true but I just didn’t want to do much else about it. A couple of days of being annoyed and I realized I was at the anger stage. So I decided to do the first sensible thing I’d done in almost a month and set up for when I’d get some sense knocked into me because I figured it probably wasn’t that far away anyway.&lt;/p&gt;
&lt;p&gt;As much as it pains the nonconformist in me to admit, I did what I was supposed to do and set up some achievable goals and got myself productive again. Which brings us to the last and final stage – denial. Denial of what, you ask? Nothing much, really. Just that the first four weeks ever happened.&lt;/p&gt;
&lt;p&gt;So, I guess people say moments of redemption are spread far and wide in our lives. But I had one every week; every Sunday before I went to bed (although, technically, that would make it Monday) when I realised I just lost another week to make myself productive. Luckily, the third time was a charm. Remember that guilt I wrote about at the beginning? Turns out it’s a potent getter-off-your-butt-er. &lt;em&gt;So&lt;/em&gt; (That’s just me stressing on the word) potent, in fact, that it got me back to finish the second half of this article. Twice. So wherever you are in this process, remember that May sets it up for you and the more guilt you accumulate the better. Because that’ll fuel you through the five stages of summer.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>He Who Derived His Excellence From That Of Others</title>
      <link>https://abhinaukumar.github.io/post/harsha-bhogle/</link>
      <pubDate>Tue, 12 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://abhinaukumar.github.io/post/harsha-bhogle/</guid>
      <description>&lt;p&gt;Ever since the game of cricket began to be telecast to crowds, live commentary has played a great role in bringing the game closer to laypeople. This, they achieved, not just by their knowledge of the game, but sometimes also simply by their choice of how to present a situation to the audience. When Mansur Ali Khan Pataudi was dismissed for one run against England at Bombay in 1973, the commentator, Bobby Talyarkhan remarked on the radio, “Pataudi is out 99 runs short of an expected century!”&lt;/p&gt;
&lt;p&gt;For long, Indians hadn’t been considered great commentators. But then came one man who, at his peak, had played for his college and was best appreciated for getting off strike when he batted. And he took Indian cricket commentary to new heights.&lt;/p&gt;
&lt;p&gt;Harsha Bhogle started at the age of nineteen at the All India Radio. He became the first Indian to be invited by the Australian Broadcasting Corporation to cover India’s tour of Australia before the 1992 World Cup.  The catch was that the ABC would not pay the foreign presenter, and it was up to the home broadcasting stations to pay the presenter. And the AIR did not even know he went to Australia. But being signed did not mean respect and that is probably the thing he handled best. Instead of competing against other cricketer-turned-commentators, he adopted a different style of commentary; a non-technical flavour which captivated the audience as no one had before.&lt;/p&gt;
&lt;p&gt;He gained the support and respect of active cricketers at that time because he made them feel comfortable during their interviews. And for someone who is constantly being judged on his performance and worth, there is nothing more refreshing than being spoken to, not as an authority, but as an admirer.&lt;/p&gt;
&lt;p&gt;In 2005, he visited his alma mater and gave a pretty cool speech on excellence. And his opening line went something like this. “I am here, not as someone who has achieved excellence in life, but as someone who has seen excellence first-hand” It is this kind of modesty that made him the most popular cricket commentator in the world. And as there weren’t many “great” presenters at that time, he sought inspiration from the cameramen and technicians he worked with and tried to emulate the kind of perfection they brought to their professions.&lt;/p&gt;
&lt;p&gt;And he had a brilliant way of putting things. During the recently concluded Cricket World Cup, when playing against India, AB de Villiers missed a rather simple run-out chance. While the others were ruing the missed opportunity or criticizing the undue risk taken by the Indian batsmen, Harsha Bhogle chose to see the positive side. “The one good thing we can all take from this run out is that AB de Villiers has proved to the world that he too is human and that he too can make mistakes” Rewind back to a warm-up game before the World Cup. The Indian Team dropped a considerable number of catches in that match. Once again, optimism had not seen a more faithful patron, till that night. “It’s okay. The Indians can drop as many as they want now and let the law of averages catch up.” In another ODI game, Sachin Tendulkar and MS Dhoni were playing at the end of the innings. Dhoni had dismissed the ball to all sides of the park. Getting in on the act, Sachin caressed one to the boundary. In all this mayhem, Harsha Bhogle’s voice was the only one that left the commentary box. “We have a surgeon at one end and a butcher at the other”.&lt;/p&gt;
&lt;p&gt;Test games are known for their long-drawn-out nature, and it is in such situations that the role of a commentator doubles up as an entertainer to keep the readers hooked to the game. In a game against India, Alastair Cook had scored one run in 52 balls. The next ball, he scored a boundary. Seizing the opportunity, Bhogle remarked, “80% of his runs came off that one ball!” His commentary wasn’t limited to what happened on the field at that time. Speaking about the legend that is Sachin Tendulkar, he said. “Eruption of joy at an Indian wicket can only mean one thing – that Sachin is next to the crease.” And how good is a commentator who focuses on just his home country? When Misbah-ul-Haq came under criticism from players like Shoaib Akthar and other Pakistani players, he commented, “I find this criticism of Misbah very strange. It is like a family of ten complaining that the sole breadwinner isn’t doing enough. Misbah is rated far higher outside Pakistan than within. Afridi is rated much higher in Pakistan that outside!” Sometimes, even the most optimistic of people must bow down to reality. And Harsha Bhogle did so gracefully. When Ian Chappell asked him if Narendra Hirwani can bat, he said, “If you make a team with all the No.11s of all the teams, Narendra Hirwani would still be the No.11.” On Rahul Dravid being bowled on his last international appearance, “In a career that is marked by grace, style and beautiful batsmanship, it’s a slog that’s ended Rahul Dravid‘s career. But once again, it was what THE TEAM needed.”&lt;/p&gt;
&lt;p&gt;Starting as an outlaw in a profession dominated by former cricketers, he went on to carve a niche for himself, applying one cardinal rule he claimed to have learnt from journalism. “Don’t let the truth get in the way of a good story.”. Everywhere he went, he was consistently asked one question; “How many games have you played?” Initially determined to prove himself as one of them, he later realized that there were too many of “them” anyway and that he shouldn’t try to “examine Sachin Tendulkar’s cover drive” or “explain the art of reverse swing with Wasim Akram beside me in the box”.&lt;/p&gt;
&lt;p&gt;And I think the success of Harsha Bhogle lies in that moment of realization. He embraced his lack of technical knowledge and replaced it with his supreme presenting skills, and that has made all the difference.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Man Who Sits Under The Banyan Tree</title>
      <link>https://abhinaukumar.github.io/post/banyan-tree/</link>
      <pubDate>Sat, 15 Aug 2015 00:00:00 +0000</pubDate>
      <guid>https://abhinaukumar.github.io/post/banyan-tree/</guid>
      <description>&lt;p&gt;There was once a man, quite old was he,&lt;br&gt;
He spent his day under a Banyan tree.&lt;br&gt;
Men would come, and men would go&lt;br&gt;
Up to twilight, from the cock’s crow.&lt;br&gt;
They came from all walks of life,&lt;br&gt;
Seemingly struck by mental strife.&lt;br&gt;
When they left, they left with a smile,&lt;br&gt;
No wonder he had people waiting in a file.&lt;br&gt;
And I wonder, “So knowledgeable is he,&lt;br&gt;
The Man Who Sits Under The Banyan Tree?”&lt;/p&gt;
&lt;p&gt;And where would he spend the night in?&lt;br&gt;
For he didn’t seem to have any next of kin.&lt;br&gt;
Back then, I was new to the city,&lt;br&gt;
And so, all this was quite the mystery.&lt;br&gt;
I asked around, and answers I got,&lt;br&gt;
Which were contrary to what I thought.&lt;br&gt;
And I mused, “What a fool he must be,&lt;br&gt;
The Man Who Sits Under The Banyan Tree?&amp;quot;&lt;/p&gt;
&lt;p&gt;As he was an imbecile, he was thrown&lt;br&gt;
Out to the streets, by his very own.&lt;br&gt;
The rest of it is an urban lore,&lt;br&gt;
Bottom line being, he found a door&lt;br&gt;
That fed him well, and made him gain&lt;br&gt;
In health, only to get back on the streets again.&lt;br&gt;
And I ask myself, “What do people see,&lt;br&gt;
In The Man Who Sits Under The Banyan Tree?”&lt;/p&gt;
&lt;p&gt;Atop a pedestal, he sat on a bed,&lt;br&gt;
And so, I couldn’t listen to what he said.&lt;br&gt;
But, I wanted to see what pulled such a crowd&lt;br&gt;
Which made the whole street buzz aloud.&lt;br&gt;
After pushing, pulling, a minor tussle&lt;br&gt;
And two hours of being asked to hustle,&lt;br&gt;
I got to where he was, eventually,&lt;br&gt;
To meet The Man Who Sits Under The Banyan Tree.&lt;/p&gt;
&lt;p&gt;“All my life, Sir, what have I done?”&lt;br&gt;
“You have done a good job, my son.”&lt;br&gt;
So, I told myself, “This man, here,&lt;br&gt;
He must be a great seer.”&lt;br&gt;
And then, I realized, for whatever I said,&lt;br&gt;
“You’ve done a good job, son; go ahead.”&lt;br&gt;
Looking back - Irrespective of the crowd there,&lt;br&gt;
The locals never did seem to care.&lt;br&gt;
So, let me give you some advice for free,&lt;br&gt;
Don’t fall for The Man Who Sits Under The Banyan Tree.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
