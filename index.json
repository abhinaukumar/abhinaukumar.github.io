[{"authors":["admin"],"categories":null,"content":"I recently obtained my PhD in Electrical and Computer Engineering at the University of Texas at Austin. I worked at the Laboratory for Image and Video Engineering (LIVE) with Prof. Alan Bovik, where we used statistical, signal processing and machine learning methods to evaluate and optimize the subjective quality of images and videos. This is especially relevant in the context of the online video explosion in recent years. Because of my researcrh in the area, I have had the pleasure of working with leading companies like Meta Platforms and Apple. I have also worked as a research intern at Carnegie Mellon University, in 2018.\nI graduated from the Indian Institute of Technology, Hyderabad in India with a Bachelors in Electrical Engineering, where I was awarded an Institute Silver Medal for having the highest score in my department. While at IIT Hyderabad, I was involved in several clubs. During my sophomore year, I was selected as a student mentor at Sunshine, the Counseling Cell of IIT Hyderabad, which I became the student head of the next year. I was also heavily involved with the Literary Society, writing for Lexicon, the magazine of the Literary Society and serving as its editor during my junior year. That same year, I was also the Literary Events coordinator at Elan, the cultural fest of IIT Hyderabad and a coordinator of the Quiz Club.\nWhen I am not trying to figure out why you didn\u0026rsquo;t like that one cat video as much as the others, I play badminton, counter strike, and chess.\n","date":1607558400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1607558400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://abhinaukumar.github.io/author/abhinau-kumar/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/abhinau-kumar/","section":"authors","summary":"I recently obtained my PhD in Electrical and Computer Engineering at the University of Texas at Austin. I worked at the Laboratory for Image and Video Engineering (LIVE) with Prof. Alan Bovik, where we used statistical, signal processing and machine learning methods to evaluate and optimize the subjective quality of images and videos.","tags":null,"title":"Abhinau Kumar","type":"authors"},{"authors":null,"categories":null,"content":"Authors: Abhinau Kumar, Shreshth Saini, and Prof. Alan Bovik, from the Laboratory for Image and Video Engineering\nE-Mails: abhinaukumar@utexas.edu, saini.2@utexas.edu\nHello! My name is Abhinau and I am a Graduate Research Assistant at UT Austin. Shreshth Saini, another graduate student at UT Austin, and I are looking to bridge a large gap in the literature - the absence of publicly available High Dynamic Range videos! HDR is all around us, and you might have seen it advertised on TVs, phones, and cameras recently. At the Laboratory for Image and Video Engineering, we conduct pioneering research to improve users' experience with HDR videos (among others). But, because there is no data, we are at an impasse.\nThat is why we need your help! If you have an HDR-capable phone (list below!), please share a few videos that you\u0026rsquo;ve filmed! We intend to create a database of hundreds to thousands of videos, so every single one counts! Below is a more detailed explanation of our goals and how you can participate and support this research! We hope you\u0026rsquo;ll take a few minutes to contribute videos and share this survey with your friends to support our PhD research.\n(We\u0026rsquo;re PhD students after all, so we like to write a lot! Don\u0026rsquo;t be alarmed by the length of this webpage! Feel free to skip the technical mumbo jumbo and use the paragraph headings to üé∂ skip to the good part(s) üé∂.)\nIntroduction and Context: In the evolving realm of video technology, the emergence of High Dynamic Range (HDR) videos marks a significant advancement. Unlike the Standard Dynamic Range (SDR) videos, which are more commonly used, HDR videos represent a broader range of luminance. This not only ensures the depiction of brighter highlights but also retains details in shadows. Our research at the Laboratory for Image and Video Engineering (LIVE) at the University of Texas at Austin aims to delve deeper into studying HDR videos, especially for developing algorithms for HDR content. Shorts refer to brief video segments designed to capture specific moments in time. For the purpose of our research, these shorts are invaluable as they provide snapshots under diverse luminosity (brightness) conditions, emphasizing the richness of HDR.\nProject Specifications: Objective: Our primary goal is to compile a comprehensive database of pristine HDR shorts captured from a diverse set of contributors.\nData Type: Videos should be HDR and have a minimum duration of 4 seconds to ensure the consistent quality and relevance of data. To assist you in identifying and selecting appropriate HDR videos from your mobile phone‚Äôs camera gallery, please refer to the next section.\nUploading Process: Contributors should fill out this Qualtrics form to upload videos -  HDR Video Upload Form.\nCopyright and Privacy Considerations: To maintain academic integrity and ensure the unrestricted use of the videos for research and development, contributors will be prompted to agree to a set of general copyright and consent terms when filling in the Qualtrics form -  Copyright and Consent Terms.\nTechnical Guidelines and Recommendations: Device Compatibility: Ensuring that submissions stem from devices adept at capturing true HDR content is pivotal for the integrity of our research. The following is a list of mobile devices known for their capability to capture genuine HDR content: iPhone 12 and onwards, Samsung Galaxy S21 and onwards, Samsung Z Flip3/Fold3 and onwards, Oneplus 7 Pro and onwards, Google Pixel 6 Pro and Onwards, Xiaomi 12 Pro and Onwards, Xiaomi Mi 11i and Onwards, Huawei Mate 4 and Onwards, etc.\nNote that the above list is not exhaustive, and we recommend you check if your device camera is HDR capable. Please feel free to contact Abhinau or Shreshth to help you.\nSelecting HDR Shorts: We are interested only in HDR videos, not \u0026ldquo;regular\u0026rdquo; SDR videos. Most devices show a separate tag when you view the HDR video in the gallery, or you can check the video details. Eg: iPhone users can discern HDR videos by the specific tag located on the left corner of the video thumbnail or by checking the video\u0026rsquo;s detailed properties. Make sure to upload videos from the Files App if your gallery app processes the video before uploading/exporting.\nNOTE: iPhone users, please first select all HDR videos from the gallery and save them in the Files App, and upload them from Files only. Uploading from the gallery renders the video in SDR (which is of no use to us).\nIf you are unsure, we recommend that you select all videos that you don\u0026rsquo;t mind sharing, and we\u0026rsquo;ll filter HDR videos from your uploaded corpus. Alternatively, you can contact either Abhinau Abhinau or Shreshth for help. .\nSuggested Content: Given the visual richness of HDR, we recommend scenes that oscillate between bright and dark or those rich in colors. Urban landscapes at night, nature during the golden hours, or settings with vibrant color contrasts could exemplify HDR\u0026rsquo;s capabilities. Videos filmed indoors of objects and people (read the consent form!), and selfie-style videos will also be accepted!\nIf you are unsure, you can skip filtering for content type and simply upload all videos that you don\u0026rsquo;t mind sharing, and we\u0026rsquo;ll filter the ones that we think are most appropriate. \nPurpose and Impact of the Research: Why Your Contribution Matters: Each submitted video enriches our database, enabling us to refine our understanding and methodologies for HDR content. This could revolutionize how content is viewed in the future.\nResearch Focus: Using the collected shorts, our team will undertake extensive experiments, targeting quality evaluation and tone mapping to improve the delivery of HDR videos to a wide range of consumers without significant quality degradation.\nMessage from Authors: As we said before, there is no large-scale open-source database of HDR videos available to researchers today. Your support and contributions serve as the backbone for this pioneering research. Tell your friends and colleagues about our study! A broader data set promises richer insights and better user experience on the internet!\nYour contribution will play a vital role in molding the future of video processing tools, ensuring content is viewed at its very best!\nOnce again, for any clarifications or inquiries, please reach out to Abhinau or Shreshth. We will gladly hold a conversation with you to guide you through the process as you help us with our research!\n","date":1698361200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698361200,"objectID":"dc49b5b3d02592847e9bb9d15be8078e","permalink":"https://abhinaukumar.github.io/hdr-videos-call/","publishdate":"2023-10-27T00:00:00+01:00","relpermalink":"/hdr-videos-call/","section":"","summary":"Authors: Abhinau Kumar, Shreshth Saini, and Prof. Alan Bovik, from the Laboratory for Image and Video Engineering\nE-Mails: abhinaukumar@utexas.edu, saini.2@utexas.edu\nHello! My name is Abhinau and I am a Graduate Research Assistant at UT Austin.","tags":null,"title":"Please Donate HDR Videos for Good Karma","type":"page"},{"authors":["Abhinau K. Venkataramanan","Cosmin Stejerean","Ioannis Katsavounidis","Alan C. Bovik"],"categories":[],"content":"","date":1680841614,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1680841614,"objectID":"79a291762dadff08ffb97ba3fad66cfe","permalink":"https://abhinaukumar.github.io/publication/funque-plus/","publishdate":"2023-04-28T00:28:00-05:00","relpermalink":"/publication/funque-plus/","section":"publication","summary":"The Visual Multimethod Assessment Fusion (VMAF) algorithm has recently emerged as a state-of-the-art approach to video quality prediction, that now pervades the streaming and social media industry. However, since VMAF requires the evaluation of a heterogeneous set of quality models, it is computationally expensive. Given other advances in hardware-accelerated encoding, quality assessment is emerging as a significant bottleneck in video compression pipelines. Towards alleviating this burden, we propose a novel Fusion of Unified Quality Evaluators (FUNQUE) framework, by enabling computation sharing and by using a transform that is sensitive to visual perception to boost accuracy. Further, we expand the FUNQUE framework to define a collection of improved low-complexity fused-feature models that advance the state-of-the-art of video quality performance with respect to both accuracy and computational efficiency.","tags":[],"title":"One Transform To Compute Them All: Efficient Fusion-Based Full-Reference Video Quality Assessment","type":"publication"},{"authors":["Abhinau K Venkataramanan","Cosmin Stejerean","Alan C Bovik"],"categories":[],"content":"","date":1666069200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666069200,"objectID":"bcb7a5b803977566f15b97f161dc0452","permalink":"https://abhinaukumar.github.io/publication/funque/","publishdate":"2023-04-28T00:28:00-05:00","relpermalink":"/publication/funque/","section":"publication","summary":"Fusion-based quality assessment has emerged as a powerful method for developing high-performance quality models from quality models that individually achieve lower performances. A prominent example of such an algorithm is VMAF, which has been widely adopted as an industry standard for video quality prediction along with SSIM. In addition to advancing the state-of-the-art, it is imperative to alleviate the computational burden presented by the use of a heterogeneous set of quality models. In this paper, we unify \"atom\" quality models by computing them on a common transform domain that accounts for the Human Visual System, and we propose FUNQUE, a quality model that fuses unified quality evaluators. We demonstrate that in comparison to the state-of-the-art, FUNQUE offers significant improvements in both correlation against subjective scores and efficiency, due to computation sharing.","tags":[],"title":"FUNQUE: Fusion of Unified Quality Evaluators","type":"publication"},{"authors":["Abhinau K. Venkataramanan","Marius Facktor","Praful Gupta","Alan C. Bovik"],"categories":[],"content":"","date":1641013200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641013200,"objectID":"c20660bf97597416d14ec7cd925c39c8","permalink":"https://abhinaukumar.github.io/publication/object-detection-quality/","publishdate":"2022-04-28T00:28:00-05:00","relpermalink":"/publication/object-detection-quality/","section":"publication","summary":"Many algorithms have been developed to evaluate the perceptual quality of images and videos, based on models of picture statistics and visual perception. These algorithms attempt to capture user experience better than simple metrics like the peak signal-to-noise ratio (PSNR) and are widely utilized on streaming service platforms and in social networking applications to improve users' Quality of Experience. The growing demand for high-resolution streams and rapid increases in user-generated content (UGC) sharpens interest in the computation involved in carrying out perceptual quality measurements. In this direction, we propose a suite of methods to efficiently predict the structural similarity index (SSIM) of high-resolution videos distorted by scaling and compression, from computations performed at lower resolutions. We show the effectiveness of our algorithms by testing on a large corpus of videos and on subjective data.","tags":[],"title":"Assessing the impact of image quality on object-detection algorithms","type":"publication"},{"authors":["Abhinau K. Venkataramanan","Chengyang Wu","Alan C. Bovik","Ioannis Katsavounidis","Zafar Shahid"],"categories":[],"content":"","date":1612242000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612242000,"objectID":"2f37e25627834263795d1eb4b36453f8","permalink":"https://abhinaukumar.github.io/publication/hitchhikers-guide/","publishdate":"2022-04-28T00:28:00-05:00","relpermalink":"/publication/hitchhikers-guide/","section":"publication","summary":"The Structural Similarity (SSIM) Index is a very widely used image/video quality model that continues to play an important role in the perceptual evaluation of compression algorithms, encoding recipes and numerous other image/video processing algorithms. Several public implementations of the SSIM and Multiscale-SSIM (MS-SSIM) algorithms have been developed, which differ in efficiency and performance. This ‚Äúbendable ruler‚Äù makes the process of quality assessment of encoding algorithms unreliable. To address this situation, we studied and compared the functions and performances of popular and widely used implementations of SSIM, and we also considered a variety of design choices. Based on our studies and experiments, we have arrived at a collection of recommendations on how to use SSIM most effectively, including ways to reduce its computational burden.","tags":[],"title":"A hitchhiker‚Äôs guide to structural similarity","type":"publication"},{"authors":["Abhinau K Venkataramanan","Chengyang Wu","Alan C Bovik"],"categories":[],"content":"","date":1608094800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608094800,"objectID":"3c2eba5bd0b3fb560769fdefdabe40ac","permalink":"https://abhinaukumar.github.io/publication/scaled-ssim/","publishdate":"2022-04-28T00:28:00-05:00","relpermalink":"/publication/scaled-ssim/","section":"publication","summary":"Many algorithms have been developed to evaluate the perceptual quality of images and videos, based on models of picture statistics and visual perception. These algorithms attempt to capture user experience better than simple metrics like the peak signal-to-noise ratio (PSNR) and are widely utilized on streaming service platforms and in social networking applications to improve users' Quality of Experience. The growing demand for high-resolution streams and rapid increases in user-generated content (UGC) sharpens interest in the computation involved in carrying out perceptual quality measurements. In this direction, we propose a suite of methods to efficiently predict the structural similarity index (SSIM) of high-resolution videos distorted by scaling and compression, from computations performed at lower resolutions. We show the effectiveness of our algorithms by testing on a large corpus of videos and on subjective data.","tags":[],"title":"Optimizing Video Quality Estimation Across Resolutions","type":"publication"},{"authors":["Abhinau Kumar"],"categories":["Projects"],"content":"Patients with prolonged stay often account for high resource consumption [1], and so, are a potential loss of revenue from a hospital management perspective. This is especially important in the case of Intensive Care Unit (ICU) and electronic ICU (eICU) patients due to their intensive need for care resources like nurses, drugs, surgeons, X-ray machines, etc., for continuous monitoring. Data mining and predicting/projecting the remaining length of stay (rLOS) for a patient will help allocate resources efficiently and provide timely services to patients.\nThe challenge for developing such a predictive model arises from the incomplete understanding of the complex clinical factors that may be involved and their interactions or relationships that lead to a specific LOS. This is a potential application for neural networks, which are considered universal function approximators. Since we are handling time-series data in the form of patient records, we mainly use CNNs and RNNs (and related architectures) to predict rLOS.\nIn addition to having a predictive model, an accurate understanding of the factors that are closely associated with LOS can allow for efficient hospital resource management, savings to a healthcare system, and better patient care by reducing readmissions. In this regard, neural networks fall short, as they are not interpretable. This ‚Äúblack box‚Äù behavior of neural networks leads us to the first tradeoff that we consider in this blog ‚Äì performance vs. interpretability.\nAlso, the dataset is sensitive for the patients and must be protected. All countries have laws to protect privacy, leading to difficulties in the data collection phase. Many hospitals may refuse to share healthcare data without any Health Insurance Portability and Accountability Act (HIPAA) agreements or data governance in place. Therefore, we also investigated a Federated Learning framework to address such privacy issues. This distributed learning framework leads us to the second tradeoff ‚Äì performance vs. privacy security.\nThis post is part of my term project for the Data Mining (EE 380L) class with Nishamathi Kumaraswamy, Zhengzhong Tu, Parveen Kumari, and Huancheng Chen who are all students of UT Austin.\nThe eICU Database The eICU Collaborative Research Database [2] is a multi-center intensive care unit database with high granularity data for over 200,000 admissions to ICUs monitored by eICU programs across the United States. The database is publicly available here, and comprises 200,859 patient unit encounters for 139,367 unique patients admitted between 2014 and 2015 to 208 hospitals located throughout the US.\nThe database is de-identified and includes vital sign measurements, care plan documentation, severity of illness measures, diagnosis information, treatment information, and more. The data is publicly available after registration, including completion of a training course on research with human subjects and signing of a data use agreement mandating responsible handling of the data and adhering to the principle of collaborative research. We would like to thank PhysioNet for granting accesss to the eICU database, which forms the backbone of all of our experiments.\nPreprocessing the Data To clean and preprocess the data, we use the same procedure used in [3]. To clean the data, we use several exclusion criteria.\n  We first only consider patients who are ‚Äúadults,‚Äù i.e., patients who are between 18 and 89 years old.\n  We only consider patients who have one ICU stay on record because we are interested in ICU length of stay, and we want to avoid ambiguity.\n  We exclude patients who have less than 15 or more than 200 records to keep the length of inputs significant but not excessive.\n  We exclude patients whose gender or hospital/ICU discharge status (alive vs. expired) are unknown.\n  Finally, we only include records that lie between the times of admission and discharge, which corresponds to having positive offset and rLOS.\n  As a side note, we use the term ‚Äúrecord‚Äù to refer to a set of 21 features collected at a time step, which is called an ‚Äúoffset.‚Äù After cleaning the data, we bin the offsets into hours. Within each hour, we consider the first record, and we impute its missing values with the average value over the hour. Often, features are missing over the entire hour, so we will need to impute values again at the hour level. We will discuss this ‚Äúsparsity‚Äù of the database in further detail below. To handle these missing values, we follow the literature and use ‚Äútypical‚Äù values of each feature.\nAfter cleaning and binning the data, we have a total of 74686 patients, leading to a total of just over 3.1 million records. We then split the data randomly to use 80% of the patients for training and 20% for testing. It is important to note that the records of each patient are assigned either to the training or the test set, but not both.\nWe have several categorical variables, and some can take several values. For example, admission diagnosis can take almost 400 different values. So, we cannot use the one-hot encoding to represent these variables without adding a large number of features.\nInstead, we handle categorical variables by a method called ‚ÄúTarget Encoding.‚Äù In target encoding, each value of a categorical variable is replaced by the average value of the target variable corresponding to all examples which have this value. More formally, the value $v$ of a categorical variable $X_j$ is replaced by\n$$E[Y | X_j = v]$$\nIn some cases, we may see new values of the categorical variable in the test data that we have not encountered. In this case, we default to using the average value of the target variable instead of such a conditional average.\nIn our code, we created a class TargetEncoder to carry this out, which adheres to the Scikit-Learn API format. So, in practice, target encoding variables in a dataset is as simple as\nencoder = TargetEncoder() encoder.fit(X, y, ['column1', 'column2', 'column3']) X = encoder.transform(X)  Finally, we applied a ‚ÄúMin-Max Scaler‚Äù which maps the values of each feature to the range $[-1, 1]$. This kind of scaling helps improve the convergence of many machine learning models, especially those that use gradient-based optimization methods.\nUnderstanding the Data Sparsity Before we begin training the models, it is useful to understand the nature of the database. As we have mentioned above, a notable feature of the database is its sparsity. This is not surprising because not every feature is recorded at each time step. Often, the frequency at which, say, oxygen saturation is measured may be higher than that at which blood glucose level or pH are measured. This is because oxygen saturation is easier to measure, and is not a diagnostic test specific to any disease.\nIn the following figure, we show the fraction of missing values of each feature.\nIn this plot, we plot the histogram of the number of missing features in each record.\nFrom these two plots, it‚Äôs clear that the dataset is quite sparse, with almost most records missing at least three features. Also, the features that are missing the most often are the pH, glucose, and FiO2, which are likely measured only for specific patients, depending on the necessity. So, we don‚Äôt expect them to be missing completely at random (MCAR), and we cannot omit any records or features. As a result, imputation becomes a crucial data processing step. Also, because the data is time-series, imputing using the mean or median, which are common choices, can give undue weight to patients who have more records. So, to avoid these complex considerations, using ‚Äútypical‚Äù values is a good practical choice.\nVisualizing Features and Targets After target encoding, we have converted all variables into numerical variables, so we can visualize their dependencies using the correlation plot below.\nLooking at the plot, it is not surprising that the height and the weight are highly correlated. Another interesting observation is that the features obtained from the Glasgow Coma Score (GCS) suite - GCS Total, Eyes, Verbal, Motor are all highly correlated. It is important to note that these features were originally categorical variables, so these correlations are over a small set of unique values. Other than these, the variables are weakly correlated, so we can expect that they provide diverse information about the LOS/rLOS targets.\nTo round out the data visualization exercise, we show the histogram of lengths of stays below. Since most values lie in the range 0-20, we restrict the histogram to this range.\nIn the histogram, we can notice several local modes corresponding to integer values of LOS. We think this shows a natural bias of hospitals to discharge patients in whole numbers of days, perhaps due to administrative convenience.\nLooking Deeper Beyond these summaries, we want to look a little deeper into the data. Specifically, we are interested in finding the effect of gender and ethnicity on the LOS and the discharge status - alive vs. expired. Such an analysis will help us uncover biases (conscious or otherwise) in the healthcare system that need to be addressed.\nTo find the effect of gender on the LOS, we will compare the average LOS between male and female patients. Visually, we can illustrate the values in the form of the following box plot.\nAnalytically, because we want to compare the means of two groups, we will use a statistical test called the ‚Äúindependent t-test.‚Äù We will skip the details here, but this blog post is a good resource to learn about the t-test. The t-test returned a ‚Äúp-value‚Äù of about $10^{-3}$. Loosely speaking, a low p-value means that our observations are very unlikely if the means were the same. So, we say that the difference in the means is ‚Äúsignificant,‚Äù i.e., the mean LOS in ICUs depends on gender. We see from the data that men have a larger average LOS, and from the box plot, we see that men are also more likely to have abnormally high LOS when compared to women.\nWe can ask the same question of ethnicity. The dataset contains patients of the following ethnicities - Asian, African American, Caucasian, Hispanic, Native American, Other/Unknown. Once again, we can visualize the data using a box plot.\nBut now that we have more than two groups, we use an F-test (also called a one-way analysis of variance), which compares the means of several groups. The p-value returned by the F-test was $10^{-17}$, so once again, the test is ‚Äúsignificant,‚Äù and the mean LOS varies with ethnicity. From the data, we see that African-American patients have the longest stays, while Native Americans have the shortest. We also see from the box plot that Caucasian patients are more likely to have abnormally high LOS when compared to Asian, Hispanic, or Native American patients.\nTo investigate the effect of gender on discharge status, we can tabulate the number of men and women who were discharged alive or expired. Such a table is called a ‚Äúcontingency table.‚Äù\n .basic-styling td, .basic-styling th { border: 1px solid #999; padding: 0.5rem; }       Alive Expired     Female 31943 (94%) 2054 (6%)   Male 38146 (93.8%) 2543 (6.2%)     Analytically, we want to test if the discharge status depends on gender. Because we are testing the dependence between two categorical variables, we use a $\\chi^2$ test of indepence. For a $\\chi^2$ test, the p-value is the probability of observing the data if the two quantities (gender and discharge status, in our case) are independent. So, a low p-value means that the two quantities are dependent. The p-value returned by the $\\chi^2$ test was 0.245, which means that we did not find a significant dependence between discharge status and gender. As above, we can compute a similar contingency table to analyze the effect of ethnicity on discharge status.\n .basic-styling td, .basic-styling th { border: 1px solid #999; padding: 0.5rem; }      Ethnicity / Discharge Status Alive Expired     Unknown/Other 4075 (93.3%) 291 (6.7%)   Asian 1136 (94.3%) 69 (5.7%)   African American 7667 (94.5%) 442 (5.4%)   Caucasian 54007 (93.7%) 3609 (6.3%)   Hispanic 2818 (94.9%) 151 (5.1%)   Native American 386 (91.7%) 35 (8.3%)     Applying a $\\chi^2$ test again, the p-value that we obtained was $10^{-3}$, which means that the test was significant; the discharge status does depend on ethnicity. From the contingency table, we see that Native Americans are significantly more likely to die in ICUs than patients of other ethnicities. This may explain the shorter ICU stays.\nA more detailed exploratory analysis of the dataset can be found in [4].\nRelated Work Our proposed work follows the footsteps of other researchers who answer similar problems using the same, or similar, data. Harutyunyan et al. [5] trained a multi-task RNN model on the MIMIC-III [6] database to perform\n In-hospital mortality prediction, based on the first 48 hours of ICU stay Decompensation prediction, as an early-warning system to predict the rapid deterioration of the patient over the next 24 hours Length of stay prediction Phenotype classification, classifying which of 25 acute care conditions are present in the patient‚Äôs record  The use of a multi-task model leads to significant feature reuse and a reduction in the total computational load during inference.\nRajabalizadeh et al. [4] conducted an exploratory analysis of the eICU dataset, quantifying the distribution of frequencies of visits, co-presence of diagnoses, racial bias, etc. They also show that the Acute Physiology Age Chronic Health Evaluation (APACHE) III system does not predict LOS well.\nSheikhalishahi et al. [3] proposed a Bidirectional Long Short Term Memory (BiLSTM) model to perform the same four tasks. An important difference between the two models, aside from being trained on different databases, is that Sheikhalishahi et al. use a single-task BiLSTM, while Rajabalizadeh et al. use a multi-task RNN model.\nOther than unsupervised and supervised methods, reinforcement learning (RL) models have also been developed using the eICU database. Komorowski et al. Komorowsksi et al. [7] trained an RL agent to learn an optimal ‚Äúclinician policy,‚Äù to reduce the mortality risk of patients. The agent was trained and validated using the MIMIC-III database, while the eICU database was used for model testing.\nThe Models We Tested We can broadly divide the different trained models into three broad categories, baseline, advanced, and federated models. The baseline models consist of standard regression models like decision trees, linear regression, and support vector regressors, and ensemble models built on these base regressors.\nAmong the advanced models, we have Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and an interpretable RNN called the Reverse Time Attention (RETAIN) model.\nWe use the RETAIN and Federated Learning models to understand the tradeoffs in performance when enforcing interpretability and privacy, respectively. We report the performance of all our models using three metrics - the Mean Absolute Error (MAE), the (Root) Mean Squared Error (RMSE / MSE), and the $R^2$ score. While the MAE and the (R)MSE are self-explanatory, the $R^2$ score warrants explanation.\nThe $R^2$ score is the fraction of the variance in targets that is explained by the regression model. So, if the predictions are perfect, all the variance in the data is explained by the model, and the $R^2$ score achieves its maximum value of 1. If the model is naive and predicts the mean value of the target, irrespective of input, none of the variation is explained by the model and the $R^2$ score is 0. Note that the $R^2$ score can be negative, which means that the model performs worse than a naive constant-prediction model.\nBaseline Models We considered the following baseline regression models in our experiments.\n Decision trees Regularized linear models (Ridge, LASSO) Support Vector Machines (SVM)  We also considered the following ensemble models\n Random forest regressor (RF) Extreme gradient boosting (XGB) Light gradient boosting machines (LGBM).  We chose this set of models because of their simplicity and the ease of output interpretation. Many of these models are also commonly used in the bio-sciences and medical communities, thus providing a robust comparison of the tradeoffs we explore in this blog. We will explain and compare these models in subsequent sections.\nDecision Trees (DT) The main idea behind Decision Trees (DTs) is to create a model that predicts the target variable (rLOS or LOS) by learning simple decision rules deduced from the 21 data features. Three models (each with a different set of inputs) were explored with decision trees. We will call them BMDT1, BMDT2, and BMDT3 for purposes of model tracking in this blog.\nWe used the Scikit-Learn library module Decision Tree Regressor to build our DT regressors. The default criterion of \u0026ldquo;mean square error\u0026rdquo; was used as the objective. This criterion minimizes the L2 loss when using the mean from each terminal node.\nWe trained the BMDT1 model on the entire database to predict the rLOS, considering each record as an independent sample. We also trained an optimized version of this model, where the best parameters were found using a grid search.\nThe parameter ranges for grid search were varied based on the $R^2$ results achieved from predicting the training data ground truth values. The BMDT1 model was the worst-performing DT model based on our comparison metrics ($R^2$ on test data). We also used this model to test for feature robustness by using features without target encoding or feature scaling.\nTo train the BMDT2 models, we used either the first or the last record of each patient as a proxy for the whole time-series data. The BMDT2 models also differ from BMDT1 because they predict the LOS instead of the rLOS.\nWe modify the BMDT2 models by using the number of records (offsets) for a patient as an input feature, instead of the offset itself. This engineered feature is a way for baseline models to include some information about the time-series portion of the data. We call this model BMDT3.\nIt is natural to expect that that offset of the last record is very indicative of the LOS of the patient. So, we would expect the baseline models that were trained on the last record to perform very well. We will show the impact of using the last record, in our results.\nMore Baselines (Ridge, Lasso, SVM, RF, XGB, LGBM) Until now, we have just tried various features and different training methods using the decision tree regressor as an example. Here we branch out to try more popular baseline regression models to get a sense of how these regressors perform on this dataset. These models include Ridge, Lasso, SVM, RF, XGB, and LGBM.\nWe used the best two groups of features we observed in decision trees - one is to predict the LOS using the features of the first record and an additional feature, the number of records, for each patient; another one is to predict the LOS using the last record (including the last offset as the 21st feature) of features.\nLinear Models: A linear model fits a multiple linear regression with coefficents $w=(w_1,‚Ä¶,w_p)$ to minimize the residual sum of squares, $||Xw-y||^2_2$, between the observed targets in the dataset and the predictions. Ridge is a variant of multiple linear regression that imposes an $L_2$ penalty on the size of the coefficents: $||Xw-y||^2_2+\\alpha||w||^2_2$. On the other hand, Lasso imposes an $L_1$ penalty on the size of the coeffiicents: $||Xw-y||^2_2+\\alpha||w||_1$. As a result, Lasso produces sparser results as compared to Ridge, i.e., it typically has a higher number of zero coefficients.\nSupport Vector Regression: SVM is a maximum-margin method that only penalizes the points that lie closest to the decision boundary of the classifier. SVR is an extension of SVM to the regression problem, where the model depends only on a subset of the training data because the cost function ignores samples whose prediction is close to their target. Since SVR with non-linear kernels scales poorly with data and feature dimension, we stuck to a Linear SVR.\nRandom Forest: RF is an ensemble of many decision trees that are built from bootstrap samples from the training set. A bootstrap sample is obtained by randomly sampling the training set with replacement. Another layer of randomness is introduced by fitting each node during the construction of a tree on a random subset of features. RF has been shown to have very good generalization properties compared to many other models.\nXGBoost: XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible, and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. XGBoost scales well to run on major distributed environments like Hadoop, SGE, and MPI and can solve problems beyond billions of examples.\nLight GBM: LightGBM is a gradient boosting framework that uses tree-based learning algorithms. The main advantages of LightGBM are faster training speed, higher efficiency, lower memory usage, better accuracy, support of parallel and GPU learning, and the capability to handle large-scale data.\nHyperparameter tuning: The performance of regressors is greatly dependent on their hyperparameters. So, we adopted a randomized grid search cross-validation on the training set to search for the best hyperparameters. However, there is no standard on how to select the parameter range for each regression model. We referred to both official Scikit-learn documents and unofficial blogs to get an empirical parameter search space for each model. The final parameter search spaces are as follows: Ridge:\nparam_grid = {'alpha': np.logspace(-3, 3, 10)}  Lasso:\nparam_grid = {'alpha': np.logspace(-5, 0, 10)}  LinearSVR\nparam_grid = {'C': np.logspace(-5, 15, 15, base=2)}  RF\nparam_grid = {'n_estimators': [100, 200, 300, 400, 500], 'max_features': ['auto', 'sqrt'], 'max_depth': [3, 4, 5, 6, 7, 9], 'min_samples_split': [2, 5, 10, 15], 'min_samples_leaf': [1, 2, 5], 'bootstrap': [True, False]}  XGBoost\nparam_grid = {'max_depth': range(3,12), 'min_child_weight': range(1,10), 'gamma': list([i/10.0 for i in range(0,5)]), 'subsample': list([i/10.0 for i in range(6,10)]), 'colsample_bytree': list([i/10.0 for i in range(6,10)]), 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]}  LightGBM\nparam_grid = {'num_leaves': [7, 15, 31, 61, 81, 127], 'max_depth': [3, 4, 5, 6, 7, 9, 11, -1], 'learning_rate': [0.001, 0.005, 0.01, 0.05, 0.1, 0.3, 0.5], 'n_estimators': [100, 200, 300, 400, 500], 'boosting_type': ['gbdt', 'dart'], 'class_weight': [None, 'balanced'], 'min_child_samples': [10, 20, 40, 60, 80, 100, 200], 'subsample': [0.5, 0.7, 0.8, 0.9, 1.0], 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 10, 100], 'reg_lambda': [1e-5, 1e-2, 0.1, 1, 10, 100]}  Convolutional Neural Networks (CNNs) It is well known that CNNs are highly noise-resistant models, and they can extract very informative features that capture local contexts. While CNNs have typically been used in image processing and computer vision, in this section, we will examine in detail how we can apply a 1-D CNN to time-series data.\nAt the first layer, the convolution kernels (windows) always have the same depth as the number of features in time series, while their width can be varied. This way, the kernel moves in one direction from the beginning of a time series towards its end, performing convolution. In other words, we mapped each feature to a channel of the convolution layer.\nEach element of the kernel is multiplied by the corresponding element of the time-series input that it covers at a given point. The results of this multiplication are added together, and a nonlinear activation function (like ReLU) is applied to the value. The resulting value becomes an element of a new ‚Äúfiltered‚Äù time series, and the kernel moves forward along the time series to produce the next value. The depth of the new ‚Äúfiltered‚Äù time series is the same as the number of convolution kernels. Depending on the width of the kernel, different aspects, properties, and ‚Äúfeatures‚Äù of the initial time series get captured in each of the new filtered series.\nThe baseline model is a Multi-Layered Perceptron (MLP), which processes only one record at a time. Point-wise convolutions compute higher-level features from interactions in the feature domain.\nWe include temporal information by processing all the records corresponding to a patient together and having wider kernel sizes. In these temporal models, wider kernels compute interaction features from the feature set at each time step.\nWe report the results for the following models.\n Kernel size = 1 for all layers Kernel size = 3 for all layers Kernel size = 15 for all layers Variable kermel size: Kernel size = 3 for the first two layers and 15 for the last layer  Recurrent Neural Networks (RNNs) While CNNs can extract features based on neighboring records in time, they do not truly address the time-series nature of the data. Also, they are unable to extract long-range dependencies between inputs. Recurrent Neural Networks (RNNs) were designed to handle such time-series data and have found great success, especially in the field of natural language processing (NLP).\nThe basic unit that makes a neural network ‚Äúrecurrent‚Äù is called a recurrent layer. There are several types of recurrent layers, but we can describe a general recurrent layer by a function that satisfies the relationship, $$v_t, c_t = RNN(x_t, v_{t-1}, c_{t-1})$$\nIn simple terms, the output (also called the hidden state) $v$ and ‚Äúcell state‚Äù $c$ at every time instant is a function of the current input, the previous output, and the previous cell state. We can illustrate this as\n$v_{t-1}$ and $c_{t-1}$ themselves can be written in terms of the previous outputs. So, $$v_t, c_t = RNN(x_t, f(x_{t-1}, v_{t-2}, c_{t-2})) = RNN(x_t, f(x_{t-1}, f(x_{t-2}, v_{t-3}, c_{t-3}))) \\dots $$\nIn RNN literature, this is called ‚Äúunrolling‚Äù because we can represent this using the block diagram\nwhich is an ‚Äúunrolled‚Äù version of the previous figure. We can repeat this process endlessly, so is it turtles all the way down? Not really. In practice, we can assume that the input $x$ starts at t = 1, and initialize $v_0 = c_0 = 0$ for simplicity. This choice is arbitrary. You could use 0, 1, $\\pi$, $e$, $e^\\pi$, or anything else, depending on how creative you feel. Another common choice for the ‚Äúinitial state‚Äù $(v_0, c_0)$ is random noise, which adds a layer of stochasticity to the model.\nThere is some interesting evidence that shows that learning the initial state like a parameter can lead to an improvement in performance. This can be taken a step further by predicting the hidden state as a function of the input. Such models are called ‚ÄúContextual‚Äù RNNs [8]\nThose with a background in Digital Signal Processing (DSP) might find the above expression for a general recurrent layer familiar. The time-domain description of an Infinite Impulse Response (IIR) filter $$y[t] = \\sum_{k=0}^{p} b_k x[n-k] - \\sum_{k=1}^{q} a_k y[n-k] $$ resembles the description of a kind of ‚Äúlinear‚Äù recurrent layer, when $p = q = 1$. In time series analysis, this is called the Auto-Regressive Moving Average (ARMA) model, where the $\\sum a_k y[n-k]$ is the AR part, and $\\sum b_k x[n-k]$ is the MA part. In practice, however, recurrent layers are often non-linear in nature, to build more complex time-series models.\nWhile many recurrent layers exist, we used the Long Short Term Memory (LSTM) cell. The LSTM is one of the most popular choices of recurrent layers, due to its ability to capture long-range dependencies.\nSource: ResearchGate\nA detailed explanation of LSTMs warrants a blog post of its own, of which this is a fantastic example. The broad idea is that the line that runs through the LSTM offers a pathway for information from the past to be passed to the future easily. This gives the RNN cell a sort of ‚Äúmemory,‚Äù and the cell state is modified by the input at every time step. Finally, the output is a function of the cell state and the input. The pointwise multiplications are interpreted as ‚Äúgates‚Äù that decide the amount of ‚Äúattention‚Äù (in the range $[0, 1]$ or $[-1, 1]$) given to each feature.\nReverse Time Attention Models (RETAIN) The Reverse Time Attention (RETAIN) model is an interpretable alternative to RNNs which was proposed in [9]. In this blog, we will look at a simplified version of the model, which worked better than the one proposed in the paper. We will also investigate the effect of the direction of time.\nSpecifically, we will feed the input to both the traditional RNN and the RETAIN model in three ways - forward, which is the standard form of the input, backward, by reversing the order of records in the input, and bidirectional (we call this BiRETAIN), by feeding in both the forward and reverse directions. Nevertheless, we will call all these models ‚ÄúRETAIN‚Äù as they are based on the original RETAIN architecture.\nThe RETAIN model can be illustrated by the figure\nAs we had mentioned in the previous section, a core concept of RNNs is ‚Äúgating‚Äù, which is achieved by ‚Äúattention‚Äù. RETAIN provides an interpretable model by explicitly finding record-level (corresponding to each time step) and feature-level (corresponding to each feature at a time step) attention values, as a function of the ‚Äúembedding‚Äù v, which is a linear function of the input features. As with all attention, these values are multiplied pointwise to the values of the embedding, and each gated embedding is mapped to the output, which is the rLOS in our case. Formally, we can write these as the following set of equations $$v_t = W_{emb}x_t + b_{emb}, \\quad 1 \\leq t \\leq T$$\nThat is, the embedding at each time step is a linear (technically, affine) function of the input features. This is equivalent to a single layer perceptron with no non-linear activation.\n$$z_{\\alpha1} \\dots z_{\\alpha T} = RNN_\\alpha(v_1 \\dots v_T) $$ $$z_{\\beta1} \\dots z_{\\beta T} = RNN_\\beta(v_1 \\dots v_T)$$\nThese two networks obtain feature vectors which are used to compute\n$$\\alpha_t = \\sigma(w_\\alpha^Tz_{\\alpha t} + b_\\alpha)$$ $$\\beta_t = \\tanh(W_\\beta z_{\\beta t} + b_\\beta)$$\nThe $\\sigma$ and $\\tanh$ are the sigmoid and tanh activations, which respectively map inputs to the range $[0,1]$ and $[-1, 1]$. These $\\alpha_t$ and $\\beta_t$ values are our attention values. We can now multiply these pointwise with the embeddings, to obtain the gated embeddings\n$$\\tilde{v}_t = \\alpha_t \\beta_t \\odot v_t$$\nwhere $\\odot$ is called the ‚ÄúHadamard product‚Äù which is just a fancy term for element-wise multiplication.\nFinally, the gated embedding is mapped to the output using the linear (again, technically, affine) mapping $$y_t = w^T \\tilde{v_t} + b$$\nThis final mapping is just linear regression, but now, the features $\\tilde{v_t}$ are learned from data. We will show an even stronger relationship with linear regression when we discuss the interpretability of RETAIN.\nAs mentioned earlier, a neat implementation trick that can be very useful is that we can apply an MLP (of which a linear function is a very special case) to a set of points using 1D convolutions. We can do this by mapping the features to the channels of the convolution and using a kernel size of 1. Using this trick, we can avoid looping over inputs to apply the MLP, which can be very inefficient.\nNow, we look at the most interesting part of RETAIN - how do we interpret the predictions? As hinted earlier, the interpretability of RETAIN comes from its relationship to linear regression. To understand this connection, let us take another look at the equations that describe RETAIN.\nSubstituting the value of the gated embeddings, $$y_t = w^T (\\alpha_t \\beta_t \\odot v_t) + b$$\nWe can expand this further by writing the embeddings as a function of inputs, i.e. $$ \\begin{aligned} y_t \u0026amp;= w^T (\\alpha_t \\beta_t \\odot (W_{emb} x_t + b_{emb})) + b \\\\ \u0026amp;= (\\alpha_t (\\beta_t \\odot w)^T W_{emb}) x_t + (\\alpha_t (\\beta_t \\odot w)^T b_{emb} + b) \\end{aligned} $$ Even though the math looks complicated, we have basically written $y_t$ as a function of the form $$y_t = \\tilde{w_t}^T x_t + \\tilde{b_t}$$\nThis resembles the linear regression formulation, so we can interpret the coefficients $\\tilde{w_t}$ as weights given to each feature. So, the RETAIN model is a method by which the weights of a linear regression model are predicted as a function of the input, and these weights represent the contribution of each feature.\nFederated Learning Standard machine learning approaches like the baseline model and the interpretable models require centralizing the training data on one machine or in a data center. Because our dataset is from the healthcare area, we need to consider the important problems of protecting patients‚Äô privacy and the business secrets of hospitals. To address this, we use a Federated Learning [10] model for distributed learning, which does not require collecting all data in a center. So, the data providers do not need to share their data, making the training process more secure.\nIn our federated learning model, we assume that our eICU data originated from different hospitals. To simulate this, we divided our original dataset into $k$ sub-datasets, representing $k$ hospitals. We called these hospitals clients in the rest of the blog. As the figure shows, all the clients train their models locally on their sub-datasets. In every epoch, all the clients can get their local model, and they only upload the models instead of data to the server. The server integrates all the local models into the global model and then sends it back to all clients. All the clients update their models to the updated global model and search for better weights. Because all the clients only share their models instead of original data, federated learning addresses privacy problems.\nSource: NVIDIA\nHowever, it is important to note that the federated learning model has the following disadvantages:\n Training local models with sub-datasets will decrease the accuracy of the global model because the global model does not use whole data to search for optimal weights of the network. Instead, the local models are merged in a weighted way, which influences the accuracy of the global model. If differences between local models trained on sub-datasets are large, the global model suffers from the worst local model. Communication between clients, where the local models reside, and the server, where the global model resides, takes a significant amount of time.  Nevertheless, we are interested in using federated learning to introduce a privacy constraint to model building. In this experiment, we divided the original training set into five equal-sized parts. For comparison, we chose BiLSTM and BiRETAIN models as our baseline models. These two models have the lowest RMSE and highest $R^2$ among all the models in previous sections, making them good candidates for illustrating the effect of the federated learning model.\nIID data group In this section, we randomly divide the training set into five parts. So, all the features in these sub-training sets have similar distributions, which helps reduce the variation in performance between local models. With these five independent and identically distributed (IID) training sets, we have two ways of training the local models:\n Full training: At every epoch, we train all the local models and update the weights of the global model with these five models. Partial training: At every epoch, we randomly choose two of the five local models. Then, we only train these models and update the weights of the global model using these two.  Non-IID data group In this section, we divide the training set into five equal-sized parts according to three features - height, weight, and age. As a result, the features in these sub-training sets have different distributions. The criteria used to construct the sub-training sets are shown below.\n .basic-styling td, .basic-styling th { border: 1px solid #999; padding: 0.5rem; }      Features/Group Group A Group B Group C Group D Group E     Height (cm) 100 - 160 161 - 167 168 - 173 174 - 180 181 - 218   Weight (kg) 30 - 65 66 - 75 76 - 86 87 - 102 103 - 272   Age (years) 18 - 53 54 - 64 65 - 71 72 - 78 79 - 89     For convenience, we only use the partial training strategy on non-IID data.\nResults Decision Trees Below are the optimized parameter values for the best decision tree.\n  .basic-styling td, .basic-styling th { border: 1px solid #999; padding: 0.5rem; }      Parameter Value     DT Maximum depth 10   DT Max leaf nodes 200   DT Max sample leaf 100   DT Min samples split 10      We found that the optimized decision tree that used the last record of each patient performed the best with an $R^2$ value of 0.96 as shown in the table below.\n  .basic-styling td, .basic-styling th { border: 1px solid #999; padding: 0.5rem; }      Model Target MAE MSE $R^2$     BMDT1 rLOS 1.696 6.944 -0.834   BMDT1 ‚Äì Optimized rLOS 1.237 3.46 0.084   BMDT2 ‚Äì Optimized (first record) LOS 1.268 3.779 0.092   BMDT2 ‚Äì Optimized (last record) LOS 0.072 0.150 0.964   BMDT3 ‚Äì Optimized (first record and # of offsets) LOS 0.552 1.603 0.615   BMDT3 ‚Äì Optimized (last record and # of offsets) LOS 0.584 1.671 0.599      Overall Baseline model Performance Using the first record plus the number of offsets for each patient to predict LOS, the performance is as follows.\n  .basic-styling td, .basic-styling th { border: 1px solid #999; padding: 0.5rem; }      Model MAE MSE $R^2$ Time (sec)     Decision Tree 0.552 1.603 0.615 0.222   Ridge 0.623 1.735 0.583 0.008   Lasso 0.623 1.735 0.583 0.016   LinearSVR 0.492 1.942 0.533 1.483   RF 0.559 1.598 0.616 50.591   XGBoost 0.537 1.521 0.634 5.914   LightGBM 0.533 1.500 0.639 9.239      From this table, we can see that using the first record plus the number of offsets for each patient, we can get reasonable performance. The best model in terms of MAE is the LinearSVR, while in terms of MSE and $R^2$, LightGBM ranks first. We also observed that linear models trained very fast, while tree-based models were generally slow.\nUsing the last record for each patient to predict LOS, we obtain the performance\n  .basic-styling td, .basic-styling th { border: 1px solid #999; padding: 0.5rem; }      Model MAE MSE $R^2$ Time (sec)     Decision Tree 0.072 0.150 0.964 0.204   Ridge 0.057 0.010 0.997 0.014   Lasso 0.057 0.011 0.997 0.013   LinearSVR 0.052 0.011 0.997 7.017   RF 0.058 0.024 0.994 62.924   XGBoost 0.059 0.016 0.996 3.634   LightGBM 0.074 0.073 0.982 0.581      From this table, we see that using the last record with the time offset, all the models achieved nearly perfect predictions. Specifically, the Ridge and Linear SVR models performed the best in terms of MAE and MSE, respectively, and both models achieved the highest $R^2$. Convolutional Neural Networks The results of our experiments with baseline MLP and CNNs having various kernel sizes are shown below.\n  .basic-styling td, .basic-styling th { border: 1px solid #999; padding: 0.5rem; }      Model MAE RMSE R2     Baseline MLP 1.35 2.14 -0.02   Kernel size = 1 1.19 1.97 -0.03   Kernel size = 3 1.08 1.873 0.073   Kernel size = 15 1.41 1.99 -0.05   Variable kernel size 0.97 1.74 0.19      From these results, we can make the following observations. The most complex model, which has a kernel of size 15 performs the worst. So there is some gap between our intuition and how the CNNs behave with this kind of dataset.\nAlso, while CNNs are computationally less intensive as compared to RNNs, they perform poorly because the features extracted from the time-series data are shallow, in the sense that only closely localized relationships between a few neighbors are factored into the feature representations.\nRecurrent Neural Networks and RETAIN The performance of various RNN and the corresponding RETAIN models have been listed in the table below.\n  .basic-styling td, .basic-styling th { border: 1px solid #999; padding: 0.5rem; }      Model MAE RMSE R2 Score     LSTM 1.166 1.927 0.019   LSTM (Reversed) 0.149 0.325 0.972   BiLSTM 0.108 0.316 0.973   RETAIN 1.169 1.927 0.019   RETAIN (Reversed) 0.157 0.383 0.961   BiRETAIN 0.085 0.255 0.983      From this table, we see that the two bidirectional models outperform all other models, and the BiRETAIN model, in particular, performs the best. This makes sense because the bidirectional models can capture dependencies in both forward and reversed time. More importantly, all RETAIN models achieve a similar performance as their corresponding RNN counterparts. This shows that we can offer interpretability without incurring any cost in performance. As explained above, we interpret the RETAIN model by examining the coefficients of the effective linear regression model. We visualize the change in the coefficients over time for an example from the test set below.\nFrom the figure, we see that the weights of the offset and the admission diagnosis change rapidly, while the other weights remain approximately constant. This makes sense because the admission diagnosis may become less important as time progresses, and the change in rLOS over time is fuelled by the offset.\nFederated Learning Performance on IID Data  .basic-styling td, .basic-styling th { border: 1px solid #999; padding: 0.5rem; }   Baseline     Model Time (min.sec) MAE MSE $R^2$     BiLSTM 3.24 0.108 0.316 0.973   BiRETAIN 7.48 0.085 0.255 0.983      .basic-styling td, .basic-styling th { border: 1px solid #999; padding: 0.5rem; }   IID - Full Training     Model Time (min.sec) MAE MSE $R^2$     BiLSTM 4.27 0.2442 0.3947 0.9605   BiRETAIN 8.55 0.1791 0.3757 0.9654      .basic-styling td, .basic-styling th { border: 1px solid #999; padding: 0.5rem; }   IID - Partial Training     Model Time (min.sec) MAE MSE $R^2$     BiLSTM 1.45 0.2970 0.3516 0.9437   BiRETAIN 3.40 0.2644 0.3903 0.9307     From the tables above, we can see the federated learning model with IID sub-datasets doesn‚Äôt affect the performance of the baseline models much.\nUnder the full-training regime, the training time for BiLSTM and BiRETAIN federated models is considerably more (about 30% for BiLSTM and 15% BiRETAIN) than the corresponding baseline models because of the communication cost mentioned before. The performance of the federated BiLSTM model decreases slightly, and $R^2$ doesn‚Äôt change much. In contrast, the federated BiRETAIN model is affected significantly, with $R^2$ decreasing from 0.9819 to 0.9654.\nUnder the partial-training regime, we see that the training time decreases dramatically because only 40% of the data is used at each epoch. In this regime, the MAEs for both models increase dramatically while the $R^2$ decreases. These two partially-trained models are affected by the federated framework, but the performances are still acceptable.\nPerformance on non IID Data  .basic-styling td, .basic-styling th { border: 1px solid #999; padding: 0.5rem; }   Grouping by Height     Model Time (min.sec) MAE MSE $R^2$     BiLSTM 1.56 0.2579 0.3529 0.9474   BiRETAIN 3.44 0.3114 0.4017 0.9149      .basic-styling td, .basic-styling th { border: 1px solid #999; padding: 0.5rem; }   Grouping by Weight     Model Time (min.sec) MAE MSE $R^2$     BiLSTM 1.58 0.2921 0.3868 0.9327   BiRETAIN 3.52 0.3120 0.4976 0.8774      .basic-styling td, .basic-styling th { border: 1px solid #999; padding: 0.5rem; }   Grouping by Age     Model Time (min.sec) MAE MSE $R^2$     BiLSTM 1.45 0.3275 0.3824 0.9319   BiRETAIN 3.31 0.2873 0.4071 0.9269     From the tables above, we see that all the models in the non-IID setting perform worse than the corresponding partially-trained model on IID datasets. Models in height and age groups show a small decline in $R^2$, while the models in the weight group show a steeper decline. This is especially the case for the BiRETAIN model, with $R^2$ decreasing from 0.9307 to 0.8774.\nComprehensive Results In the table below, we compare the best performing models of each type. We omit the federated learning models, because they will naturally achieve a lower performance, while the real benefit is being able to protect patient privacy.\n .basic-styling td, .basic-styling th { border: 1px solid #999; padding: 0.5rem; }      Model Target MAE RMSE $R^2$     BMDT2 (last record) LOS 0.072 0.388 0.964   Ridge (last record) LOS 0.057 0.100 0.997   Linear SVR (last record) LOS 0.052 0.105 0.997   Variable size CNN rLOS 0.97 1.74 0.19   BiRETAIN rLOS 0.085 0.255 0.983     Because the targets predicted by the baselines models are different from those predicted by the advanced models, knowing the LOS is equivalent to know the rLOS, because the current offset is always known. So, even though we cannot directly compare the values, it is clear that simple regression models are able to match, if not exceed, the performance of sophisticated neural networks by having access to the last data. This means that almost all of the predictive power of a patient\u0026rsquo;s records is in the last record.\nWhat Have We Learnt? Based on our experiments, we have the following key takeways.\n Simple regressors that have access to the last record achieve almost perfect predictions. CNNs are unable to capture long-term dependencies, so they perform poorly in spite of having access to all records. The RETAIN model is able to match, and even out perform, traditional RNN models, with the added benefit of interpretability. In most cases, the federated learning model can guarantee the confidentiality of patient data, while incurring a small sacrifice in performance.  The code for all our experiments can be found at this GitHub repository.\nSome Caveats The eICU database is large and comprises of routinely collected clinical data in an eICU setting. However, the database also contains non-ICU stays (e.g., step down units SDUs). This means that some patients (patient unit stay ids) in our dataset could be from SDUs, rather than an actual ICU unit stay. Nevertheless, we do not expect this to affect our results because all our models use the same data, and we only analyze tradeoffs.\nAn important observation about our results is that models that did not have access to all records (and so, the last record), performed very poorly. Having access to all records is a profoundly impractical assumption, because simply knowing that we have all the records means knowing that the patient\u0026rsquo;s stay has ended, which renders predicting the LOS useless.\nSo, even though we have shown that using interpretable and confidential models does not affect performance too much, the base problem of predicting rLOS in a causal way, meaning that we do not have access to future records, remains a very hard problem.\nMoreover, while our experiments with federated learning showed good results, there is still room for improvement in the following areas:\n Our main goal is regression, which is simpler than classification and other advanced tasks. So, separation of original data doesn\u0026rsquo;t affect the results much, and we may not have stretched our federated learning models to see its side effects. Even though our non-IID groups had different height, weight, and age distributions, all the sub-datasets were of the same size. Therefore, we did not get the results under the extreme situation that one of the sub-training set is significantly larger than another. In such a situation, we expect the performance of the federated models to decrease dramatically. All clients have the same form of data, which only differ in their values. This homogeneity might be inflating the results of our non-IID models.  References [1] Rapoport J, Teres D, Zhao Y, Lemeshow S: Length of stay data as a guide to hospital economic performance for ICU patients. Med Care. 2003, 41: 386-397. 10.1097/00005650-200303000-00007.\n[2] Pollard, T. J., Johnson, A. E., Raffa, J. D., Celi, L. A., Mark, R. G., \u0026amp; Badawi, O. (2018). The eICU Collaborative Research Database, a freely available multi-center database for critical care research. Scientific data, 5, 180178.\n[3] Sheikhalishahi, S., Balaraman, V., \u0026amp; Osmani, V. (2020). Benchmarking machine learning models on multi-centre eICU critical care dataset. Plos one, 15(7), e0235424.\n[4] Rajabalizadeh A, Nia N, Safaei N, Talafidaryani M, Bijari R, Zarindast A, Fotouhi F, Salehi M, Moqri M (2020). Exploratory Analysis of Electronic Intensive Care Unit (eICU) Database. medRxiv 2020.03.29.20042028; doi: https://doi.org/10.1101/2020.03.29.20042028\n[5] Harutyunyan, H., Khachatrian, H., Kale, D. C., Ver Steeg, G., \u0026amp; Galstyan, A. (2019). Multitask learning and benchmarking with clinical time series data. Scientific data, 6(1), 1-18.\n[6] Johnson, A. E., Pollard, T. J., Shen, L., Li-Wei, H. L., Feng, M., Ghassemi, M., Moody, B., Szolovits, P., Celi, L. A., Mark, R. G. (2016). MIMIC-III, a freely accessible critical care database. Scientific data, 3(1), 1-9.\n[7] Komorowski, M., Celi, L.A., Badawi, O. et al. The Artificial Intelligence Clinician learns optimal treatment strategies for sepsis in intensive care. Nat Med 24, 1716‚Äì1720 (2018). https://doi.org/10.1038/s41591-018-0213-5\n[8] Wenke, S., \u0026amp; Fleming, J. (2019). Contextual Recurrent Neural Networks. arXiv preprint arXiv:1902.03455.\nSmirnova, E., \u0026amp; Vasile, F. (2017, August). Contextual sequence modeling for recommendation with recurrent neural networks. In Proceedings of the 2nd Workshop on Deep Learning for Recommender Systems (pp. 2-9).\n[9] Choi, E., Bahadori, M. T., Sun, J., Kulas, J., Schuetz, A., \u0026amp; Stewart, W. (2016). Retain: An interpretable predictive model for healthcare using reverse time attention mechanism. In Advances in Neural Information Processing Systems (pp. 3504-3512).\n[10] Yang, Q., Liu, Y., Chen, T., \u0026amp; Tong, Y. (2019). Federated machine learning: Concept and applications. ACM Transactions on Intelligent Systems and Technology (TIST), 10(2), 1-19.\n","date":1607558400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607558400,"objectID":"1c9cd2c04f13d9ceb0f779bed58be653","permalink":"https://abhinaukumar.github.io/post/eicu-tradeoffs/","publishdate":"2020-12-10T00:00:00Z","relpermalink":"/post/eicu-tradeoffs/","section":"post","summary":"Patients with prolonged stay often account for high resource consumption [1], and so, are a potential loss of revenue from a hospital management perspective. This is especially important in the case of Intensive Care Unit (ICU) and electronic ICU (eICU) patients due to their intensive need for care resources like nurses, drugs, surgeons, X-ray machines, etc.","tags":["Academic"],"title":"Analyzing Model Tradeoffs in Predicting Length of Stay (LOS) in eICU Patients","type":"post"},{"authors":["Abhinau Kumar"],"categories":["Reading"],"content":"In this post, we will introduce two broad categories of image/video quality models, and discuss two algorithms that fall under a third, hybrid, category. We will heavily borrow from concepts of Natural Scene Statics (NSS) and Information theory discussed in my earlier post on VIF.\nConsider the following scenario. You go on a hike and come back with a collection of high-quality pictures on your favourite DSLR camera. You now want to share these pictures, but the \u0026ldquo;raw\u0026rdquo; files on your camera are massive. So, you need to compress them, which leads to some data loss, changing the image ever so slightly. We want to measure how this \u0026ldquo;distortion\u0026rdquo; affects the quality of the image. We can ask this question in two ways.\nYou hold the original \u0026ldquo;pristine\u0026rdquo; image and this compressed image side by side and compare them. This way, you will be able to identify exactly where the image has been changed, and by how much. Or, as in our case, you could use an algorithm which takes both the pristine image, called the reference, and the compressed image, called the test image, and predicts how good/poor you perceive the compressed image to be. This is called a Full Reference (FR) quality metric because it compares the test image against a gold-standard reference image.\nAnother way one could ask this question is to only consider the compressed image. Instead of measuring the change due to compression, we only predict how good the final product looks. Because we do not compare the test image with any gold-standard, we call this a No Reference (NR) quality model. In the case of compression, this does not seem like a great idea because we are ignoring information about the gold standard even though we have access to it.\nHowever, NR models are very useful in other settings where a pristine image/video does not exist. The most important use of NR models is in estimating the quality of User Generated Content (UGC). While large studios create movies and advertisements with professional camera work and editing, a large volume of content on the internet is created by individuals, often with hand-held cameras. Such images/videos often suffer from degradations like noise, blur, low exposure. So, even though there is no \u0026ldquo;gold standard\u0026rdquo; to compare against, it is clear that such videos are of poor quality. NR models help us capture exactly that.\nNaturally, because have access to more information, FR models typically predict subjective quality better than NR models. But, we are not restricted to this all-or-nothing binary. For example, let us consider the case of transmitting a video over a lossy network. As the service provider, we would like to measure the quality of the video at the user\u0026rsquo;s end.\nOf course, we cannot use an FR model. If we could have a copy of the pristine video at the user\u0026rsquo;s end, we could declare victory and move on. On the other hand, no reference models do not perform very well. So, we find a middle ground. We can transmit some side information about the pristine video over the network and use this to calculate quality. Such models are called reduced-reference (RR) models and they occupy the region between FR and NR models.\nNow that we know what a RR algorithm should look like, let us discuss the first of two very similar algorithms - Spatio-Temporal Reduced Reference Entropic Differencing (ST-RRED) [1]. As in VIF [2], this model also uses the Gaussian Scale Mixture (GSM) to model the statistics of wavelet coefficients. However, this model deviates from VIF in two important ways. First, instead of modelling a distortion channel, frames from both the reference and the test videos are modelled as GSM random vectors (RVs). This is because modelling the distortion channel requires both reference and test data, which is only possible in an FR setting. Secondly, ST-RRED extends the statistical model to the temporal dimension by using GSM to also model the statistics of frame differences. This allows the model to determine both the spatial and temporal quality of videos.\nThe following few paragraphs contain some tedious notation but it will all be worth it in the end. We first consider corresponding frames from the reference and distorted videos and calculate their wavelet coefficients using the Steerable Pyramid [3] decomposition. Coefficients from each subband (corresponding to one scale and one orientation) are split into $M \\times M$ blocks. Let us denote the $m^{th}$ coefficient in the $k^{th}$ subband of frame $f$ in the reference and distorted videos by $C_{mkfr}$ and $C_{mkfd}$ respectively.\nBecause we model these coefficients as a GSM, we can express them as the product of a non-negative scalar random variable $S$ and a Gaussian RV $U$. That is, $$ C_{mkfr} = S_{mkfr} U_{mkfr}, \\ C_{mkfd} = S_{mkfd} U_{mkfd}$$\nSimilarly, to measure the temporal quality, we consider the wavelet coefficients of the differences between successive frames and model them also as GSM RVs. $$ D_{mkfr} = T_{mkfr} V_{mkfr}, \\ D_{mkfd} = T_{mkfd} V_{mkfd}$$\nThese are passed through an Additive White Gaussian Noise Channel (AWGN) which models neural noise in the Human Visual System (HVS) to get $$C_{mkfr}' = C_{mkfr} + W_{mkfr}, \\ C_{mkfd}' = C_{mkfd} + W_{mkfd}$$ $$D_{mkfr}' = D_{mkfr} + Z_{mkfr}, \\ D_{mkfd}' = D_{mkfd} + Z_{mkfd}$$\nwhere the noise RVs $W_{mkfr}$ and $W_{mkfd}$ are distributed as $\\mathcal{N}(0, \\sigma_w^2 I)$, and $Z_{mkfr}$ and $Z_{mkfd}$ are distributed as $\\mathcal{N}(0, \\sigma_z^2 I)$.\nAs in VIF, we estimate the covariance of the \u0026ldquo;underlying\u0026rdquo; Gaussian in each case, denoting them by $K_{U_{kfr}}$, $K_{V_{kfd}}$, $ K_{V_{kfr}}$, and the value of the scalar multipliers at each point (also called the variance field), denoting them by $s_{mkfr}$, $s_{mkfd}$, $t_{mkfr}$, and $t_{mkfd}$.\nUsing these quantities, we can compute the conditional differential entropy of the $d = M^2$ dimensional GSM vectors $C$ and $D$ given $s$ and $t$ respectively as $$ h_{C_{mkfr}} = h\\left(C_{mkfr} | S_{mkfr} = s_{mkfr}\\right) = \\frac{1}{2} \\log\\left[(2 \\pi e)^d \\left\\lvert s_{mkfr}^2 K_{U_{kfr}} + \\sigma_w^2I\\right\\rvert\\right]$$ $$ h_{D_{mkfr}} = h\\left(D_{mkfr} | T_{mkfr} = t_{mkfr}\\right) = \\frac{1}{2} \\log\\left[(2 \\pi e)^d \\left\\lvert t_{mkfr}^2 K_{V_{kfr}} + \\sigma_z^2I\\right\\rvert\\right]$$\nand similarly for coefficients from the test images. A weighting term is added to these entropy values, which assigns higher weights to regions having higher local variance. This is done because areas having higher spatial information (variance) are perceptually more important. The spatial and temporal scaling factors are computed from the local variance estimates as $$\\gamma_{mkfr} = \\log\\left(1 + s_{mkfr}^2\\right)$$ $$\\delta_{mkfr} = \\log\\left(1 + s_{mkfr}^2\\right)\\log\\left(1 + t_{mkfr}^2\\right)$$\nand similarly in the distorted coefficients. The Spatial RRED index for a video having $F$ frames, in subband $k$, which has $M_k$ samples is defined as the average difference between weighted entropies $$ SRRED_{k}^{M_k} = \\frac{1}{F M_k} \\sum_{f=1}^{F}\\sum_{m=1}^{M_k} \\lvert \\gamma_{mkfr} h_{C_{mkfr}} - \\gamma_{mkfd} h_{C_{mkfd}} \\rvert$$\nNote that to calculate this, all $M_k$ values of $\\gamma_{mkfr} h_{C_{mkfr}}$ must be available. This is effectively a FR model, because it has access to all the available information. However, we can reduce the number of scalars by summing up neighbouring values. As an extreme case, the SRRED index using just one scalar is obtained by summing up all the scalars in each subband, giving $$ SRRED_{k}^{1} = \\frac{1}{F M_k} \\sum_{f=1}^{F} \\left\\lvert \\sum_{m=1}^{M_k} \\gamma_{mkfr} h_{C_{mkfr}} - \\sum_{m=1}^{M_k} \\gamma_{mkfd} h_{C_{mkfd}} \\right\\rvert $$\nWe can choose to transmit any intermediate number of scalars, say $N$, by summing up $M_k/N$ neighbouring values\n$$ SRRED_{k}^{N} = \\frac{1}{F M_k} \\sum_{f=1}^{F} \\sum_{n=1}^{N} \\left\\lvert \\sum_{m=\\frac{(n-1)M_k}{N}}^{\\frac{nM_k}{N}} \\gamma_{mkfr} h_{C_{mkfr}} - \\sum_{m=\\frac{(n-1)M_k}{N}}^{\\frac{nM_k}{N}} \\gamma_{mkfd} h_{C_{mkfd}} \\right\\rvert $$\nIn exactly the same manner, we can define the temporal RRED index using $N$ scalars in the $k^{th}$ subband as $$ TRRED_{k}^{N} = \\frac{1}{F M_k} \\sum_{f=1}^{F} \\sum_{n=1}^{N} \\left\\lvert \\sum_{m=\\frac{(n-1)M_k}{N}}^{\\frac{nM_k}{N}} \\delta_{mkfr} h_{D_{mkfr}} - \\sum_{m=\\frac{(n-1)M_k}{N}}^{\\frac{nM_k}{N}} \\delta_{mkfd} h_{D_{mkfd}} \\right\\rvert $$\nFinally, the STRRED index is obtained as the product $$ STRRED_k = SRRED_k \\cdot TRRED_k$$\nSo, we have developed an information-theoretic model of subjective quality, where the amount of information available at the receiver\u0026rsquo;s end can be controlled by choosing the number of scalars to be transmitted. Accordingly, the performance of the quality model also varies. In their experiments, the authors showed that ST-RRED reported higher performance than Multi-Scale SSIM (MS-SSIM) and PSNR using just 0.1% of the number of scalars at the receiver.\nThe Spatial Efficient Entropic Differences (SpEED-QA) [4] model applies the same algorithm, but in the spatial domain. This is done by replacing the wavelet transform with a simple local mean subtraction. This significantly reduces the computational cost with a small decrease (sometimes increase!) in performance.\nSo, there we have it! By modelling reference and test images independently and comparing derived quantities, we can control the amount of information that is available at the receiver and leverage the resulting tradeoff.\nReferences [1] Rajiv Soundararajan and Alan C. Bovik. Video Quality Assessment by Reduced Reference Spatio-Temporal Entropic Differencing. IEEE Transactions on Circuits and Systems for Video Technology, 2013 Link\n[2] Hamid R. Sheikh and Alan C. Bovik. Image Information and Visual Quality. IEEE Transactions on Image Processing, 2006 Link\n[3] Eero P. Simoncelli and William T. Freeman. The Steerable Pyramid: A Flexible Architecture For Multi-Scale Derivative Computation. IEEE Conference on Image Processing, 1995 Link\n[4] Christos G. Bampis, Praful Gupta, Rajiv Soundararajan and Alan C. Bovik. SpEED-QA: Spatial Efficient Entropic Differencing for Image and Video Quality. IEEE Signal Processing Letters, 2017 Link\n","date":1597536000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597536000,"objectID":"6c79425cd1cd31bdcb25f235f5586972","permalink":"https://abhinaukumar.github.io/post/reduced-reference/","publishdate":"2020-08-16T00:00:00Z","relpermalink":"/post/reduced-reference/","section":"post","summary":"In this post, we will introduce two broad categories of image/video quality models, and discuss two algorithms that fall under a third, hybrid, category. We will heavily borrow from concepts of Natural Scene Statics (NSS) and Information theory discussed in my earlier post on VIF.","tags":["Academic"],"title":"Information Theoretic Reduced Reference Quality Models","type":"post"},{"authors":["Abhinau Kumar"],"categories":["Reading"],"content":"This article is a review of existing texture characterization, identification and segmentation methods. These methods typically involve statistical methods, i.e. inferring from histograms or co-occurrence matrices, and/or signal processing methods, either simple filtering and energy-based methods, or as a method of pre-processing before applying statistical analysis.\nBut, what is texture? Simply, \u0026lsquo;\u0026lsquo;texture\u0026rsquo;\u0026rsquo; describes the local arrangement of pixels/intensity values. For a classic example, consider two $8\\times 8$ squares, one painted half white and half black, while the other has a checkerboard pattern. While they have the same mean luminance (brightness), they vary in the arrangement in the actual pixel (intensity) values. That is, they vary in \u0026lsquo;\u0026lsquo;texture\u0026rsquo;\u0026rsquo;.\nTexture can also be thought of as local \u0026lsquo;\u0026lsquo;complexity\u0026rsquo;\u0026rsquo;. The first square in our example had two plain regions, so it was \u0026lsquo;\u0026lsquo;simple\u0026rsquo;\u0026rsquo;, while the checkerboard pattern is more \u0026lsquo;\u0026lsquo;complex\u0026rsquo;\u0026rsquo;. Natural examples of texture include grass, fabric patterns, ripples, falling confetti, etc. Note that we have not rigorously defined the term yet. That is because texture is not a precisely defined concept, merely a notion. Even so, describing local arrangements can be very useful. Texture analysis has found great use in medical image processing, document processing and remote sensing. An example closer to my work would be that \u0026lsquo;\u0026lsquo;simple\u0026rsquo;\u0026rsquo; or \u0026lsquo;\u0026lsquo;plain\u0026rsquo;\u0026rsquo; regions can be compressed easily, while complex regions may demand higher bandwidth.\nWhat type of questions can we ask about texture? The simplest question is to identify it. Given a set of texture classes, can we identify a given texture as being one of these? This is called texture classification and typically involves statistical methods. Another task is texture segmentation, where we wish to segment (split) an image into regions having different textures from each other. Think of the Windows XP wallpaper, but without clouds and less tidy grass. Such a method would split the image into the sky and the grass because they have different textures.\nWe will now look at features that we can use to describe textures. Currently, I\u0026rsquo;m not interested in specific algorithms. My goal is to find ways to \u0026lsquo;\u0026lsquo;describe\u0026rsquo;\u0026rsquo; texture. This will also not be an exhaustive review of all texture features. After all, texture is not the focus of my work. Learning this is just the means to an end, so I will only go so far as I need to.\nMost of my reading has used this presentation [1] and this review article [2] as jumping points to other sources wherever necessary.\nFirst-Order Statistical Features Statistics that only depend on individual pixel values are called first-order statistics. The local range and variance are simple first-order statistical features to describe textures. Plain regions have a smaller range (max - min) while textured regions have larger ranges because of the greater diversity in intensity values. A similar argument can be made about local variances because of which we expect textured regions to have higher variance than plain regions.\nGray Level Co-occurence Matrix (GLCM) The Gray Level Co-occurence Matric (GLCM) is arguably the most common method of describing textures. The GLCM records second-order statistics, because it depends on pairs of pixels, storing information about the relative positions of pixels having similar intensity values. Given an offset $\\delta = (\\delta x, \\delta y)$ , the GLCM is a $256 \\times 256$ matrix counting the co-occurence of gray levels at an offset $ \\delta $ . That is, we construct a matrix whose entries are $$ G_{\\delta}[i,j] = \\sum_x \\sum_y \\mathbb{1}(G[x,y] = i) \\mathbb{1}(G[x+\\delta y,y+\\delta y] = j) $$\nThe entries of this matrix are then normalized by the sum of all entries, giving us a normalized GLCM, say $ P_\\delta $, which is a valid probability mass function. While the GLCM itself is not used to, say, compare textures, we derive numerical features from these which are used to describe texture. Some examples of such features are\n Maximum : $ \\max P_\\delta[i,j] $ , i.e., the most likely pair of intensities. Order $ k $ difference moment : $ E[(i-j)^k] $ , or its inverse $ E[1/(i-j)^k] $ . A special case of this is the contrast, which is the 2nd difference moment, i.e., $ E[(i-j)^2] $ . Homogeneity: $ E\\left[\\frac{1}{(1 + \\lvert i-j\\rvert)}\\right] $ . A homogeneous image will have non-zero entries close to the principal diagonal, i.e $ i \\approx j $ , while a heterogeneous image will have a more even spread since many pairs occur. Entropy: $ E[-\\log P_\\delta[i,j]] $ , which is a measure of the \u0026lsquo;\u0026lsquo;spread\u0026rsquo;\u0026rsquo; or the amount of information in the distribution. Correlation: $ \\frac{E[ij] - \\mu_i\\mu_j}{\\sigma_i \\sigma_j} $ which is high when pixels have a linear dependence.  Haralick [3] defined 14 texture features based on the GLCN. In a similar vein, the Gray Level Difference statistics (GLDS) are derived from a vector of 256 values, which count the number of times each difference $ \\lvert i-j\\rvert $ occurs between pairs of intensity values separated by a distance $ \\delta $.\nA main drawback of the GLCM and the GLDS is finding a good choice of $ \\delta $. In the current deep learning/gradient-based optimization era, I would add that the non-differentiability of the counting process is an added drawback.\nAutocorrelation Function The autocorrelation function (ACF) is a powerful signal processing method to extract repeating patterns. Given an image $ I(x,y) $, the ACF is defined as\n$$ \\rho(u,v) = \\frac{\\sum_x \\sum_y I(x,y) I(x+u, y+v)}{\\sum_x\\sum_y I^2(x,y)} $$\nThe auto-correlation function is a function of the \u0026lsquo;\u0026lsquo;offset\u0026rsquo;\u0026rsquo; between pairs of pixels. Given an offset, we multiply corresponding intensity values and consider the normalized sum. Why is this relevant? When the offset is close to the \u0026lsquo;\u0026lsquo;true\u0026rsquo;\u0026rsquo; offset between similar texture elements, the value of the ACF is close to 1, which is its highest value.\nWhy is this the case? The short technical answer is \u0026lsquo;\u0026lsquo;Cauchy-Schwarz Inequality\u0026rsquo;\u0026rsquo;. More simply, we know that textures involve patterns which repeat at some intervals (although not exactly). So, it makes sense that we would like to \u0026lsquo;\u0026lsquo;test\u0026rsquo;\u0026rsquo; various offsets. When we choose the correct offset, the repeating intensities \u0026lsquo;\u0026lsquo;line up\u0026rsquo;\u0026rsquo;, so they get squared in the summation, leading to an ACF value close to 1. When we choose an \u0026lsquo;\u0026lsquo;incorrect\u0026rsquo;\u0026rsquo; offset, the repeating values do not overlap, leading to lower values. The figure below shows a visual argument for why \u0026lsquo;\u0026lsquo;lining up\u0026rsquo;\u0026rsquo; is a good thing.\nAt this point, much like Dumbledore to Harry on the Astronomy Tower, I must ask for your trust in believing that squaring values when lining up is the \u0026lsquo;\u0026lsquo;best\u0026rsquo;\u0026rsquo; you can do (that is, lining up maximizes the ACF). Thankfully, you don\u0026rsquo;t need to wait for the death of a morally grey character to find out why this is actually the case. There are elegant proofs of the Cauchy Schwarz inequality that I would encourage you to find online. Several proofs have been reviewed in [4].\nMoving on, the ACF falls off slowly when the texture is coarse, because it takes a large shift to fall out of, or out of phase with, the texture. On the other hand, fine textures cause a sharp drop in the ACF.\nSignal Processing Methods We begin with the simple observation that the coarseness of texture in a region is related to the density of edges in that region. Fine textures have a higher edge density compared to coarse textures. To extract these features we can use edge operators like the Laplacian operator.\nAnother set of filters is used to calculate the $ (p+q) $ th moment of an image region $ \\mathcal{R} $ $$ m_{p,q}(x,y) = \\sum_{(u,v) \\in \\mathcal{R}} u^p v^q I(u,v) $$\nChoosing the region $ \\mathcal{R} $ to be a rectangular region, we can implement this as a linear filter having the appropriate weights.\nPerceptually motivated methods use filters that better represent the preattentive perception in the Human Visual System (HVS). Gabor filters, which are complex exponentials having a Gaussian envelope, are a good model of simple cells in the primary visual cortex. Because these filters are both frequency and orientation-selective, they are used to conduct a multi-scale multi-orientation analysis of images.\nTo derive texture features, an image to passed through a Gabor filter bank to obtain subbands $ r_i(x,y) $ . These responses are passed through a sigmoid non-linearity $ \\sigma $ (tanh function) and used to obtain texture features \\begin{equation} f_i(x,y) = \\sum_{(u,v) \\in \\mathcal{R}} |\\sigma(r_i(u,v))| \\end{equation}\nLaws [5] proposed a computationally efficient method to compute texture energies using spatially separable filters. The method uses a set of Texture Energy Metric (TEM) vectors. The outer product of each pair of vectors results in a filter. The five types of vectors (corresponding to different textures) are level, edge, spot, wave and ripple. These TEM filters are used to filter images and compute the local texture energy, which is simply the sum of the magnitudes in a local region.\nReferences [1] Micheal A. Wirth. Texture Analysis Link\n[2] Mihran Tuceryan and Anil K. Jain. Texture Analysis. Handbook of Pattern Recognition and Computer Vision Link\n[3] R. M. Haralick and K. Shanmugam and I. Dinstein. Textural Features for Image Classification. IEEE Transactions on Systems, Man, and Cybernetics, 1973 Link\n[4] Hui-Hua Wu and Shanhe Wu. Various proofs of the Cauchy-Schwarz inequality. Link\n[5] Kenneth I. Laws. Rapid Texture Identification. Optics \u0026amp; Photonics, 1980 Link\n","date":1596844800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597536000,"objectID":"893690f27588ddd9b9666915ec03cadd","permalink":"https://abhinaukumar.github.io/post/texture-feature/","publishdate":"2020-08-08T00:00:00Z","relpermalink":"/post/texture-feature/","section":"post","summary":"This article is a review of existing texture characterization, identification and segmentation methods. These methods typically involve statistical methods, i.e. inferring from histograms or co-occurrence matrices, and/or signal processing methods, either simple filtering and energy-based methods, or as a method of pre-processing before applying statistical analysis.","tags":["Academic"],"title":"Texture Features","type":"post"},{"authors":["Abhinau Kumar V","Shashank Gupta","Sumohana S. Channappayya"],"categories":[],"content":"","date":1585544400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585544400,"objectID":"b98f73384c3e445b52015001b3564639","permalink":"https://abhinaukumar.github.io/publication/fourier-ptychography/","publishdate":"2020-08-09T16:43:32-05:00","relpermalink":"/publication/fourier-ptychography/","section":"publication","summary":"Fourier Ptychography (FP) is a computational imaging technique which artificially increases the effective numerical aperture of an imaging system. In FP, the object is imaged using an array of Light Emitting Diodes (LEDs), each from a different illumination angle. A high resolution image is synthesized from this low resolution stack, typically using iterative phase retrieval algorithms. However, such algorithms are time consuming and fail when the overlap between the spectra of images is low, leading to high data requirements. At the crux of FP lies a phase retrieval problem. In this paper, we propose a Deep Learning (DL) algorithm to perform this synthesis under low spectral overlap between samples, and show a significant improvement in phase reconstruction over existing DL algorithms.","tags":[],"title":"Perceptually Driven Conditional GAN for Fourier Ptychography","type":"publication"},{"authors":[],"categories":["Reading"],"content":"This article is a review of a popular Image Quality Model - Visual Information Fidelity (VIF) [1] which is all based on a statistical model of natural scenes. To do this, we will first review a powerful natural scene statistics (NSS) model, the Gaussian Scale Mixture. Then, we will review some basic information theory and use these tools to derive an image quality model.\nLet us begin at the basics. A Random Variable (very informally, a variable that takes a random value) $ X $ is said to be Gaussian or Normal and is denoted by $ X \\sim \\mathcal{N}(\\mu, \\sigma^2 )$ if its associated probability distribution function (pdf) is given by $$ f_N(x, \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right).$$\nIf that does not mean a lot to you, it\u0026rsquo;s fine. One does not need to know the expression of a hyperbolic paraboloid to enjoy chips out of a Pringles can. The simplest way to think about it is that it a distribution which is centred around a mean value $\\mu$ and whose spread (variance) is controlled by $\\sigma$. It looks like so.\nThis definition can be extended naturally to random vectors (RVs), whose distribution is parameterized by a vector $\\mu$ and covariance matrix $\\Sigma$. This distribution is called a multivariate (because many variables) Gaussian and its pdf in $d$ dimensions is given by $$ f_N(x; \\mu, \\Sigma, d) = \\frac{1}{\\sqrt{(2\\pi)^d|\\Sigma|}} \\exp\\left(-\\frac{1}{2}(x - \\mu)^T \\Sigma^{-1} (x - \\mu))\\right) $$\nNow that we know what a Gaussian is, we can \u0026lsquo;\u0026lsquo;mix\u0026rsquo;\u0026rsquo; Gaussians together. The way we mix Gaussians is by having a set of Gaussian RVs, picking one of them at random and using that RV to generate a sample. In a way, we add a layer of randomness on top of the existing randomness.\nLet us call the random variable which tells us which Gaussian RV to pick the mixing variable $ S $. A compact way of expressing the explanation above is to say that given $ S = s $, RV $X$ has the conditional distribution $X|S = s \\sim \\mathcal{N}(\\mu(s), \\Sigma(s))$. Then, with a bit of probability magic, we can show that the pdf of the Gaussian Mixture is given by\n$$ f(x) = E_S[f_N(x; \\mu(s), \\Sigma(s), d)] $$\nOnce again, the actual math is not critical. The intuition here is that we picked Gaussian RVs at random, so the overall distribution is an average of their individual distributions.\nThe Gaussian Scale Mixture (GSM) is a simplified version of this distribution, where we assume all the Gaussians are centred at the origin, i.e., $ \\mu(s) = 0$ and that all covariance matrices are scaled versions of each other, i.e., $\\Sigma(s) = s^2 \\Sigma$ for some covariance matrix $\\Sigma$. That is to say, the components of a GSM only differ by a scaling factor. Interestingly, we can \u0026lsquo;\u0026lsquo;decompose\u0026rsquo;\u0026rsquo; the GSM distributed random variable using a positive random variable $S$ and a Gaussian RV $U \\sim \\mathcal{N}(0, \\Sigma)$ as $$ X = S \\cdot U $$\nWhile this model is a \u0026lsquo;\u0026lsquo;simplification\u0026rsquo;\u0026rsquo; of the Gaussian Mixture Model (GMM), it is powerful a model for natural scenes. This is mainly because unlike a Gaussian, a GSM can represent heavy-tailed distributions, i.e, distributions which decay slowly. In their seminal work [3] showed that wavelet coefficients GSM can be used to model wavelet coefficients of natural images.\nAt this point, we should clarify and introduce a few notions. First, what are natural images? They are not images of nature. A natural image is an image which has not undergone any distortions, like blur, compression. noise, etc. So, a natural image is one which looks natural.\nSecond, what are wavelets? Informally, a wavelet is a \u0026lsquo;\u0026lsquo;localized\u0026rsquo;\u0026rsquo; wave. That is, while waves (think sinusoids) are periodic and infinite, wavelets are more localized in space (or time). Much like the Heisenberg Uncertainty principle, signals also obey an uncertainty principle. Simply put, a signal cannot have an arbitrary small spread in both space and frequency. So, because a wave has zero spread in frequency (a sinusoid has only one frequency), it has an infinite spread in space. In other words, we say that a sinusoid offers perfect frequency resolution but no spatial resolution.\nA wavelet, by being localized in space, allows us to trade off spread in space for spread in frequency, giving us both (limited) spatial and frequency resolution. A wider wavelet has poorer spatial resolution, but better frequency resolution. Because images are two dimensional, there is also a notion of orientation resolution. Isotropic functions (functions which are identical in all directions) offer no orientation selectivity in space while very narrow functions offer high orientation selectivity. The steerable pyramid [3], used in VIF and STRRED [2], provides a set of wavelets which allows for an (overcomplete) multi-scale multi-orientation decomposition of images.\nFinally, let us talk about information theory. In my opinion, information theory is the most beautiful offshoot of probability theory. The goal of information theory, as one would guess, is to characterize the amount of information stored in \u0026lsquo;\u0026lsquo;sources\u0026rsquo;\u0026rsquo;, which are random variables.\nThe amount of information in, or randomness of, a random variable $ X $ is called its entropy. Mathematically, the (Shannon) entropy of a random variable having a probability mass function $f(x)$ is given by $$ H(X) = -E[\\log f(X)] $$\nThis function satisfies properties that we would expect a randomness measure to satisfy. First, we would like the amount of randomness to always be non-negative, which is true of Shannon Entropy (See Jensen\u0026rsquo;s Inequality). Second, if a random variable is constant, i.e. $Pr[X = x] = 1$ for some $x$, then it has zero randomness and its entropy is 0. Finally, the uniform distribution has the highest entropy (among all distributions having the same size of support).\nLet us now bring in a friend. Let $Y$ be another random variable, which is not independent of $X$ (I said \u0026lsquo;\u0026lsquo;friend\u0026rsquo;'). We can ask the question, \u0026ldquo;how much information is in $X$ if I already know $Y$\u0026rdquo;? To answer this quantitatively, we compute the conditional entropy. To calculate this, we first consider the entropy of the conditional distribution of $X$ when we are given each possible of $Y$, i.e., $$H(X | Y = y) = E_{X|Y = y}\\left[-\\log f(x | Y = y)\\right] $$\nBut we only assumed that we knew $Y$, not that $Y$ took any particular value. So, we average this over all possible values of $Y$ to get the conditional entropy $$ H(X|Y) = E[H(X|Y = y)] $$\nBased on entropy and conditional entropy, we define the mutual information (MI) of two random variables $X$ and $Y$ which, as the name suggests, characterizes the amount of information each variable has about the other. The intuition for the mathematical expression for MI is as follows. Let us say that $X$ has information (entropy) $H(X)$. But, if we are given the random variable $Y$, its information content decreases (it will not increase) to $H(X|Y)$. Then, the difference is the amount of information about $X$ that we have obtained from $Y$. So, the MI between $X$ and $Y$ is defined as $$ I(X;Y) = H(X) - H(Y|X) = H(Y) - H(X|Y) $$\nNow let us briefly look at its properties. As suggested by the expression, the MI is symmetric, i.e. the MS between $X$ and $Y$ is equal to the MI between $Y$ and $X$. After all, we called it mutual information. Secondly, the MI between two random variables is non-negative. If knowing $Y$ means we know $X$ exactly, $H(X|Y)=0$ (because $X$ is known deterministically, there is no randomness) and the MI is just the information in $X$, i.e. $H(X)$. If $X$ and $Y$ are independent random variables, $H(X|Y) = H(X)$ and the MI is zero, which is what we would expect.\nI must confess, I played a little bait and switch routine over the last few paragraphs. I allured you with the promise of continuous random variables (random vectors, even!), but defined these information-theoretic quantities only for discrete distributions. For continuous random variables, we define these quantities in analogous ways, using the pdf instead of the probability mass functions, although some pleasant properties are lost.\nWe are finally ready to discuss the three information-theoretic quality models in question. Let us begin with VIF. VIF assumes a statistical model of the natural source (the GSM model discussed above), a distortion channel which distorts this \u0026lsquo;\u0026lsquo;pristine\u0026rsquo;\u0026rsquo; image, and an additive noise model of the human visual system (HVS). We can illustrate the VIF model as below.\ngraph LR; A[Source] -- B[HVS] B -- C[Receiver] A -- D[Distortion Channel] D -- E[HVS] E -- F[Receiver]  So, in line with our source model, let us define the source random variable as a GSM distributed random vector\n$$ C = S \\cdot U $$\nThe distortion channel is described as having a deterministic scalar gain $g$ and an additive White Gaussian Noise (AWGN) $V \\sim \\mathcal{N}(0, \\sigma_v^2 I)$. So, the distorted random variable is given by\n$$ D = g \\cdot C + V $$\nFinally, we model the neural noise of the HVS as an AWGN channel having $N, N\u0026rsquo; \\sim \\mathcal{N}(0, \\sigma_n^2)$. The final received pristine and distorted images are given by\n$$ E = C + N = S \\cdot U + N$$ $$ F = D + N' = g \\cdot S \\cdot U + V + N' $$\nWe condition all quantities on knowing $S$ because knowing $S$ allows us to predict VIF for our particular pair of reference and test images, instead of a general average case. The intuition here is that upon distortion, lesser information about the source is retained in the distorted image. So, the VIF index is defined as $$ VIF = \\frac{I(C; E | S = s)}{I(C; F | S = s)} $$\nConveniently, conditioning on $S$ results in all RVs becoming Gaussians, whose entropies are easy to compute. For an image having $N$ samples of dimension $d$, if the covariance matrix has eigenvalues $ \\lambda_j$, the mutual informations can be calculated easily as $$ I(C; E | S = s) = \\sum\\limits_{i=1}^{N}\\sum_{j=1}^{d} \\log\\left(1 + \\frac{s_i^2\\lambda_j}{\\sigma_n^2}\\right) $$ $$ I(C; F | S = s) = \\sum\\limits_{i=1}^{N}\\sum_{j=1}^{d} \\log\\left(1 + \\frac{g_i^2 s_i^2 \\lambda_j}{\\sigma_v^2 + \\sigma_n^2}\\right) $$\nIf you have made it this far, congratulations! You now know what natural scene statistics and information-theoretic quality models look like. We will end by briefly commenting on the implementation details. In practice, both the reference and test images are first transformed into the Wavelet domain using the Steerable Pyramid. The coefficients (which look like filtered images) are collected in 3x3 blocks and modelled as 9-dim vectors $C_i$ and $D_i$ respectively.\nWhat remains is to find the parameters of the GSM distribution and the distortion channel from these wavelet coefficients. The HVS is assumed to have a known channel noise $ \\sigma_n^2 = 0.1 $. Because these parameter estimation details are not a part of the core idea behind VIF, I will just state them without proof.\n$$ \\Sigma = \\frac{1}{N} \\sum C_i C_i^T $$ $$ s_i = \\frac{C_i^T \\Sigma^{-1} C_i}{9} $$ $$ g_i = \\frac{Cov(C, D)}{Cov(D, D)} $$ $$ \\sigma_{v,i}^2 = Cov(D, D) - g_i Cov(C, D)$$\nNote that the covariances in the last two equations are scalar covariances calculated in corresponding local neighbourhoods of wavelet coefficients. And there we have it - VIF! Admittedly, this post turned out to be much longer than I had anticipated. But, it is a good thing because I will write about STRRED and SpEED-QA [5] next, which use these same concepts, but differently. So now, even though you may not realize it, you know enough to understand two more information-theoretic quality models.\nReferences [1] Hamid R. Sheikh and Alan C. Bovik. Image Information and Visual Quality. IEEE Transactions on Image Processing, 2006 Link\n[2] Rajiv Soundararajan and Alan C. Bovik, Video quality assessment by reduced reference spatio-temporal entropic differencing. IEEE Transactions on Circuits and Systems for Video Technology, 2013 Link\n[3] Martin J. Wainwright and Eero P. Simoncelli. Scale Mixtures of Gaussians and the Statistics of Natural Images. Proceedings of the 12th International Conference on Neural Information Processing Systems. 1999 Link\n[4] Eero P. Simoncelli and William T. Freeman. The Steerable Pyramid: A Flexible Architecture For Multi-Scale Derivative Computation. IEEE Conference on Image Processing, 1995 Link\n[5] Christos G. Bampis and Praful Gupta and Rajiv Soundararajan and Alan C. Bovik. SpEED-QA: Spatial Efficient Entropic Differencing for Image and Video Quality. IEEE Signal Processing Letters, 2017 Link\n","date":1565136000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565136000,"objectID":"9e861cf00dbc43ad2dca342613686dde","permalink":"https://abhinaukumar.github.io/post/nss-and-vif/","publishdate":"2019-08-07T00:00:00Z","relpermalink":"/post/nss-and-vif/","section":"post","summary":"This article is a review of a popular Image Quality Model - Visual Information Fidelity (VIF) [1] which is all based on a statistical model of natural scenes. To do this, we will first review a powerful natural scene statistics (NSS) model, the Gaussian Scale Mixture.","tags":["Academic"],"title":"Natural Scene Statistics and Visual Information Fidelity","type":"post"},{"authors":["Akshat Agarwal","Abhinau Kumar V","Kyle Dunovan","Erik Peterson","Timothy Verstynen","Katia Sycara"],"categories":[],"content":"","date":1537830781,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537830781,"objectID":"1ee9577e67e1e538e5a0236df41a758f","permalink":"https://abhinaukumar.github.io/publication/safe-reinforcement-learning/","publishdate":"2020-08-09T16:38:12-05:00","relpermalink":"/publication/safe-reinforcement-learning/","section":"publication","summary":"In the real world, agents often have to operate in situations with incomplete information, limited sensing capabilities, and inherently stochastic environments, making individual observations incomplete and unreliable. Moreover, in many situations it is preferable to delay a decision rather than run the risk of making a bad decision. In such situations it is necessary to aggregate information before taking an action; however, most state of the art reinforcement learning (RL) algorithms are biased towards taking actions \textit{at every time step}, even if the agent is not particularly confident in its chosen action. This lack of caution can lead the agent to make critical mistakes, regardless of prior experience and acclimation to the environment. Motivated by theories of dynamic resolution of uncertainty during decision making in biological brains, we propose a simple accumulator module which accumulates evidence in favor of each possible decision, encodes uncertainty as a dynamic competition between actions, and acts on the environment only when it is sufficiently confident in the chosen action. The agent makes no decision by default, and the burden of proof to make a decision falls on the policy to accrue evidence strongly in favor of a single decision. Our results show that this accumulator module achieves near-optimal performance on a simple guessing game, far outperforming deep recurrent networks using traditional, forced action selection policies.","tags":[],"title":"Better Safe than Sorry: Evidence Accumulation Allows for Safe Reinforcement Learning","type":"publication"},{"authors":["Abhinau Kumar"],"categories":["Poetry"],"content":"Through a small opening in a coat\nGlinted a shard of iridescent blue.\nThe Tourist pulled it closer, clearing her throat,\nTo shield from the cold wind that blew\nSounding a low, ominous whistle\nAs if in taunt or admiration.\nThe sea, from behind, offered a drizzle\nUrging her to flaunt His generous creation.\nThe Tourist moved ahead, steadfast\nReading from a finger lined with the past.\nShe walked through the crowded thoroughfare\nEach standing still with muffled screams\nAlbeit with pressed shirts and slicked back hair\nLike a requiem for their forsaken dreams.\nShe reached into her coat and chipped off from an edge;\nAnd offered it to each stationed sentry\nWho stood there as if bound by a pledge,\nAs a payment for her entry.\nThe tourist entered the city at last\nHaving added to the finger lined with the past.\nTo her, all these cities looked the same;\nTwo stone walls rising on either side\nAnd yet, deserving of all their fame\nBut none offering a place to hide.\nAs often before, she was found by a man\nOffering to make her feel whole.\nShe placed a fading blue chip on his hand\nPaying her debt with a piece of her soul.\nShe walked away, hoping to be alone at last\nAdding again to the finger lined with her past.\nOne might wonder, ‚ÄúWhat use is it?‚Äù\nIs the soul too much for the feet to carry?\nWhy lose yourself to a cause unfit?‚Äù\nTo you, I say, it is the contrary.\nLike others, she was born set into motion.\nSo while the wind howled through each rift\nShe rolled ahead like waves of the ocean\nWhile denying herself her own weight to lift.\nAnd with every loss she did outlast\nShe added to the finger lined with her past.\n","date":1519862400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519862400,"objectID":"a55212ca1ac6ddbc4902cea62477bc23","permalink":"https://abhinaukumar.github.io/post/the-tourist/","publishdate":"2018-03-01T00:00:00Z","relpermalink":"/post/the-tourist/","section":"post","summary":"Through a small opening in a coat\nGlinted a shard of iridescent blue.\nThe Tourist pulled it closer, clearing her throat,\nTo shield from the cold wind that blew\nSounding a low, ominous whistle","tags":["Casual"],"title":"The Tourist","type":"post"},{"authors":["V. Abhinau Kumar","Shashank Gupta","Sai Sheetal Chandra","Shanmuganathan Raman","Sumohana S. Channappayya"],"categories":[],"content":"","date":1499058000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1499058000,"objectID":"5bf11853caa47b0ffb14b55f2493460a","permalink":"https://abhinaukumar.github.io/publication/hdr-quality-assessment/","publishdate":"2020-08-09T16:34:44-05:00","relpermalink":"/publication/hdr-quality-assessment/","section":"publication","summary":"We present a transfer learning framework for no-reference image quality assessment (NRIQA) of tonemapped High Dynamic Range (HDR) images. This work is motivated by the observation that quality assessment databases in general, and HDR image databases in particular are ‚Äúsmall‚Äù relative to the typical requirements for training deep neural networks. Transfer learning based approaches have been successful in such scenarios where learning from a related but larger database is transferred to the smaller database. Specifically, we propose a framework where the successful AlexNet is used to extract image features. This is followed by the application of Principal Component Analysis (PCA) to reduce the dimensionality of the feature vector (from 4096 to 400), given the small database size. A linear regression model is then fit to Mean Opinion Scores (MOS) using L2 regularization to prevent overfitting. We demonstrate state-of-the-art performance of the proposed approach on the ESPL-LIVE database.","tags":[],"title":"No-reference quality assessment of tone mapped High Dynamic Range (HDR) images using transfer learning","type":"publication"},{"authors":["Abhinau Kumar"],"categories":["Poetry"],"content":"The old gentleman lifted the glass of champagne in his hand Carrying with it the honour of being the best man, ‚ÄúTo the lovely couple, I would like to raise a toast And tell you a story that I find most Entertaining, but it has nothing to do With our beloved bride or groom.\nOnce upon a time, as many times before, I walked out into a forest that began at my door.\nFor a troubled mind, it was a beautiful sight,\nAnd my aching feet carried me into the white.\nThe forest would wrap me up in her trees, Providing a virgin land of undisturbed peace. But when the clear day became a smoky screen, I missed, for once, the colours of red and green.\nOf all the things I could have done,\nI abandoned classical notions of ‚Äúfun‚Äù\nAnd chose the path that many dread; The path that only the vagabonds tread.\nThe trail was dark but uncannily pleasant; Calming, yet chaotic like a raging adolescent. The moon offered only attention, not love; She was a lone friend loaned from above.\nI admitted I was lost, as my watch struck eleven.\nIt was too cold for hell, but it sure wasn\u0026rsquo;t heaven.\nThe wind carried the call of an unknown beast, Waiting in the bushes for his Christmas feast. Finding myself on the ground, I tried to remember why And before I could look up, something dropped from the sky. Shivering hard, I closed my eyes shut.\nI had come for some peace; this was anything but.\nEven so, I could tell it swung around Waiting, as a forgotten victim, to be found.\n\u0026lsquo;That was a man‚Äô, my brain tried to reaffirm.\n‚ÄòOf course, it was‚Äô. But my eyes wouldn\u0026rsquo;t confirm.\nIn spite of the crippling fear and fatigue, The man\u0026rsquo;s identity was a cause of intrigue.\nMaybe he was someone I had met. Maybe he just couldn‚Äôt outrun his debt. Or a man of great wisdom and might\nBrought to the ground on his greatest flight\nUnable to share life\u0026rsquo;s sense of humour\nAnd gone at the dawn of the vilest rumour. Or maybe just a man tired of his woeful existence\nWho declined to offer any further resistance.\nOr maybe he just took a fateful drink,\nLost his sacred ability to think,\nThought a rope around his neck suited him best\nAnd allowed gravity to do the rest.\nYou‚Äôd expect me to say I had learnt something by now\nAbout life, its deep meaning and how\nIt was stupid of me to have stayed on the ground.\nBut no, I waited till the sun came around.\nI opened my eyes as if to look up was a sin\nAnd there he hung, the wax mannequin.‚Äù\n","date":1481846400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1481846400,"objectID":"be14c8ae6c76cb0fb850cb7e3a0fe509","permalink":"https://abhinaukumar.github.io/post/the-old-mans-tale/","publishdate":"2016-12-16T00:00:00Z","relpermalink":"/post/the-old-mans-tale/","section":"post","summary":"The old gentleman lifted the glass of champagne in his hand Carrying with it the honour of being the best man, ‚ÄúTo the lovely couple, I would like to raise a toast And tell you a story that I find most Entertaining, but it has nothing to do With our beloved bride or groom.","tags":["Casual"],"title":"The Old Man's Tale","type":"post"},{"authors":["Abhinau Kumar"],"categories":["Article"],"content":"I‚Äôve lived a vast majority of my life in a dominion where a minority (one-fourth) of the population wielded a disproportionate amount of power over the numerical majority. Much like the British did in colonial India. I speak, of course, of the great power bestowed upon my mother, by the unmentioned powers of the universe. Anyone even mildly familiar with the display of said power would know that the element of surprise is the (metaphorical) nutrient-rich petri-dish upon which the effect of said display is cultured. And there is nothing more effortlessly middle class, yet vastly effective as the sudden issue of a diktat to get a haircut.\nBefore you judge, I am aware of those kids in Africa everyone talks about. I know it is, classically, a more socially relevant issue to write about. But, I haven‚Äôt seen any of them being threatened with surprise haircuts, so I reject the comparison. (That was a joke. Please don‚Äôt threaten to [or actually] post a status update or start a hashtag. I sympathise with them on a daily basis. Well, not daily, but you get the idea.) Anyway, let\u0026rsquo;s get back on topic.\nThe first plan of attack is, of course, retaliation. So, refuse. And then watch them refuse to accept your refusal. And then realise that your plan never factored in history. Never has a puny child‚Äôs ‚Äúno‚Äù overpowered that of that singularity of all power (read mom). (‚ÄúAnd it will stay that way‚Äù, she says.) Back home, school on weekdays meant that Sunday was the day allotted for this last legal form of systematic, and forced, acquisition of personal property (read hair). Invariably, the day of issue would give my brother and me a couple of days to figure out who would go that Sunday and who would go the next. (Side note: All stalemates were dealt with by the age-old method of conflict resolution called ‚ÄúLet‚Äôs ask mom‚Äù. Side-side note: Who thought that was a good idea?) Being the one with the longer hair never worked in my favour, but I had to hold on to that as it was the last domain that I had any control over. But more on that later.\nSo, on average, one gets three days to mentally prepare for the sacrifice. This period is usually characterized by increased mirror usage, new-found fondness for hair products and hallucinations of sounds made by electric razors (Not really, so if you could relate to that last one, you need to go see a doctor. Stat.) This is also the period of bargaining. With my mum about how short I had to get my hair cut. I‚Äôm not aware of the professionalism among barbers in other places in India, but where I come from, we use a time-tested scientifically-designed scale of ‚Äúshort or medium‚Äù. ‚ÄúMedium‚Äù, stands for ‚Äúshorter than you think it will be‚Äù, and ‚Äúshort‚Äù stands for ‚ÄúStop crying, you‚Äôre not bald yet‚Äù. To put it into perspective with the bargain, the difference between the length of your hair in ‚Äúmedium‚Äù and ‚Äúshort‚Äù is that little personal space you let your parents invade. This is the one part of the whole ordeal I win at. Although it takes a great deal of effort and having to summon all of my skills of parental persuasion, I manage to get away with ‚Äúmedium‚Äù, which is the lesser of two evils.\nOn that Saturday night, one learns the most important lesson anyone can hope to learn; that life has a twisted sense of humour. By then, I‚Äôve silently resigned to my fate and even convinced myself that my hair is an unqualified mess and therefore, must be sentenced to the equivalent of capital punishment in the world of bodily produced keratinous filaments. So, that night, I walk towards my bed after a long and emotionally exhausting week and pass by a mirror I had forgotten was even there. Turn around and look at myself in the mirror (I was really tempted to make a ‚Äúhow do you sleep at night‚Äù and/or a ‚Äúhow do you look at yourself in the mirror‚Äù joke at this point but couldn‚Äôt come up with one that was relevant) and my hair is on point. Just perfect. It‚Äôs the best it‚Äôs looked in forever, and all of that emotional foundation-laying goes to waste. But, I have no choice but to go ahead with it. The next morning, I‚Äôd embark on the journey (that one must take alone) to the hallowed ceremonial grounds (read salon) and when I‚Äôd reach there the next life lesson would await me.\nBarbers are a sadistic lot. I would, honestly, not be surprised if most psychopathic and sadistic serial killers worked day jobs as barbers. By the time I‚Äôve been led up to the chair, they‚Äôve used their acute senses to determine my willingness (or lack thereof). And, because I‚Äôm, well, me, the thinly veiled look of acute depression and loss doesn‚Äôt help. They pick their tools up and make to stand behind me. Much like this.\nAnd every time, I can almost hear them think to themselves, ‚ÄúOh, I‚Äôm not gonna shave your head. I‚Äôm just gonna cut your hair. Really, really, short.‚Äù I know, at this point, all this sounds like I‚Äôm reading too much into body language. Well, explain this. For those not familiar with the process, the electric razor is used to trim the hair on the sides of the head first. When that‚Äôs done, you reach a state where even looking at that figure staring at you from the mirror in front of you is difficult. Because you look like a sentient mushroom cloud. This is where the psychopathy of the barber kicks in. Right then, at your most vulnerable, he stops. And walks away. For ‚Äútea‚Äù. And as he sips on his beverage that was, conveniently, delivered just then, he looks across at you trying to take it all in; trying to come to terms with the ramifications of his not coming back. Imagine having to walk out like that! It would mean the complete and utter annihilation of any social, and therefore, self-image you had spent your life painstakingly constructing. Just before you‚Äôve given up any hope of walking out of there without pulling off a Shia LaBeouf (Google: Shia LaBeouf paper bag, for more details), he comes back, now a saviour. With nowhere left to go but up.\nIf this is not oppression I don‚Äôt know what is. To convince you, I am now going to quote someone famous. Was it not Martin Luther King who said, ‚ÄúI have a dream that my four little children will one day live in a nation where they will not be judged by the length of their hair but by the content of their character.‚Äù It wasn‚Äôt? Well, it ought to have been. So, I‚Äôd like to send a message to all the kids out there who have been through this. Remember that countless lives have been lost to the cause of swaraj and we are not going to let them take it away from us.\n","date":1476835200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1476835200,"objectID":"7249de721fe9da8f5d8db9fc83265b4b","permalink":"https://abhinaukumar.github.io/post/in-search-of-swaraj/","publishdate":"2016-10-19T00:00:00Z","relpermalink":"/post/in-search-of-swaraj/","section":"post","summary":"I‚Äôve lived a vast majority of my life in a dominion where a minority (one-fourth) of the population wielded a disproportionate amount of power over the numerical majority. Much like the British did in colonial India.","tags":["Casual"],"title":"In Search of Swaraj","type":"post"},{"authors":["Abhinau Kumar"],"categories":["Article"],"content":"Because I‚Äôve had a lot of people tell me the worst way to start an article is using a quote, I‚Äôm going to do just that.\n‚ÄúEverything good, everything magical happens between the months of June and August.‚Äù Now, before you think I‚Äôm some literary savant who knows all quotes by anybody famous, I‚Äôd like to confess that I Googled for this after I wrote down the title of the article. And I found it on Buzzfeed, not Goodreads; so you can imagine my literary inclinations. One of the things that struck me about it, though, was that it was tagged under ‚ÄúSummer‚Äù. I think it should be tagged under ‚ÄúGuilt‚Äù.\nFor the less informed, summer vacations at IITH start in May and extend through to the end of July. When you spend the last week of the semester writing tests and assignments, you convince yourself that it‚Äôs all worth it for the amazing three months ahead. As I stood at the exit of the hostels waiting for a bus to take me on a 14-hour journey home, I made a commitment to myself. A commitment that I‚Äôd use my holidays to do everything I ever wanted to do because I had finally understood the value of a vacation. ‚ÄúEver‚Äù, obviously, referring to the previous week because that‚Äôs the only thing I had a memory of. Who has life goals anyway, right? Thus began my journey to the discovery that the five stages of summer are, actually, the five stages of grief backwards\nIt was a week after I sank into a sofa for the first time in two months that I realised that that moment of realisation was truly once-in-a-vacation. Cuz you ain‚Äôt gonna have one after you get home. You see, trauma-induced motivation is like that cheap deodorant you bought on a trip because you suddenly realised you needed one. You‚Äôre not going back to it once you get home. And I didn‚Äôt mind that at all. There it was; stage 1 ‚Äì Acceptance.\nThe first week had passed informing people I was in town (although, I only actually met family in the first six weeks) and catching up on the shows I thought I was tired of, spending time I thought I had nothing to do with. Of course, I was wrong on both counts.\nAs I‚Äôve shown in, what I call, the awesomeness vs time curve up there, week 2 is when you really ease into the routine and start to appreciate it. This was the life, you know. Food laid out on a table, watching Survivor, Masterchef and House on repeat. Especially because Masterchef was on right about lunchtime. I didn‚Äôt know which meal I was having but it was pretty convenient to be treated like the judges were on TV. And you‚Äôve gotta admit, watching people fight for food on reward challenges really boosts the ego about having it for free.\nStage 2, depression, began about midway into the third week with Survivor going off-air, leaving a vacuum that could only be filled in by rational thought. I began to realise what a waste of a day it was to watch other people make a million dollars. The excitement just dropped, and it was all downhill from there. I‚Äôd explain in detail, but given that almost all of you who‚Äôre reading this have been through JEE, you‚Äôd have been acquainted with it at some point. To sum it up, I spent all my time complaining that I couldn‚Äôt do the things I wanted to do because I was too bored because I had nothing to do. (It made sense at the time)\nThen began the bargaining. I figured if I could spend about an hour a day doing something productive, it‚Äôd atone for how I spent the other 23. It went on for a couple of days but I ran out of easy things to do very soon. (That‚Äôs when I began this article, by the way) Then, mathematics took over and I figured I didn‚Äôt have to be productive all at once, and I could work on something in parts spread out between the TV shows I was addicted to by now, and still do the same amount of work.\nBy the end of that week, I realised that I wasn‚Äôt the only one who noticed I hadn‚Äôt been much use to anybody in a while. And the others began to feel they should, at least, make me aware. It was hard to make a case to them because even I knew it was true but I just didn‚Äôt want to do much else about it. A couple of days of being annoyed and I realized I was at the anger stage. So I decided to do the first sensible thing I‚Äôd done in almost a month and set up for when I‚Äôd get some sense knocked into me because I figured it probably wasn‚Äôt that far away anyway.\nAs much as it pains the nonconformist in me to admit, I did what I was supposed to do and set up some achievable goals and got myself productive again. Which brings us to the last and final stage ‚Äì denial. Denial of what, you ask? Nothing much, really. Just that the first four weeks ever happened.\nSo, I guess people say moments of redemption are spread far and wide in our lives. But I had one every week; every Sunday before I went to bed (although, technically, that would make it Monday) when I realised I just lost another week to make myself productive. Luckily, the third time was a charm. Remember that guilt I wrote about at the beginning? Turns out it‚Äôs a potent getter-off-your-butt-er. So (That‚Äôs just me stressing on the word) potent, in fact, that it got me back to finish the second half of this article. Twice. So wherever you are in this process, remember that May sets it up for you and the more guilt you accumulate the better. Because that‚Äôll fuel you through the five stages of summer.\n","date":1468022400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1468022400,"objectID":"877a22a939148eb7cf9c6b49c7d1ceab","permalink":"https://abhinaukumar.github.io/post/the-five-stages-of-summer/","publishdate":"2016-07-09T00:00:00Z","relpermalink":"/post/the-five-stages-of-summer/","section":"post","summary":"Because I‚Äôve had a lot of people tell me the worst way to start an article is using a quote, I‚Äôm going to do just that.\n‚ÄúEverything good, everything magical happens between the months of June and August.","tags":["Casual"],"title":"The Five Stages of Summer","type":"post"},{"authors":["Abhinau Kumar"],"categories":["Article"],"content":"Ever since the game of cricket began to be telecast to crowds, live commentary has played a great role in bringing the game closer to laypeople. This, they achieved, not just by their knowledge of the game, but sometimes also simply by their choice of how to present a situation to the audience. When Mansur Ali Khan Pataudi was dismissed for one run against England at Bombay in 1973, the commentator, Bobby Talyarkhan remarked on the radio, ‚ÄúPataudi is out 99 runs short of an expected century!‚Äù\nFor long, Indians hadn‚Äôt been considered great commentators. But then came one man who, at his peak, had played for his college and was best appreciated for getting off strike when he batted. And he took Indian cricket commentary to new heights.\nHarsha Bhogle started at the age of nineteen at the All India Radio. He became the first Indian to be invited by the Australian Broadcasting Corporation to cover India‚Äôs tour of Australia before the 1992 World Cup. The catch was that the ABC would not pay the foreign presenter, and it was up to the home broadcasting stations to pay the presenter. And the AIR did not even know he went to Australia. But being signed did not mean respect and that is probably the thing he handled best. Instead of competing against other cricketer-turned-commentators, he adopted a different style of commentary; a non-technical flavour which captivated the audience as no one had before.\nHe gained the support and respect of active cricketers at that time because he made them feel comfortable during their interviews. And for someone who is constantly being judged on his performance and worth, there is nothing more refreshing than being spoken to, not as an authority, but as an admirer.\nIn 2005, he visited his alma mater and gave a pretty cool speech on excellence. And his opening line went something like this. ‚ÄúI am here, not as someone who has achieved excellence in life, but as someone who has seen excellence first-hand‚Äù It is this kind of modesty that made him the most popular cricket commentator in the world. And as there weren‚Äôt many ‚Äúgreat‚Äù presenters at that time, he sought inspiration from the cameramen and technicians he worked with and tried to emulate the kind of perfection they brought to their professions.\nAnd he had a brilliant way of putting things. During the recently concluded Cricket World Cup, when playing against India, AB de Villiers missed a rather simple run-out chance. While the others were ruing the missed opportunity or criticizing the undue risk taken by the Indian batsmen, Harsha Bhogle chose to see the positive side. ‚ÄúThe one good thing we can all take from this run out is that AB de Villiers has proved to the world that he too is human and that he too can make mistakes‚Äù Rewind back to a warm-up game before the World Cup. The Indian Team dropped a considerable number of catches in that match. Once again, optimism had not seen a more faithful patron, till that night. ‚ÄúIt‚Äôs okay. The Indians can drop as many as they want now and let the law of averages catch up.‚Äù In another ODI game, Sachin Tendulkar and MS Dhoni were playing at the end of the innings. Dhoni had dismissed the ball to all sides of the park. Getting in on the act, Sachin caressed one to the boundary. In all this mayhem, Harsha Bhogle‚Äôs voice was the only one that left the commentary box. ‚ÄúWe have a surgeon at one end and a butcher at the other‚Äù.\nTest games are known for their long-drawn-out nature, and it is in such situations that the role of a commentator doubles up as an entertainer to keep the readers hooked to the game. In a game against India, Alastair Cook had scored one run in 52 balls. The next ball, he scored a boundary. Seizing the opportunity, Bhogle remarked, ‚Äú80% of his runs came off that one ball!‚Äù His commentary wasn‚Äôt limited to what happened on the field at that time. Speaking about the legend that is Sachin Tendulkar, he said. ‚ÄúEruption of joy at an Indian wicket can only mean one thing ‚Äì that Sachin is next to the crease.‚Äù And how good is a commentator who focuses on just his home country? When Misbah-ul-Haq came under criticism from players like Shoaib Akthar and other Pakistani players, he commented, ‚ÄúI find this criticism of Misbah very strange. It is like a family of ten complaining that the sole breadwinner isn‚Äôt doing enough. Misbah is rated far higher outside Pakistan than within. Afridi is rated much higher in Pakistan that outside!‚Äù Sometimes, even the most optimistic of people must bow down to reality. And Harsha Bhogle did so gracefully. When Ian Chappell asked him if Narendra Hirwani can bat, he said, ‚ÄúIf you make a team with all the No.11s of all the teams, Narendra Hirwani would still be the No.11.‚Äù On Rahul Dravid being bowled on his last international appearance, ‚ÄúIn a career that is marked by grace, style and beautiful batsmanship, it‚Äôs a slog that‚Äôs ended Rahul Dravid‚Äòs career. But once again, it was what THE TEAM needed.‚Äù\nStarting as an outlaw in a profession dominated by former cricketers, he went on to carve a niche for himself, applying one cardinal rule he claimed to have learnt from journalism. ‚ÄúDon‚Äôt let the truth get in the way of a good story.‚Äù. Everywhere he went, he was consistently asked one question; ‚ÄúHow many games have you played?‚Äù Initially determined to prove himself as one of them, he later realized that there were too many of ‚Äúthem‚Äù anyway and that he shouldn‚Äôt try to ‚Äúexamine Sachin Tendulkar‚Äôs cover drive‚Äù or ‚Äúexplain the art of reverse swing with Wasim Akram beside me in the box‚Äù.\nAnd I think the success of Harsha Bhogle lies in that moment of realization. He embraced his lack of technical knowledge and replaced it with his supreme presenting skills, and that has made all the difference.\n","date":1452556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1452556800,"objectID":"1164e6d18f84beb223de8ec08841b5f6","permalink":"https://abhinaukumar.github.io/post/harsha-bhogle/","publishdate":"2016-01-12T00:00:00Z","relpermalink":"/post/harsha-bhogle/","section":"post","summary":"Ever since the game of cricket began to be telecast to crowds, live commentary has played a great role in bringing the game closer to laypeople. This, they achieved, not just by their knowledge of the game, but sometimes also simply by their choice of how to present a situation to the audience.","tags":["Casual"],"title":"He Who Derived His Excellence From That Of Others","type":"post"},{"authors":["Abhinau Kumar"],"categories":["Poetry"],"content":"There was once a man, quite old was he,\nHe spent his day under a Banyan tree.\nMen would come, and men would go\nUp to twilight, from the cock‚Äôs crow.\nThey came from all walks of life,\nSeemingly struck by mental strife.\nWhen they left, they left with a smile,\nNo wonder he had people waiting in a file.\nAnd I wonder, ‚ÄúSo knowledgeable is he,\nThe Man Who Sits Under The Banyan Tree?‚Äù\nAnd where would he spend the night in?\nFor he didn‚Äôt seem to have any next of kin.\nBack then, I was new to the city,\nAnd so, all this was quite the mystery.\nI asked around, and answers I got,\nWhich were contrary to what I thought.\nAnd I mused, ‚ÄúWhat a fool he must be,\nThe Man Who Sits Under The Banyan Tree?\u0026quot;\nAs he was an imbecile, he was thrown\nOut to the streets, by his very own.\nThe rest of it is an urban lore,\nBottom line being, he found a door\nThat fed him well, and made him gain\nIn health, only to get back on the streets again.\nAnd I ask myself, ‚ÄúWhat do people see,\nIn The Man Who Sits Under The Banyan Tree?‚Äù\nAtop a pedestal, he sat on a bed,\nAnd so, I couldn‚Äôt listen to what he said.\nBut, I wanted to see what pulled such a crowd\nWhich made the whole street buzz aloud.\nAfter pushing, pulling, a minor tussle\nAnd two hours of being asked to hustle,\nI got to where he was, eventually,\nTo meet The Man Who Sits Under The Banyan Tree.\n‚ÄúAll my life, Sir, what have I done?‚Äù\n‚ÄúYou have done a good job, my son.‚Äù\nSo, I told myself, ‚ÄúThis man, here,\nHe must be a great seer.‚Äù\nAnd then, I realized, for whatever I said,\n‚ÄúYou‚Äôve done a good job, son; go ahead.‚Äù\nLooking back - Irrespective of the crowd there,\nThe locals never did seem to care.\nSo, let me give you some advice for free,\nDon‚Äôt fall for The Man Who Sits Under The Banyan Tree.\n","date":1439596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1439596800,"objectID":"036d793f080d079a4cb00bff09d4447c","permalink":"https://abhinaukumar.github.io/post/banyan-tree/","publishdate":"2015-08-15T00:00:00Z","relpermalink":"/post/banyan-tree/","section":"post","summary":"There was once a man, quite old was he,\nHe spent his day under a Banyan tree.\nMen would come, and men would go\nUp to twilight, from the cock‚Äôs crow.","tags":["Casual"],"title":"The Man Who Sits Under The Banyan Tree","type":"post"}]